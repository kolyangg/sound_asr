{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import kenlm\n",
    "from transformers import Wav2Vec2Processor, AutoProcessor\n",
    "from pyctcdecode import build_ctcdecoder\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from typing import List, Tuple, Optional, Union\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "class CTCTextEncoder:\n",
    "    def __init__(\n",
    "        self,\n",
    "        arpa_path: Optional[str] = None,\n",
    "        binary_path: Optional[str] = None,\n",
    "        unigram_path: Optional[str] = None,\n",
    "        pretrained_tokenizer: str = \"facebook/wav2vec2-base-960h\",\n",
    "        lm_weight: float = 0.5,\n",
    "        beam_size: int = 100,\n",
    "        use_lm: bool = False,     # **Added use_lm parameter**\n",
    "        use_bpe: bool = False,    # **Added use_bpe parameter**\n",
    "        blank_token: str = \"[pad]\",  # Blank token as <pad> for Wav2Vec2\n",
    "        unk_token: str = \"[unk]\",     # UNK token\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize encoder with conditional tokenizer/processor and language model.\n",
    "\n",
    "        Parameters:\n",
    "        - use_lm (bool): Whether to use the Language Model (LM) during decoding.\n",
    "        - use_bpe (bool): Whether to use Byte Pair Encoding (BPE) via tokenizer/processor.\n",
    "                           If False, perform character-based encoding/decoding without tokenizer.\n",
    "        \"\"\"\n",
    "        self.beam_size = beam_size\n",
    "        self.lm_weight = lm_weight\n",
    "        self.arpa_path = arpa_path\n",
    "        self.binary_path = binary_path\n",
    "        self.blank_token = blank_token\n",
    "        self.unk_token = unk_token\n",
    "        self.use_lm = use_lm # False\n",
    "        self.use_bpe = False # False MANUAL FOR NOW\n",
    "        # self.use_bpe = False # use_bpe # False # use_bpe\n",
    "        self.printed_samples = 0\n",
    "        self.max_printed_samples = 5\n",
    "        print('CTC Text Encoder:')\n",
    "        print('pretrained_tokenizer:', pretrained_tokenizer)\n",
    "        print('lm_weight:', lm_weight)\n",
    "        print('beam_size:', beam_size)\n",
    "        print('binary_path:', binary_path)\n",
    "        print('use_lm:', self.use_lm)\n",
    "        print('use_bpe:', self.use_bpe)\n",
    "\n",
    "        # Load unigrams if provided\n",
    "        self.unigrams = None\n",
    "        if unigram_path and os.path.exists(unigram_path):\n",
    "            print(f\"Loading unigrams from: {unigram_path}\")\n",
    "            with open(unigram_path, 'r', encoding='utf-8') as f:\n",
    "                self.unigrams = [line.strip().lower() for line in f if line.strip()]\n",
    "            print(f\"Loaded {len(self.unigrams)} unigrams\")\n",
    "\n",
    "        # Initialize the tokenizer or set up character-based vocab\n",
    "        self._initialize_vocabulary(pretrained_tokenizer)\n",
    "\n",
    "        # Create index mappings\n",
    "        self.ind2char = dict(enumerate(self.vocab))\n",
    "        self.char2ind = {v: k for k, v in self.ind2char.items()}\n",
    "        self.blank_index = self.char2ind.get(self.blank_token, None)\n",
    "\n",
    "        print(f\"\\nVocabulary Info:\")\n",
    "        print(f\"Size: {len(self.vocab)}\")\n",
    "        print(\"Full Vocabulary (up to first 50 tokens):\", self.vocab[:50])\n",
    "        print(f\"Blank token: {self.blank_token}, Blank index: {self.blank_index}\")\n",
    "\n",
    "        print(\"Sample ind2char mappings:\", {k: self.ind2char[k] for k in list(self.ind2char.keys())[:10]})\n",
    "        print(\"Sample char2ind mappings:\", {k: self.char2ind[k] for k in list(self.char2ind.keys())[:10]})\n",
    "\n",
    "        # **Conditionally initialize language model based on use_lm**\n",
    "        if self.use_lm:\n",
    "            self._initialize_language_model()\n",
    "        else:\n",
    "            print(\"Language model usage is disabled.\")\n",
    "            self.lm = None\n",
    "            self.decoder = None\n",
    "\n",
    "    def _initialize_vocabulary(self, pretrained_tokenizer: str):\n",
    "        \"\"\"\n",
    "        Initialize the vocabulary either using a tokenizer/processor (BPE) or character-based.\n",
    "\n",
    "        Parameters:\n",
    "        - pretrained_tokenizer (str): Name or path of the pretrained tokenizer.\n",
    "        \"\"\"\n",
    "        if self.use_bpe:\n",
    "            # === NEW TOKENIZER LOGIC ===\n",
    "            print(\"Initializing tokenizer and using BPE for encoding/decoding.\")\n",
    "\n",
    "            # Initialize Wav2Vec2Processor\n",
    "            self.processor = Wav2Vec2Processor.from_pretrained(pretrained_tokenizer)\n",
    "\n",
    "            # 1) Retrieve tokenizer vocab\n",
    "            vocab_dict = self.processor.tokenizer.get_vocab()\n",
    "            # 2) Sort by index; keep uppercase form as the official indices\n",
    "            sorted_vocab_dict = {k: v for k, v in sorted(vocab_dict.items(), key=lambda item: item[1])}\n",
    "            # 3) Also create a lowercase list for 'labels' usage in LM\n",
    "            sorted_vocab_dict_for_labels = {k.lower(): v for k, v in sorted(vocab_dict.items(), key=lambda item: item[1])}\n",
    "            self.labels = list(sorted_vocab_dict_for_labels.keys())\n",
    "\n",
    "            # 4) Convert the sorted vocab to a list (maintaining original order)\n",
    "            #    Then replace '|' with space if present.\n",
    "            self.vocab = list(sorted_vocab_dict.keys())\n",
    "            self.vocab = [token.replace('|', ' ') for token in self.vocab]\n",
    "\n",
    "            # # 5) Add the unique blank token if not present\n",
    "            # if self.blank_token not in self.processor.tokenizer.get_vocab():\n",
    "            #     self.processor.tokenizer.add_tokens([self.blank_token])\n",
    "            #     print(f\"Added '{self.blank_token}' to the tokenizer's vocabulary.\")\n",
    "            # else:\n",
    "            #     print(f\"'{self.blank_token}' already exists in the tokenizer's vocabulary.\")\n",
    "\n",
    "            # # 6) Add the unk token if not present (optional; already usually in Wav2Vec2 vocab)\n",
    "            # if self.unk_token not in self.processor.tokenizer.get_vocab():\n",
    "            #     self.processor.tokenizer.add_tokens([self.unk_token])\n",
    "            #     print(f\"Added '{self.unk_token}' to the tokenizer's vocabulary.\")\n",
    "            # else:\n",
    "            #     print(f\"'{self.unk_token}' already exists in the tokenizer's vocabulary.\")\n",
    "\n",
    "            # Debug prints\n",
    "            print(\"BPE-based vocab size:\", len(self.vocab))\n",
    "            print(\"Modified vocab sample:\", self.vocab[:20])\n",
    "            # === END NEW TOKENIZER LOGIC ===\n",
    "\n",
    "\n",
    "        else:\n",
    "            print(\"Initializing character-based vocabulary without using tokenizer.\")\n",
    "            # Define a simple character-based vocabulary: a-z and space\n",
    "            self.vocab = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',\n",
    "                         'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x',\n",
    "                         'y', 'z', ' ']\n",
    "            # Optionally, add special tokens\n",
    "            self.vocab += [self.blank_token, self.unk_token]\n",
    "\n",
    "            # **Create index mappings**\n",
    "            self.ind2char = dict(enumerate(self.vocab))\n",
    "            self.char2ind = {v: k for k, v in self.ind2char.items()}\n",
    "            self.blank_index = self.char2ind.get(self.blank_token, None)\n",
    "\n",
    "            # No processor is used in this mode\n",
    "            self.processor = None\n",
    "\n",
    "    def _initialize_language_model(self):\n",
    "        \"\"\"Initialize language model with explicit blank token handling.\"\"\"\n",
    "        self.lm = None\n",
    "        self.decoder = None\n",
    "\n",
    "        model_path = self.binary_path if self.binary_path else self.arpa_path\n",
    "        print('model_path: ', model_path)\n",
    "        if not model_path or not os.path.exists(model_path):\n",
    "            print(\"No language model path provided or file does not exist.\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            self.lm = kenlm.Model(model_path)\n",
    "            print(f\"Loaded {'binary' if self.binary_path else 'ARPA'} language model.\")\n",
    "\n",
    "            decoder_config = {\n",
    "                \"labels\": self.labels if self.use_bpe else self.vocab,\n",
    "                \"kenlm_model_path\": model_path,\n",
    "                \"alpha\": self.lm_weight,\n",
    "                \"beta\": 0.1,\n",
    "                \"unk_score_offset\": -10.0,\n",
    "            }\n",
    "\n",
    "            if self.unigrams:\n",
    "                print(\"\\n--- Unigrams List ---\")\n",
    "                # Save the unigrams to a file\n",
    "                with open(\"unigrams_list.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "                    for unigram in self.unigrams:\n",
    "                        f.write(f\"{unigram}\\n\")\n",
    "                print(f\"Unigrams list saved to 'unigrams_list.txt'. Total unigrams: {len(self.unigrams)}\")\n",
    "                print(\"----------------------\\n\")\n",
    "                decoder_config[\"unigrams\"] = self.unigrams\n",
    "\n",
    "            self.decoder = build_ctcdecoder(**decoder_config)\n",
    "            print(\"Successfully initialized language model and decoder.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Failed to initialize decoder: {str(e)}\")\n",
    "            self.decoder = None\n",
    "\n",
    "    def encode(self, text: str) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Encode text either using tokenizer/processor (BPE) or character-based encoding.\n",
    "\n",
    "        Parameters:\n",
    "        - text (str): The input text to encode.\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: Tensor of token indices.\n",
    "        \"\"\"\n",
    "        debug = False\n",
    "\n",
    "        if self.printed_samples < self.max_printed_samples:\n",
    "            original_text = text\n",
    "            text = self.normalize_text(text)\n",
    "            if debug:\n",
    "                print(f\"samples: {str(self.printed_samples)}\")\n",
    "                print(f\"\\nEncoding text:\")\n",
    "                print(f\" Original: '{original_text}'\")\n",
    "                print(f\" Normalized: '{text}'\")\n",
    "                for ch in text:\n",
    "                    print(ch, ord(ch))\n",
    "\n",
    "        if self.use_bpe:\n",
    "            try:\n",
    "                encoded = self.processor.tokenizer(text, return_tensors=\"pt\", padding=False, truncation=False)\n",
    "                token_indices = encoded.input_ids[0].tolist()\n",
    "                if self.printed_samples < self.max_printed_samples:\n",
    "                    # Convert indices to tokens from self.vocab\n",
    "                    tokens = [self.vocab[idx] if 0 <= idx < len(self.vocab) else \"<invalid>\" for idx in token_indices]\n",
    "                    # Optionally print tokens for debugging\n",
    "                    # print(f\" Tokens (lowercased and '|'->' '): {tokens}\")\n",
    "                    # print(f\" Token indices: {token_indices}\")\n",
    "                    self.printed_samples += 1\n",
    "                return torch.tensor(token_indices).unsqueeze(0)\n",
    "            except KeyError as e:\n",
    "                unknown_tokens = set([token for token in text.split() if token not in self.char2ind])\n",
    "                raise Exception(f\"Unknown tokens: '{' '.join(unknown_tokens)}'\")\n",
    "            except Exception as e:\n",
    "                raise Exception(f\"Encoding error: {str(e)}\")\n",
    "        else:\n",
    "            # Character-based encoding\n",
    "            try:\n",
    "                normalized_text = self.normalize_text(text)\n",
    "                token_indices = [self.char2ind.get(char, self.char2ind.get(self.unk_token)) for char in normalized_text]\n",
    "                return torch.tensor(token_indices).unsqueeze(0)\n",
    "            except Exception as e:\n",
    "                raise Exception(f\"Encoding error: {str(e)}\")\n",
    "\n",
    "    def decode(self, indices: List[int]) -> str:\n",
    "        \"\"\"\n",
    "        Decode indices to text using beam search decoder if available.\n",
    "\n",
    "        Parameters:\n",
    "        - indices (List[int]): List of token indices.\n",
    "\n",
    "        Returns:\n",
    "        - str: Decoded text.\n",
    "        \"\"\"\n",
    "        if self.decoder:\n",
    "            decoded_text = self.decoder.decode(indices)\n",
    "            # Convert to lower case\n",
    "            decoded_text = decoded_text.lower()\n",
    "            return decoded_text\n",
    "        else:\n",
    "            decoded_text = self.decode_simple(indices)\n",
    "            # Convert to lower case\n",
    "            decoded_text = decoded_text.lower()\n",
    "            return decoded_text  # Ensure the decoded text is returned\n",
    "\n",
    "    # def decode_simple(self, indices: List[int]) -> str:\n",
    "    #     \"\"\"\n",
    "    #     Simple CTC decoding without language model.\n",
    "    #     Collapses consecutive duplicate tokens and removes blanks.\n",
    "\n",
    "    #     Parameters:\n",
    "    #     - indices (List[int]): List of token indices.\n",
    "\n",
    "    #     Returns:\n",
    "    #     - str: Decoded text.\n",
    "    #     \"\"\"\n",
    "    #     decoded_chars = []\n",
    "    #     previous_idx = None\n",
    "\n",
    "    #     for idx in indices:\n",
    "    #         if idx == self.blank_index:\n",
    "    #             previous_idx = idx\n",
    "    #             continue  # Skip blank tokens\n",
    "    #         if idx == previous_idx:\n",
    "    #             continue  # Skip duplicate tokens\n",
    "    #         if 0 <= idx < len(self.ind2char):\n",
    "    #             char = self.ind2char[idx]\n",
    "    #             decoded_chars.append(char)\n",
    "    #         previous_idx = idx\n",
    "\n",
    "    #     # Join characters without spaces and convert to lowercase\n",
    "    #     text = \"\".join(decoded_chars).strip().lower()\n",
    "\n",
    "    #     if self.use_bpe and self.processor:\n",
    "    #         # Clean up tokenization using the tokenizer's method\n",
    "    #         return self.processor.tokenizer.clean_up_tokenization(text)\n",
    "    #     else:\n",
    "    #         # For character-based decoding, additional cleanup can be added if necessary\n",
    "    #         return text\n",
    "        \n",
    "    def decode_simple(self, indices: List[int]) -> str:\n",
    "        \"\"\"\n",
    "        Simple CTC decoding without language model.\n",
    "        Collapses consecutive duplicate tokens and removes blanks.\n",
    "        \"\"\"\n",
    "        decoded_chars = []\n",
    "        previous_idx = None\n",
    "\n",
    "        for idx in indices:\n",
    "            # Skip blank tokens (blank index is 27 based on your debug info)\n",
    "            if idx == self.blank_index:\n",
    "                previous_idx = idx\n",
    "                continue\n",
    "\n",
    "            # Skip consecutive duplicates\n",
    "            if idx == previous_idx:\n",
    "                continue\n",
    "\n",
    "            # Check if the index is valid, otherwise raise an error\n",
    "            if 0 <= idx < len(self.ind2char):\n",
    "                decoded_chars.append(self.ind2char[idx])\n",
    "            else:\n",
    "                print(f\"[DEBUG] Invalid index encountered: {idx}\")\n",
    "\n",
    "            previous_idx = idx\n",
    "\n",
    "        # Join characters and return the decoded string\n",
    "        decoded_text = \"\".join(decoded_chars).strip()\n",
    "        print(f\"[DEBUG] Decoded Text: {decoded_text}\")\n",
    "        return decoded_text\n",
    "\n",
    "\n",
    "\n",
    "    def decode_logits(self, logits: Union[torch.Tensor, List[List[float]], np.ndarray]) -> str:\n",
    "        \"\"\"\n",
    "        Decode logits using the decoder if available, otherwise use greedy decoding.\n",
    "\n",
    "        Parameters:\n",
    "        - logits (Union[torch.Tensor, List[List[float]], np.ndarray]): Logits output from the model.\n",
    "\n",
    "        Returns:\n",
    "        - str: Decoded text.\n",
    "        \"\"\"\n",
    "        if isinstance(logits, torch.Tensor):\n",
    "            logits = logits.cpu().numpy()\n",
    "        elif isinstance(logits, list):\n",
    "            logits = np.array(logits)\n",
    "        elif not isinstance(logits, np.ndarray):\n",
    "            raise TypeError(\"logits must be a torch.Tensor, list of lists, or numpy.ndarray\")\n",
    "\n",
    "        if logits.ndim == 3:\n",
    "            logits = logits[0]\n",
    "\n",
    "        if logits.ndim != 2:\n",
    "            raise ValueError(f\"Logits should be 2D (time_steps, vocab_size), got {logits.ndim}D\")\n",
    "\n",
    "        if self.decoder:\n",
    "            decoded_text = self.decoder.decode(logits)\n",
    "            return decoded_text\n",
    "        else:\n",
    "            predicted_indices = np.argmax(logits, axis=-1).tolist()\n",
    "            return self.decode_simple(predicted_indices)\n",
    "\n",
    "    def decode_indices(self, indices: Union[torch.Tensor, List[int], np.ndarray]) -> str:\n",
    "        \"\"\"\n",
    "        Decode token indices to text using simple decoding (no LM).\n",
    "\n",
    "        Parameters:\n",
    "        - indices (Union[torch.Tensor, List[int], np.ndarray]): Token indices.\n",
    "\n",
    "        Returns:\n",
    "        - str: Decoded text.\n",
    "        \"\"\"\n",
    "        if isinstance(indices, torch.Tensor):\n",
    "            indices = indices.squeeze().tolist()\n",
    "        elif isinstance(indices, np.ndarray):\n",
    "            indices = indices.tolist()\n",
    "        elif not isinstance(indices, list):\n",
    "            raise TypeError(\"decode_indices expects a list, torch.Tensor, or numpy.ndarray.\")\n",
    "\n",
    "        return self.decode_simple(indices)\n",
    "\n",
    "    def ctc_decode(self, logits: Union[torch.Tensor, List[int], np.ndarray]) -> str:\n",
    "        \"\"\"\n",
    "        Perform CTC decoding on logits.\n",
    "\n",
    "        Parameters:\n",
    "        - logits (Union[torch.Tensor, List[int], np.ndarray]): Logits or token indices.\n",
    "\n",
    "        Returns:\n",
    "        - str: Decoded text.\n",
    "        \"\"\"\n",
    "        if isinstance(logits, np.ndarray):\n",
    "            logits = torch.from_numpy(logits)\n",
    "        elif isinstance(logits, list):\n",
    "            logits = torch.tensor(logits)\n",
    "\n",
    "        if logits.dim() == 3:\n",
    "            decoded_text = self.decode_logits(logits)\n",
    "            return decoded_text\n",
    "        elif logits.dim() == 2:\n",
    "            decoded_text = self.decode_logits(logits)\n",
    "            return decoded_text\n",
    "        elif logits.dim() == 1:\n",
    "            decoded_text = self.decode_indices(logits)\n",
    "            return decoded_text\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported logits shape: {logits.shape}. Expected 1D, 2D, or 3D.\")\n",
    "\n",
    "    def ctc_beam_search(self, probs, beam_size, use_lm = False, debug: bool = False) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Beam search with optional Language Model support.\n",
    "\n",
    "        Parameters:\n",
    "        - probs: Probability distributions over tokens.\n",
    "        - debug (bool): Whether to print debug information.\n",
    "\n",
    "        Returns:\n",
    "        - List[Tuple[str, float]]: List of decoded text with scores.\n",
    "        \"\"\"\n",
    "        beam_size = self.beam_size\n",
    "        debug = False\n",
    "\n",
    "        if self.use_lm and self.decoder is not None:\n",
    "            try:\n",
    "                if isinstance(probs, torch.Tensor):\n",
    "                    probs = probs.cpu().numpy()\n",
    "                elif isinstance(probs, list):\n",
    "                    probs = np.array(probs)\n",
    "                elif isinstance(probs, np.ndarray):\n",
    "                    pass\n",
    "                else:\n",
    "                    raise TypeError(\"probs must be a torch.Tensor, list, or numpy.ndarray\")\n",
    "\n",
    "                beams = self.decoder.decode_beams(\n",
    "                    probs,\n",
    "                    beam_prune_logp=-10.0,\n",
    "                    token_min_logp=-5.0,\n",
    "                    hotwords=[],\n",
    "                    hotword_weight=10.0,\n",
    "                )\n",
    "\n",
    "                formatted_beams = []\n",
    "                for beam in beams[:beam_size]:\n",
    "                    text = beam[0]\n",
    "                    acoustic_score = beam[3]\n",
    "                    lm_score = beam[4]\n",
    "\n",
    "                    if self.use_bpe and self.processor:\n",
    "                        text = self.processor.tokenizer.clean_up_tokenization(text)\n",
    "                    text = text.lower().strip()\n",
    "\n",
    "                    combined_score = (1 - self.lm_weight) * acoustic_score + self.lm_weight * lm_score\n",
    "                    text_len = max(1, len(text.split())) if self.use_bpe else max(1, len(text))\n",
    "                    normalized_score = combined_score / text_len\n",
    "\n",
    "                    formatted_beams.append((text, normalized_score))\n",
    "\n",
    "                if debug:\n",
    "                    print(\"\\nFormatted beam results with Language Model:\")\n",
    "                    for text, score in formatted_beams[:3]:\n",
    "                        print(f\"Text: '{text}', Score: {score:.4f}\")\n",
    "\n",
    "                if formatted_beams:\n",
    "                    return sorted(formatted_beams, key=lambda x: -x[1])\n",
    "                else:\n",
    "                    print(\"No valid beams found, falling back to standard beam search\")\n",
    "                    return self._standard_beam_search(probs, debug)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Beam search with LM failed: {str(e)}, falling back to standard beam search\")\n",
    "                return self._standard_beam_search(probs, debug)\n",
    "        else:\n",
    "            return self._standard_beam_search(probs, debug)\n",
    "\n",
    "    def _standard_beam_search(self, probs, debug: bool = False) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Original beam search implementation without Language Model.\n",
    "\n",
    "        Parameters:\n",
    "        - probs: Probability distributions over tokens.\n",
    "        - debug (bool): Whether to print debug information.\n",
    "\n",
    "        Returns:\n",
    "        - List[Tuple[str, float]]: List of decoded text with scores.\n",
    "        \"\"\"\n",
    "        beam_size = self.beam_size\n",
    "\n",
    "        if isinstance(probs, np.ndarray):\n",
    "            probs = torch.from_numpy(probs)\n",
    "\n",
    "        if probs.device != torch.device('cpu'):\n",
    "            probs = probs.cpu()\n",
    "\n",
    "        dp = {(\"\", self.blank_token): 0.0}\n",
    "        log_probs = torch.log(probs + 1e-8)\n",
    "\n",
    "        if debug:\n",
    "            print(\"\\nStarting beam search with beam size:\", beam_size)\n",
    "\n",
    "        for t, prob in enumerate(log_probs):\n",
    "            new_dp = defaultdict(lambda: float('-inf'))\n",
    "            top_k = torch.topk(prob, k=min(beam_size, len(prob)))\n",
    "\n",
    "            if debug and t < self.max_printed_samples:\n",
    "                print(f\"\\nTimestep {t}:\")\n",
    "                print(\"Top tokens:\", [(self.ind2char[idx.item()], val.item()) \n",
    "                                    for val, idx in zip(top_k.values, top_k.indices)])\n",
    "\n",
    "            for val, ind in zip(top_k.values, top_k.indices):\n",
    "                curr_char = self.ind2char[ind.item()]\n",
    "                next_token_log_prob = val.item()\n",
    "\n",
    "                for (prefix, last_char), log_prob in dp.items():\n",
    "                    if last_char == curr_char and curr_char != \" \":\n",
    "                        new_prefix = prefix\n",
    "                    else:\n",
    "                        if curr_char != self.blank_token:\n",
    "                            if curr_char == \" \" and prefix.endswith(\" \"):\n",
    "                                continue\n",
    "                            new_prefix = prefix + curr_char\n",
    "                        else:\n",
    "                            new_prefix = prefix\n",
    "\n",
    "                    new_log_prob = log_prob + next_token_log_prob\n",
    "                    key = (new_prefix, curr_char)\n",
    "                    new_dp[key] = max(new_dp[key], new_log_prob)\n",
    "\n",
    "            if len(new_dp) > 0:\n",
    "                max_score = max(score for _, score in new_dp.items())\n",
    "                new_dp = {key: score - max_score for key, score in new_dp.items()}\n",
    "\n",
    "            dp = dict(sorted(new_dp.items(), key=lambda x: -x[1])[:beam_size])\n",
    "\n",
    "            if debug and t < 2:\n",
    "                print(\"\\nCurrent beam:\")\n",
    "                for (text, last_char), score in list(dp.items())[:3]:\n",
    "                    print(f\"Text: '{text}', Last: '{last_char}', Score: {score:.4f}\")\n",
    "\n",
    "        final_beams = []\n",
    "        for (text, _), score in dp.items():\n",
    "            if self.use_bpe and self.processor:\n",
    "                text = self.processor.tokenizer.clean_up_tokenization(text)\n",
    "            text = text.lower().strip()\n",
    "            text_len = max(1, len(text.split())) if self.use_bpe else max(1, len(text))\n",
    "            normalized_score = score / text_len\n",
    "            final_beams.append((text, normalized_score))\n",
    "\n",
    "        final_beams.sort(key=lambda x: -x[1])\n",
    "        if not final_beams:\n",
    "            final_beams = [(\"\", float('-inf'))]\n",
    "\n",
    "        return final_beams[:beam_size]\n",
    "\n",
    "    def test_language_model(self):\n",
    "        \"\"\"Debug function to verify LM functionality\"\"\"\n",
    "        print(\"\\nTesting Language Model...\")\n",
    "\n",
    "        if self.lm is None:\n",
    "            print(\"Error: Language model is not loaded!\")\n",
    "            return\n",
    "\n",
    "        test_sentences = [\n",
    "            \"this is a good sentence\",\n",
    "            \"this is also a good sentence\",\n",
    "            \"thiss iss nott aa goodd sentencee\",\n",
    "            \"random word salad box cat\",\n",
    "            \"the cat sat on the mat\",\n",
    "            \"\",\n",
    "            \"a\",\n",
    "        ]\n",
    "\n",
    "        print(\"\\nTesting individual sentences:\")\n",
    "        for sentence in test_sentences:\n",
    "            score = self.score_with_lm(sentence)\n",
    "            print(f\"\\nText: '{sentence}'\")\n",
    "            print(f\"LM Score: {score:.4f}\")\n",
    "\n",
    "        test_prefixes = [\n",
    "            \"the quick brown\",\n",
    "            \"how are\",\n",
    "            \"thank\",\n",
    "            \"nice to\",\n",
    "        ]\n",
    "\n",
    "        print(\"\\nTesting word completions:\")\n",
    "        for prefix in test_prefixes:\n",
    "            print(f\"\\nPrefix: '{prefix}'\")\n",
    "            completions = [\n",
    "                prefix + \" \" + word for word in [\"you\", \"fox\", \"cat\", \"xyz\", \"meet\"]\n",
    "            ]\n",
    "            scores = [(completion, self.score_with_lm(completion)) \n",
    "                    for completion in completions]\n",
    "            scores.sort(key=lambda x: x[1], reverse=True)\n",
    "            print(\"Top completions by score:\")\n",
    "            for completion, score in scores[:3]:\n",
    "                print(f\"  '{completion}': {score:.4f}\")\n",
    "\n",
    "    def score_with_lm(self, text: str) -> float:\n",
    "        \"\"\"\n",
    "        Score text using language model, handling edge cases.\n",
    "\n",
    "        Parameters:\n",
    "        - text (str): The input text to score.\n",
    "\n",
    "        Returns:\n",
    "        - float: LM score.\n",
    "        \"\"\"\n",
    "        if self.lm is None:\n",
    "            return 0.0\n",
    "\n",
    "        if not text or len(text.strip()) == 0:\n",
    "            return float('-inf')\n",
    "\n",
    "        text = text.lower().strip()\n",
    "        return self.lm.score(text, bos=True, eos=True)\n",
    "\n",
    "    def _basic_ctc_decode(self, logits: np.ndarray, sequence_length: int) -> List[str]:\n",
    "        \"\"\"\n",
    "        Basic CTC decoding without LM.\n",
    "\n",
    "        Parameters:\n",
    "        - logits (np.ndarray): Logits from the model.\n",
    "        - sequence_length (int): Length of the sequence to decode.\n",
    "\n",
    "        Returns:\n",
    "        - List[str]: Decoded text.\n",
    "        \"\"\"\n",
    "        argmax_indices = np.argmax(logits, axis=-1)\n",
    "\n",
    "        if len(argmax_indices.shape) == 0:\n",
    "            argmax_indices = np.array([argmax_indices])\n",
    "\n",
    "        if len(argmax_indices.shape) == 1:\n",
    "            argmax_indices = np.expand_dims(argmax_indices, axis=0)\n",
    "\n",
    "        predictions = []\n",
    "        for sequence in argmax_indices:\n",
    "            decoded = []\n",
    "            last_idx = None\n",
    "\n",
    "            for idx in sequence[:sequence_length]:\n",
    "                if idx != self.blank_index and idx != last_idx:\n",
    "                    decoded.append(self.ind2char[idx])\n",
    "                last_idx = idx\n",
    "\n",
    "            text = \"\".join(decoded)\n",
    "            if self.use_bpe and self.processor:\n",
    "                text = self.processor.tokenizer.clean_up_tokenization(text)\n",
    "            predictions.append(text)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_text(text: str) -> str:\n",
    "        \"\"\"Normalize input text.\"\"\"\n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"[^a-z ]\", \"\", text)\n",
    "        return text\n",
    "\n",
    "    def test_decoder(self, sample_text: str = \"test decoder functionality\"):\n",
    "        \"\"\"Test the decoder setup.\"\"\"\n",
    "        print(\"\\nTesting decoder configuration...\")\n",
    "\n",
    "        encoded = self.encode(sample_text)\n",
    "        decoded = self.decode(encoded[0].tolist())\n",
    "        print(f\"Original text: {sample_text}\")\n",
    "        print(f\"Basic decode: {decoded}\")\n",
    "\n",
    "        sequence_length = 50\n",
    "        vocab_size = len(self)\n",
    "        fake_logits = torch.randn(1, sequence_length, vocab_size)\n",
    "        fake_length = torch.tensor([sequence_length])\n",
    "\n",
    "        if self.decoder is not None:\n",
    "            print(\"\\nTesting pyctcdecode integration...\")\n",
    "            decoded_with_lm = self.ctc_decode(fake_logits)\n",
    "            print(f\"Decoded with LM: {decoded_with_lm}\")\n",
    "\n",
    "            print(f\"\\nBeam width: {self.beam_size}\")\n",
    "            print(f\"LM weight: {self.lm_weight}\")\n",
    "        else:\n",
    "            print(\"\\nNo language model loaded - using basic CTC decoding\")\n",
    "            basic_decoded = self._basic_ctc_decode(fake_logits.numpy(), sequence_length)\n",
    "            print(f\"Basic CTC decoded: {basic_decoded[0]}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the size of the vocabulary.\"\"\"\n",
    "        return len(self.vocab)\n",
    "\n",
    "    def ctc_decode(self, logits: Union[torch.Tensor, List[int], np.ndarray]) -> str:\n",
    "        \"\"\"\n",
    "        Perform CTC decoding on logits.\n",
    "\n",
    "        Parameters:\n",
    "        - logits (Union[torch.Tensor, List[int], np.ndarray]): Logits or token indices.\n",
    "\n",
    "        Returns:\n",
    "        - str: Decoded text.\n",
    "        \"\"\"\n",
    "        if isinstance(logits, np.ndarray):\n",
    "            logits = torch.from_numpy(logits)\n",
    "        elif isinstance(logits, list):\n",
    "            logits = torch.tensor(logits)\n",
    "\n",
    "        if logits.dim() == 3:\n",
    "            decoded_text = self.decode_logits(logits)\n",
    "            return decoded_text\n",
    "        elif logits.dim() == 2:\n",
    "            decoded_text = self.decode_logits(logits)\n",
    "            return decoded_text\n",
    "        elif logits.dim() == 1:\n",
    "            decoded_text = self.decode_indices(logits)\n",
    "            return decoded_text\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported logits shape: {logits.shape}. Expected 1D, 2D, or 3D.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import kenlm\n",
    "from transformers import Wav2Vec2Processor, AutoProcessor\n",
    "from pyctcdecode import build_ctcdecoder\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from typing import List, Tuple, Optional, Union\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "class CTCTextEncoder:\n",
    "    def __init__(\n",
    "        self,\n",
    "        arpa_path: Optional[str] = None,\n",
    "        binary_path: Optional[str] = None,\n",
    "        unigram_path: Optional[str] = None,\n",
    "        pretrained_tokenizer: str = \"facebook/wav2vec2-base-960h\",\n",
    "        lm_weight: float = 0.5,\n",
    "        beam_size: int = 100,\n",
    "        blank_token: str = \"<pad>\",  # Blank token as <pad> for Wav2Vec2\n",
    "        unk_token: str = \"<unk>\",     # UNK token\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize encoder with Wav2Vec2 tokenizer and beam search decoder.\n",
    "\n",
    "        Changes:\n",
    "        - Ensure normalization is strictly lowercase with only [a-z ].\n",
    "        \"\"\"\n",
    "        self.beam_size = beam_size\n",
    "        self.lm_weight = lm_weight\n",
    "        self.arpa_path = arpa_path\n",
    "        self.binary_path = binary_path\n",
    "        self.blank_token = blank_token\n",
    "        self.unk_token = unk_token\n",
    "        self.printed_samples = 0\n",
    "        self.max_printed_samples = 5\n",
    "        print('CTC Text Encoder:')\n",
    "        print('pretrained_tokenizer:', pretrained_tokenizer)\n",
    "        print('lm_weight:', lm_weight)\n",
    "        print('beam_size:', beam_size)\n",
    "        print('binary_path:', binary_path)\n",
    "\n",
    "        # unigram_path = None\n",
    "\n",
    "        # Load unigrams if provided\n",
    "        \n",
    "        self.unigrams = None\n",
    "        \n",
    "        # if unigram_path and os.path.exists(unigram_path):\n",
    "        #     print(f\"Loading unigrams from: {unigram_path}\")\n",
    "        #     with open(unigram_path, 'r', encoding='utf-8') as f:\n",
    "        #         self.unigrams = [line.strip().lower() for line in f if line.strip()]\n",
    "        #     print(f\"Loaded {len(self.unigrams)} unigrams\")\n",
    "        \n",
    "        if unigram_path and os.path.exists(unigram_path):\n",
    "            print(f\"Loading unigrams from: {unigram_path}\")\n",
    "            with open(unigram_path, 'r', encoding='utf-8') as f:\n",
    "                self.unigrams = [line.strip().lower() for line in f if line.strip()]\n",
    "            print(f\"Loaded {len(self.unigrams)} unigrams\")\n",
    "\n",
    "\n",
    "        self._initialize_wav2vec2_tokenizer(pretrained_tokenizer)\n",
    "\n",
    "        # Create index mappings\n",
    "        self.ind2char = dict(enumerate(self.vocab))\n",
    "        self.char2ind = {v: k for k, v in self.ind2char.items()}\n",
    "        self.blank_index = self.char2ind[self.blank_token]\n",
    "\n",
    "        print(f\"\\nVocabulary Info:\")\n",
    "        print(f\"Size: {len(self.vocab)}\")\n",
    "        print(\"Full Vocabulary (up to first 50 tokens):\", self.vocab[:50])\n",
    "        print(f\"Blank token: {self.blank_token}, Blank index: {self.blank_index}\")\n",
    "\n",
    "        print(\"Sample ind2char mappings:\", {k: self.ind2char[k] for k in list(self.ind2char.keys())[:10]})\n",
    "        print(\"Sample char2ind mappings:\", {k: self.char2ind[k] for k in list(self.char2ind.keys())[:10]})\n",
    "\n",
    "        self._initialize_language_model()\n",
    "\n",
    "    # def _initialize_wav2vec2_tokenizer(self, pretrained_tokenizer: str):\n",
    "    #     \"\"\"Initialize vocabulary using Wav2Vec2 tokenizer.\"\"\"\n",
    "    #     self.processor = Wav2Vec2Processor.from_pretrained(pretrained_tokenizer)\n",
    "\n",
    "    #     # Add the unique blank token if not present\n",
    "    #     if self.blank_token not in self.processor.tokenizer.get_vocab():\n",
    "    #         self.processor.tokenizer.add_tokens([self.blank_token])\n",
    "    #         print(f\"Added '{self.blank_token}' to the tokenizer's vocabulary.\")\n",
    "    #     else:\n",
    "    #         print(f\"'{self.blank_token}' already exists in the tokenizer's vocabulary.\")\n",
    "\n",
    "    #     # Add the UNK token if not present\n",
    "    #     if self.unk_token not in self.processor.tokenizer.get_vocab():\n",
    "    #         self.processor.tokenizer.add_tokens([self.unk_token])\n",
    "    #         print(f\"Added '{self.unk_token}' to the tokenizer's vocabulary.\")\n",
    "    #     else:\n",
    "    #         print(f\"'{self.unk_token}' already exists in the tokenizer's vocabulary.\")\n",
    "\n",
    "    #     # Get vocab, convert to lowercase and replace '|' with ' '\n",
    "    #     original_vocab = list(self.processor.tokenizer.get_vocab().keys())\n",
    "    #     self.vocab = \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    #     self.vocab = [t.replace('|', ' ') for t in self.vocab]\n",
    "\n",
    "    #     # Debug: Print a few tokens after modification\n",
    "    #     print(\"Modified Vocabulary (first 20 tokens):\", self.vocab[:20])\n",
    "    \n",
    "\n",
    "    def _initialize_wav2vec2_tokenizer(self, pretrained_tokenizer: str):\n",
    "        \"\"\"Initialize vocabulary using Wav2Vec2 tokenizer.\"\"\"\n",
    "        # Initialize Wav2Vec2Processor\n",
    "        # self.processor = Wav2Vec2Processor.from_pretrained(pretrained_tokenizer)\n",
    "\n",
    "        \n",
    "\n",
    "        # # Add the UNK token if not present\n",
    "        # if self.unk_token not in self.processor.tokenizer.get_vocab():\n",
    "        #     self.processor.tokenizer.add_tokens([self.unk_token])\n",
    "        #     print(f\"Added '{self.unk_token}' to the tokenizer's vocabulary.\")\n",
    "        # else:\n",
    "        #     print(f\"'{self.unk_token}' already exists in the tokenizer's vocabulary.\")\n",
    "\n",
    "        # # Get vocab without altering the case\n",
    "        # self.vocab = list(self.processor.tokenizer.get_vocab().keys())\n",
    "\n",
    "        # vocab_dict = processor.tokenizer.get_vocab()\n",
    "        # sorted_vocab_dict = {k.lower(): v for k, v in sorted(vocab_dict.items(), key=lambda item: item[1])}\n",
    "        # labels_adj = list(sorted_vocab_dict.keys())\n",
    "\n",
    "        # self.vocab = [t.replace('|', ' ') for t in self.vocab]\n",
    "\n",
    "        # # Debugging: Inspect the tokenizer's vocabulary casing\n",
    "        # print(\"\\n--- Tokenizer Vocabulary Inspection ---\")\n",
    "        # sample_size = 100  # Adjust as needed\n",
    "        # sample_tokens = self.vocab[:sample_size]\n",
    "        # print(f\"First {sample_size} tokens in vocabulary:\")\n",
    "        # print(sample_tokens)\n",
    "\n",
    "        # # Save the full tokenizer vocabulary to a file for comparison\n",
    "        # with open(\"tokenizer_vocab.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        #     for token in self.vocab:\n",
    "        #         f.write(f\"{token}\\n\")\n",
    "        # print(\"Full tokenizer vocabulary saved to 'tokenizer_vocab.txt'.\")\n",
    "        # print(\"----------------------------------------\\n\")\n",
    "        \n",
    "\n",
    "        from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "\n",
    "        self.processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\") # 100h\n",
    "\n",
    "        # self.processor = AutoProcessor.from_pretrained(\"hf-test/xls-r-300m-sv\")\n",
    "\n",
    "        vocab_dict = self.processor.tokenizer.get_vocab()\n",
    "        # sorted_vocab_dict = {k.lower(): v for k, v in sorted(vocab_dict.items(), key=lambda item: item[1])}\n",
    "        sorted_vocab_dict = {k: v for k, v in sorted(vocab_dict.items(), key=lambda item: item[1])}\n",
    "\n",
    "        sorted_vocab_dict_for_labels= {k.lower(): v for k, v in sorted(vocab_dict.items(), key=lambda item: item[1])}\n",
    "        self.labels = list(sorted_vocab_dict_for_labels.keys())\n",
    "\n",
    "        # Get vocab, convert to lowercase and replace '|' with ' '\n",
    "        original_vocab = list(self.processor.tokenizer.get_vocab().keys())\n",
    "        # self.vocab = [x.lower() for x in original_vocab]\n",
    "        self.vocab = sorted_vocab_dict\n",
    "        self.vocab = [t.replace('|', ' ') for t in self.vocab]\n",
    "\n",
    "    \n",
    "        # Add the unique blank token to the tokenizer's vocabulary if not present\n",
    "        if self.blank_token not in self.processor.tokenizer.get_vocab():\n",
    "            self.processor.tokenizer.add_tokens([self.blank_token])\n",
    "            print(f\"Added '{self.blank_token}' to the tokenizer's vocabulary.\")\n",
    "        else:\n",
    "            print(f\"'{self.blank_token}' already exists in the tokenizer's vocabulary.\")\n",
    "\n",
    "        # Debug: Print a few tokens after modification\n",
    "        print(\"Modified Vocabulary:\", self.vocab)\n",
    "\n",
    "\n",
    "    def _initialize_language_model(self):\n",
    "        \"\"\"Initialize language model with explicit blank token handling.\"\"\"\n",
    "        self.lm = None\n",
    "        self.decoder = None\n",
    "\n",
    "        model_path = self.binary_path if self.binary_path else self.arpa_path\n",
    "        print('model_path: ', model_path)\n",
    "        if not model_path or not os.path.exists(model_path):\n",
    "            print(\"No language model path provided or file does not exist.\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            self.lm = kenlm.Model(model_path)\n",
    "            print(f\"Loaded {'binary' if self.binary_path else 'ARPA'} language model.\")\n",
    "\n",
    "            # labels = [self.blank_token] + [c for c in self.vocab if c != self.blank_token]\n",
    "\n",
    "            decoder_config = {\n",
    "                \"labels\": self.labels,\n",
    "                \"kenlm_model_path\": model_path,\n",
    "                \"alpha\": self.lm_weight,\n",
    "                \"beta\": 0.1,\n",
    "                \"unk_score_offset\": -10.0,\n",
    "            }\n",
    "\n",
    "            if self.unigrams:\n",
    "                print(\"\\n--- Unigrams List ---\")\n",
    "                # Save the unigrams to a file\n",
    "                with open(\"unigrams_list.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "                    for unigram in self.unigrams:\n",
    "                        f.write(f\"{unigram}\\n\")\n",
    "                print(f\"Unigrams list saved to 'unigrams_list.txt'. Total unigrams: {len(self.unigrams)}\")\n",
    "                print(\"----------------------\\n\")\n",
    "                decoder_config[\"unigrams\"] = self.unigrams\n",
    "\n",
    "            self.decoder = build_ctcdecoder(**decoder_config)\n",
    "            print(\"Successfully initialized language model and decoder.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Failed to initialize decoder: {str(e)}\")\n",
    "            self.decoder = None\n",
    "\n",
    "\n",
    "    def encode(self, text: str) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Encode text with Wav2Vec2 tokenizer.\n",
    "        \"\"\"\n",
    "        debug = False\n",
    "\n",
    "        if self.printed_samples < self.max_printed_samples:\n",
    "            original_text = text\n",
    "            text = self.normalize_text(text)\n",
    "            if debug:\n",
    "                print(f\"samples: {str(self.printed_samples)}\")\n",
    "                print(f\"\\nEncoding text:\")\n",
    "                print(f\" Original: '{original_text}'\")\n",
    "                print(f\" Normalized: '{text}'\")\n",
    "                for ch in text:\n",
    "                    print(ch, ord(ch))\n",
    "\n",
    "\n",
    "        try:\n",
    "            text = text.upper()\n",
    "            encoded = self.processor.tokenizer(text, return_tensors=\"pt\", padding=False, truncation=False)\n",
    "            token_indices = encoded.input_ids[0].tolist()\n",
    "            if self.printed_samples < self.max_printed_samples:\n",
    "                # Convert indices to tokens from self.vocab\n",
    "                tokens = [self.vocab[idx] if 0 <= idx < len(self.vocab) else \"<invalid>\" for idx in token_indices]\n",
    "                # print(f\" Tokens (lowercased and '|'->' '): {tokens}\")\n",
    "                # print(f\" Token indices: {token_indices}\")\n",
    "                self.printed_samples += 1\n",
    "            return torch.tensor(token_indices).unsqueeze(0)\n",
    "        except KeyError as e:\n",
    "            unknown_tokens = set([token for token in text.split() if token not in self.char2ind])\n",
    "            raise Exception(f\"Unknown tokens: '{' '.join(unknown_tokens)}'\")\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Encoding error: {str(e)}\")\n",
    "\n",
    "    # latest ver\n",
    "    # def decode(self, indices: List[int]) -> str:\n",
    "    #     \"\"\"\n",
    "    #     Decode indices to text using beam search decoder if available.\n",
    "    #     \"\"\"\n",
    "    #     if self.decoder:\n",
    "    #         decoded_text = self.decoder.decode(indices)\n",
    "    #         # convert to lower case\n",
    "    #         decoded_text = decoded_text.lower()\n",
    "    #         return decoded_text\n",
    "    #     else:\n",
    "    #         decoded_text = self.decode_simple(indices)\n",
    "    #         # convert to lower case\n",
    "    #         decoded_text = decoded_text.lower()\n",
    "    #         return\n",
    "    \n",
    "    def decode(self, indices: List[int]) -> str:\n",
    "        \"\"\"\n",
    "        Decode indices to text using beam search decoder if available.\n",
    "        \"\"\"\n",
    "        if self.decoder:\n",
    "            decoded_text = self.decoder.decode(indices)\n",
    "            # Convert to lower case\n",
    "            decoded_text = decoded_text.lower()\n",
    "            return decoded_text\n",
    "        else:\n",
    "            decoded_text = self.decode_simple(indices)\n",
    "            # Convert to lower case\n",
    "            decoded_text = decoded_text.lower()\n",
    "            return decoded_text  # Ensure the decoded text is returned\n",
    "\n",
    "    # latest ver\n",
    "    # def decode_simple(self, indices: List[int]) -> str:\n",
    "    #     \"\"\"\n",
    "    #     Simple CTC decoding without language model.\n",
    "    #     \"\"\"\n",
    "    #     valid_indices = [\n",
    "    #         idx for idx in indices\n",
    "    #         if idx != self.blank_index and 0 <= idx < len(self.ind2char)\n",
    "    #     ]\n",
    "    #     try:\n",
    "    #         tokens = [self.ind2char[idx] for idx in valid_indices]\n",
    "    #         text = \" \".join(tokens).strip().lower()\n",
    "    #         return self.processor.tokenizer.clean_up_tokenization(text)\n",
    "    #     except KeyError as e:\n",
    "    #         return \" \".join([self.ind2char[idx] for idx in valid_indices if idx in self.ind2char])\n",
    "\n",
    "    def decode_simple(self, indices: List[int]) -> str:\n",
    "        \"\"\"\n",
    "        Simple CTC decoding without language model.\n",
    "        Collapses consecutive duplicate tokens and removes blanks.\n",
    "        \"\"\"\n",
    "        decoded_chars = []\n",
    "        previous_idx = None\n",
    "\n",
    "        for idx in indices:\n",
    "            if idx == self.blank_index:\n",
    "                previous_idx = idx\n",
    "                continue  # Skip blank tokens\n",
    "            if idx == previous_idx:\n",
    "                continue  # Skip duplicate tokens\n",
    "            if 0 <= idx < len(self.ind2char):\n",
    "                char = self.ind2char[idx]\n",
    "                decoded_chars.append(char)\n",
    "            previous_idx = idx\n",
    "\n",
    "        # Join characters without spaces and convert to lowercase\n",
    "        text = \"\".join(decoded_chars).strip().lower()\n",
    "\n",
    "        # Clean up tokenization using the tokenizer's method\n",
    "        return self.processor.tokenizer.clean_up_tokenization(text)\n",
    "\n",
    "\n",
    "    def decode_logits(self, logits: Union[torch.Tensor, List[List[float]], np.ndarray]) -> str:\n",
    "        \"\"\"\n",
    "        Decode logits using the decoder if available, otherwise use greedy decoding.\n",
    "        \"\"\"\n",
    "        if isinstance(logits, torch.Tensor):\n",
    "            logits = logits.cpu().numpy()\n",
    "        elif isinstance(logits, list):\n",
    "            logits = np.array(logits)\n",
    "        elif not isinstance(logits, np.ndarray):\n",
    "            raise TypeError(\"logits must be a torch.Tensor, list of lists, or numpy.ndarray\")\n",
    "\n",
    "        if logits.ndim == 3:\n",
    "            logits = logits[0]\n",
    "\n",
    "        if logits.ndim != 2:\n",
    "            raise ValueError(f\"Logits should be 2D (time_steps, vocab_size), got {logits.ndim}D\")\n",
    "\n",
    "        if self.decoder:\n",
    "            decoded_text = self.decoder.decode(logits)\n",
    "            return decoded_text\n",
    "        else:\n",
    "            predicted_indices = np.argmax(logits, axis=-1).tolist()\n",
    "            return self.decode_simple(predicted_indices)\n",
    "\n",
    "    def decode_indices(self, indices: Union[torch.Tensor, List[int], np.ndarray]) -> str:\n",
    "        \"\"\"\n",
    "        Decode token indices to text using simple decoding (no LM).\n",
    "        \"\"\"\n",
    "        if isinstance(indices, torch.Tensor):\n",
    "            indices = indices.squeeze().tolist()\n",
    "        elif isinstance(indices, np.ndarray):\n",
    "            indices = indices.tolist()\n",
    "        elif not isinstance(indices, list):\n",
    "            raise TypeError(\"decode_indices expects a list, torch.Tensor, or numpy.ndarray.\")\n",
    "\n",
    "        return self.decode_simple(indices)\n",
    "\n",
    "\n",
    "    # latest ver\n",
    "    def ctc_decode(self, logits: Union[torch.Tensor, List[int], np.ndarray]) -> str:\n",
    "        \"\"\"\n",
    "        Perform CTC decoding on logits.\n",
    "        \"\"\"\n",
    "        if isinstance(logits, np.ndarray):\n",
    "            logits = torch.from_numpy(logits)\n",
    "        elif isinstance(logits, list):\n",
    "            logits = torch.tensor(logits)\n",
    "\n",
    "        if logits.dim() == 3:\n",
    "            decoded_text = self.decode_logits(logits)\n",
    "            return decoded_text.lower()\n",
    "        elif logits.dim() == 2:\n",
    "            decoded_text = self.decode_logits(logits)\n",
    "            return decoded_text.lower()\n",
    "        elif logits.dim() == 1:\n",
    "            decoded_text = self.decode_indices(logits)\n",
    "            return decoded_text.lower()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported logits shape: {logits.shape}. Expected 1D, 2D, or 3D.\")\n",
    "\n",
    "    def ctc_beam_search(self, probs, beam_size: int = 40,\n",
    "                       use_lm: bool = False, debug: bool = False) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Beam search with Wav2Vec2 support.\n",
    "        \"\"\"\n",
    "        beam_size = self.beam_size\n",
    "        debug = False\n",
    "\n",
    "        if use_lm and self.decoder is not None:\n",
    "            try:\n",
    "                if isinstance(probs, torch.Tensor):\n",
    "                    probs = probs.cpu().numpy()\n",
    "                elif isinstance(probs, list):\n",
    "                    probs = np.array(probs)\n",
    "                elif isinstance(probs, np.ndarray):\n",
    "                    pass\n",
    "                else:\n",
    "                    raise TypeError(\"probs must be a torch.Tensor, list, or numpy.ndarray\")\n",
    "\n",
    "                beams = self.decoder.decode_beams(\n",
    "                    probs,\n",
    "                    beam_prune_logp=-10.0,\n",
    "                    token_min_logp=-5.0,\n",
    "                    hotwords=[],\n",
    "                    hotword_weight=10.0,\n",
    "                )\n",
    "\n",
    "                formatted_beams = []\n",
    "                for beam in beams[:beam_size]:\n",
    "                    text = beam[0]\n",
    "                    acoustic_score = beam[3]\n",
    "                    lm_score = beam[4]\n",
    "\n",
    "                    text = self.processor.tokenizer.clean_up_tokenization(text)\n",
    "                    text = text.lower().strip()\n",
    "\n",
    "                    combined_score = (1 - self.lm_weight) * acoustic_score + self.lm_weight * lm_score\n",
    "                    text_len = max(1, len(text.split()))\n",
    "                    normalized_score = combined_score / text_len\n",
    "\n",
    "                    formatted_beams.append((text, normalized_score))\n",
    "\n",
    "                if debug:\n",
    "                    print(\"\\nFormatted beam results with Wav2Vec2:\")\n",
    "                    for text, score in formatted_beams[:3]:\n",
    "                        print(f\"Text: '{text}', Score: {score:.4f}\")\n",
    "\n",
    "                if formatted_beams:\n",
    "                    return sorted(formatted_beams, key=lambda x: -x[1])\n",
    "                else:\n",
    "                    print(\"No valid beams found, falling back to standard beam search\")\n",
    "                    return self._standard_beam_search(probs, beam_size, debug)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Beam search with LM failed: {str(e)}, falling back to standard beam search\")\n",
    "                return self._standard_beam_search(probs, beam_size, debug)\n",
    "        else:\n",
    "            return self._standard_beam_search(probs, beam_size, debug)\n",
    "\n",
    "    def _standard_beam_search(self, probs, beam_size: int = 10, debug: bool = False) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Original beam search implementation with improved debugging\"\"\"\n",
    "        beam_size = self.beam_size\n",
    "\n",
    "        if isinstance(probs, np.ndarray):\n",
    "            probs = torch.from_numpy(probs)\n",
    "\n",
    "        if probs.device != torch.device('cpu'):\n",
    "            probs = probs.cpu()\n",
    "\n",
    "        dp = {(\"\", self.blank_token): 0.0}\n",
    "        log_probs = torch.log(probs + 1e-8)\n",
    "\n",
    "        if debug:\n",
    "            print(\"\\nStarting beam search with beam size:\", beam_size)\n",
    "\n",
    "        for t, prob in enumerate(log_probs):\n",
    "            new_dp = defaultdict(lambda: float('-inf'))\n",
    "            top_k = torch.topk(prob, k=min(beam_size, len(prob)))\n",
    "\n",
    "            if debug and t < self.max_printed_samples:\n",
    "                print(f\"\\nTimestep {t}:\")\n",
    "                print(\"Top tokens:\", [(self.ind2char[idx.item()], val.item()) \n",
    "                                    for val, idx in zip(top_k.values, top_k.indices)])\n",
    "\n",
    "            for val, ind in zip(top_k.values, top_k.indices):\n",
    "                curr_char = self.ind2char[ind.item()]\n",
    "                next_token_log_prob = val.item()\n",
    "\n",
    "                for (prefix, last_char), log_prob in dp.items():\n",
    "                    if last_char == curr_char and curr_char != \" \":\n",
    "                        new_prefix = prefix\n",
    "                    else:\n",
    "                        if curr_char != self.blank_token:\n",
    "                            if curr_char == \" \" and prefix.endswith(\" \"):\n",
    "                                continue\n",
    "                            new_prefix = prefix + curr_char\n",
    "                        else:\n",
    "                            new_prefix = prefix\n",
    "\n",
    "                    new_log_prob = log_prob + next_token_log_prob\n",
    "                    key = (new_prefix, curr_char)\n",
    "                    new_dp[key] = max(new_dp[key], new_log_prob)\n",
    "\n",
    "            if len(new_dp) > 0:\n",
    "                max_score = max(score for _, score in new_dp.items())\n",
    "                new_dp = {key: score - max_score for key, score in new_dp.items()}\n",
    "\n",
    "            dp = dict(sorted(new_dp.items(), key=lambda x: -x[1])[:beam_size])\n",
    "\n",
    "            if debug and t < 2:\n",
    "                print(\"\\nCurrent beam:\")\n",
    "                for (text, last_char), score in list(dp.items())[:3]:\n",
    "                    print(f\"Text: '{text}', Last: '{last_char}', Score: {score:.4f}\")\n",
    "\n",
    "        final_beams = []\n",
    "        for (text, _), score in dp.items():\n",
    "            text = self.processor.tokenizer.clean_up_tokenization(text)\n",
    "            text = text.lower().strip()\n",
    "            text_len = max(1, len(text.split()))\n",
    "            normalized_score = score / text_len\n",
    "            final_beams.append((text, normalized_score))\n",
    "\n",
    "        final_beams.sort(key=lambda x: -x[1])\n",
    "        if not final_beams:\n",
    "            final_beams = [(\"\", float('-inf'))]\n",
    "\n",
    "        return final_beams[:beam_size]\n",
    "\n",
    "    def test_language_model(self):\n",
    "        \"\"\"Debug function to verify LM functionality\"\"\"\n",
    "        print(\"\\nTesting Language Model...\")\n",
    "\n",
    "        if self.lm is None:\n",
    "            print(\"Error: Language model is not loaded!\")\n",
    "            return\n",
    "\n",
    "        test_sentences = [\n",
    "            \"this is a good sentence\",\n",
    "            \"this is also a good sentence\",\n",
    "            \"thiss iss nott aa goodd sentencee\",\n",
    "            \"random word salad box cat\",\n",
    "            \"the cat sat on the mat\",\n",
    "            \"\",\n",
    "            \"a\",\n",
    "        ]\n",
    "\n",
    "        print(\"\\nTesting individual sentences:\")\n",
    "        for sentence in test_sentences:\n",
    "            score = self.score_with_lm(sentence)\n",
    "            print(f\"\\nText: '{sentence}'\")\n",
    "            print(f\"LM Score: {score:.4f}\")\n",
    "\n",
    "        test_prefixes = [\n",
    "            \"the quick brown\",\n",
    "            \"how are\",\n",
    "            \"thank\",\n",
    "            \"nice to\",\n",
    "        ]\n",
    "\n",
    "        print(\"\\nTesting word completions:\")\n",
    "        for prefix in test_prefixes:\n",
    "            print(f\"\\nPrefix: '{prefix}'\")\n",
    "            completions = [\n",
    "                prefix + \" \" + word for word in [\"you\", \"fox\", \"cat\", \"xyz\", \"meet\"]\n",
    "            ]\n",
    "            scores = [(completion, self.score_with_lm(completion)) \n",
    "                    for completion in completions]\n",
    "            scores.sort(key=lambda x: x[1], reverse=True)\n",
    "            print(\"Top completions by score:\")\n",
    "            for completion, score in scores[:3]:\n",
    "                print(f\"  '{completion}': {score:.4f}\")\n",
    "\n",
    "    def score_with_lm(self, text: str) -> float:\n",
    "        \"\"\"\n",
    "        Score text using language model, handling edge cases\n",
    "        \"\"\"\n",
    "        if self.lm is None:\n",
    "            return 0.0\n",
    "\n",
    "        if not text or len(text.strip()) == 0:\n",
    "            return float('-inf')\n",
    "\n",
    "        text = text.lower().strip()\n",
    "        return self.lm.score(text, bos=True, eos=True)\n",
    "\n",
    "    def _basic_ctc_decode(self, logits: np.ndarray, sequence_length: int) -> List[str]:\n",
    "        \"\"\"Basic CTC decoding without LM\"\"\"\n",
    "        argmax_indices = np.argmax(logits, axis=-1)\n",
    "\n",
    "        if len(argmax_indices.shape) == 0:\n",
    "            argmax_indices = np.array([argmax_indices])\n",
    "\n",
    "        if len(argmax_indices.shape) == 1:\n",
    "            argmax_indices = np.expand_dims(argmax_indices, axis=0)\n",
    "\n",
    "        predictions = []\n",
    "        for sequence in argmax_indices:\n",
    "            decoded = []\n",
    "            last_idx = None\n",
    "\n",
    "            for idx in sequence[:sequence_length]:\n",
    "                if idx != self.blank_index and idx != last_idx:\n",
    "                    decoded.append(self.ind2char[idx])\n",
    "                last_idx = idx\n",
    "\n",
    "            text = \"\".join(decoded)\n",
    "            if hasattr(self, 'processor'):\n",
    "                text = self.processor.tokenizer.clean_up_tokenization(text)\n",
    "            predictions.append(text)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_text(text: str) -> str:\n",
    "        \"\"\"Normalize input text\"\"\"\n",
    "        # text = text.lower()\n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"[^a-z ]\", \"\", text)\n",
    "        # text = re.sub(r\"[^A-Z ]\", \"\", text)\n",
    "        return text\n",
    "\n",
    "\n",
    "    def test_decoder(self, sample_text: str = \"test decoder functionality\"):\n",
    "        \"\"\"Test the decoder setup\"\"\"\n",
    "        print(\"\\nTesting decoder configuration...\")\n",
    "\n",
    "        encoded = self.encode(sample_text)\n",
    "        decoded = self.decode(encoded[0].tolist())\n",
    "        print(f\"Original text: {sample_text}\")\n",
    "        print(f\"Basic decode: {decoded}\")\n",
    "\n",
    "        sequence_length = 50\n",
    "        vocab_size = len(self)\n",
    "        fake_logits = torch.randn(1, sequence_length, vocab_size)\n",
    "        fake_length = torch.tensor([sequence_length])\n",
    "\n",
    "        if self.decoder is not None:\n",
    "            print(\"\\nTesting pyctcdecode integration...\")\n",
    "            decoded_with_lm = self.ctc_decode(fake_logits)\n",
    "            print(f\"Decoded with LM: {decoded_with_lm}\")\n",
    "\n",
    "            print(f\"\\nBeam width: {self.beam_size}\")\n",
    "            print(f\"LM weight: {self.lm_weight}\")\n",
    "        else:\n",
    "            print(\"\\nNo language model loaded - using basic CTC decoding\")\n",
    "            basic_decoded = self._basic_ctc_decode(fake_logits.numpy(), fake_length)\n",
    "            print(f\"Basic CTC decoded: {basic_decoded[0]}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTC Text Encoder:\n",
      "pretrained_tokenizer: facebook/wav2vec2-base-960h\n",
      "lm_weight: 0.5\n",
      "beam_size: 100\n",
      "binary_path: None\n",
      "'<pad>' already exists in the tokenizer's vocabulary.\n",
      "Modified Vocabulary: ['<pad>', '<s>', '</s>', '<unk>', ' ', 'E', 'T', 'A', 'O', 'N', 'I', 'H', 'S', 'R', 'D', 'L', 'U', 'M', 'W', 'C', 'F', 'G', 'Y', 'P', 'B', 'V', 'K', \"'\", 'X', 'J', 'Q', 'Z']\n",
      "\n",
      "Vocabulary Info:\n",
      "Size: 32\n",
      "Full Vocabulary (up to first 50 tokens): ['<pad>', '<s>', '</s>', '<unk>', ' ', 'E', 'T', 'A', 'O', 'N', 'I', 'H', 'S', 'R', 'D', 'L', 'U', 'M', 'W', 'C', 'F', 'G', 'Y', 'P', 'B', 'V', 'K', \"'\", 'X', 'J', 'Q', 'Z']\n",
      "Blank token: <pad>, Blank index: 0\n",
      "Sample ind2char mappings: {0: '<pad>', 1: '<s>', 2: '</s>', 3: '<unk>', 4: ' ', 5: 'E', 6: 'T', 7: 'A', 8: 'O', 9: 'N'}\n",
      "Sample char2ind mappings: {'<pad>': 0, '<s>': 1, '</s>': 2, '<unk>': 3, ' ': 4, 'E': 5, 'T': 6, 'A': 7, 'O': 8, 'N': 9}\n",
      "model_path:  None\n",
      "No language model path provided or file does not exist.\n",
      "CTC Text Encoder:\n",
      "pretrained_tokenizer: facebook/wav2vec2-base-960h\n",
      "lm_weight: 0.5\n",
      "beam_size: 100\n",
      "binary_path: None\n",
      "'<pad>' already exists in the tokenizer's vocabulary.\n",
      "Modified Vocabulary: ['<pad>', '<s>', '</s>', '<unk>', ' ', 'E', 'T', 'A', 'O', 'N', 'I', 'H', 'S', 'R', 'D', 'L', 'U', 'M', 'W', 'C', 'F', 'G', 'Y', 'P', 'B', 'V', 'K', \"'\", 'X', 'J', 'Q', 'Z']\n",
      "\n",
      "Vocabulary Info:\n",
      "Size: 32\n",
      "Full Vocabulary (up to first 50 tokens): ['<pad>', '<s>', '</s>', '<unk>', ' ', 'E', 'T', 'A', 'O', 'N', 'I', 'H', 'S', 'R', 'D', 'L', 'U', 'M', 'W', 'C', 'F', 'G', 'Y', 'P', 'B', 'V', 'K', \"'\", 'X', 'J', 'Q', 'Z']\n",
      "Blank token: <pad>, Blank index: 0\n",
      "Sample ind2char mappings: {0: '<pad>', 1: '<s>', 2: '</s>', 3: '<unk>', 4: ' ', 5: 'E', 6: 'T', 7: 'A', 8: 'O', 9: 'N'}\n",
      "Sample char2ind mappings: {'<pad>': 0, '<s>': 1, '</s>': 2, '<unk>': 3, ' ': 4, 'E': 5, 'T': 6, 'A': 7, 'O': 8, 'N': 9}\n",
      "model_path:  None\n",
      "No language model path provided or file does not exist.\n",
      "CTC Text Encoder:\n",
      "pretrained_tokenizer: facebook/wav2vec2-base-960h\n",
      "lm_weight: 0.5\n",
      "beam_size: 100\n",
      "binary_path: None\n",
      "'<pad>' already exists in the tokenizer's vocabulary.\n",
      "Modified Vocabulary: ['<pad>', '<s>', '</s>', '<unk>', ' ', 'E', 'T', 'A', 'O', 'N', 'I', 'H', 'S', 'R', 'D', 'L', 'U', 'M', 'W', 'C', 'F', 'G', 'Y', 'P', 'B', 'V', 'K', \"'\", 'X', 'J', 'Q', 'Z']\n",
      "\n",
      "Vocabulary Info:\n",
      "Size: 32\n",
      "Full Vocabulary (up to first 50 tokens): ['<pad>', '<s>', '</s>', '<unk>', ' ', 'E', 'T', 'A', 'O', 'N', 'I', 'H', 'S', 'R', 'D', 'L', 'U', 'M', 'W', 'C', 'F', 'G', 'Y', 'P', 'B', 'V', 'K', \"'\", 'X', 'J', 'Q', 'Z']\n",
      "Blank token: <pad>, Blank index: 0\n",
      "Sample ind2char mappings: {0: '<pad>', 1: '<s>', 2: '</s>', 3: '<unk>', 4: ' ', 5: 'E', 6: 'T', 7: 'A', 8: 'O', 9: 'N'}\n",
      "Sample char2ind mappings: {'<pad>': 0, '<s>': 1, '</s>': 2, '<unk>': 3, ' ': 4, 'E': 5, 'T': 6, 'A': 7, 'O': 8, 'N': 9}\n",
      "model_path:  path/to/lm.arpa\n",
      "No language model path provided or file does not exist.\n",
      "CTC Text Encoder:\n",
      "pretrained_tokenizer: facebook/wav2vec2-base-960h\n",
      "lm_weight: 0.5\n",
      "beam_size: 100\n",
      "binary_path: None\n",
      "'<pad>' already exists in the tokenizer's vocabulary.\n",
      "Modified Vocabulary: ['<pad>', '<s>', '</s>', '<unk>', ' ', 'E', 'T', 'A', 'O', 'N', 'I', 'H', 'S', 'R', 'D', 'L', 'U', 'M', 'W', 'C', 'F', 'G', 'Y', 'P', 'B', 'V', 'K', \"'\", 'X', 'J', 'Q', 'Z']\n",
      "\n",
      "Vocabulary Info:\n",
      "Size: 32\n",
      "Full Vocabulary (up to first 50 tokens): ['<pad>', '<s>', '</s>', '<unk>', ' ', 'E', 'T', 'A', 'O', 'N', 'I', 'H', 'S', 'R', 'D', 'L', 'U', 'M', 'W', 'C', 'F', 'G', 'Y', 'P', 'B', 'V', 'K', \"'\", 'X', 'J', 'Q', 'Z']\n",
      "Blank token: <pad>, Blank index: 0\n",
      "Sample ind2char mappings: {0: '<pad>', 1: '<s>', 2: '</s>', 3: '<unk>', 4: ' ', 5: 'E', 6: 'T', 7: 'A', 8: 'O', 9: 'N'}\n",
      "Sample char2ind mappings: {'<pad>': 0, '<s>': 1, '</s>': 2, '<unk>': 3, ' ': 4, 'E': 5, 'T': 6, 'A': 7, 'O': 8, 'N': 9}\n",
      "model_path:  path/to/lm.arpa\n",
      "No language model path provided or file does not exist.\n",
      "\n",
      "--- No BPE and No LM ---\n",
      "Encoded: tensor([[11,  5, 15, 15,  8,  4, 18,  8, 13, 15, 14]])\n",
      "Decoded: t\n",
      "\n",
      "--- Using BPE ---\n",
      "Encoded: tensor([[11,  5, 15, 15,  8,  4, 18,  8, 13, 15, 14]])\n",
      "Decoded: t\n",
      "\n",
      "--- Using LM ---\n",
      "Decoded with LM: qn</s>h</s>pqrh'apzmfwqtxgncdfwpse<s> mzpyk'hcrtyjzs\n",
      "\n",
      "--- Using BPE + LM ---\n",
      "Encoded: tensor([[11,  5, 15, 15,  8,  4, 18,  8, 13, 15, 14]])\n",
      "Decoded: qn</s>h</s>pqrh'apzmfwqtxgncdfwpse<s> mzpyk'hcrtyjzs\n"
     ]
    }
   ],
   "source": [
    "# Initialize the encoder with different configurations\n",
    "no_bpe_no_lm_encoder = CTCTextEncoder(use_bpe=False, use_lm=False)\n",
    "bpe_encoder = CTCTextEncoder(use_bpe=True, use_lm=False, pretrained_tokenizer=\"facebook/wav2vec2-base-960h\")\n",
    "lm_encoder = CTCTextEncoder(use_bpe=False, use_lm=True, arpa_path=\"path/to/lm.arpa\")\n",
    "bpe_lm_encoder = CTCTextEncoder(use_bpe=True, use_lm=True, arpa_path=\"path/to/lm.arpa\", pretrained_tokenizer=\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "# Example input text\n",
    "input_text = \"hello world\"\n",
    "\n",
    "# Fake logits for decoding test\n",
    "fake_logits = torch.randn(1, 50, len(no_bpe_no_lm_encoder.vocab))\n",
    "fake_logits = torch.log_softmax(fake_logits, dim=-1)\n",
    "\n",
    "print(\"\\n--- No BPE and No LM ---\")\n",
    "encoded_no_bpe = no_bpe_no_lm_encoder.encode(input_text)\n",
    "decoded_no_bpe = no_bpe_no_lm_encoder.ctc_decode(encoded_no_bpe)\n",
    "print(f\"Encoded: {encoded_no_bpe}\")\n",
    "print(f\"Decoded: {decoded_no_bpe}\")\n",
    "\n",
    "print(\"\\n--- Using BPE ---\")\n",
    "# input_text_for_enc = input_text.upper()\n",
    "encoded_bpe = bpe_encoder.encode(input_text)\n",
    "decoded_bpe = bpe_encoder.ctc_decode(encoded_bpe)\n",
    "decoded_bpe = decoded_bpe.lower()\n",
    "print(f\"Encoded: {encoded_bpe}\")\n",
    "print(f\"Decoded: {decoded_bpe}\")\n",
    "\n",
    "print(\"\\n--- Using LM ---\")\n",
    "decoded_lm = lm_encoder.ctc_decode(fake_logits)\n",
    "print(f\"Decoded with LM: {decoded_lm}\")\n",
    "\n",
    "print(\"\\n--- Using BPE + LM ---\")\n",
    "encoded_bpe_lm = bpe_lm_encoder.encode(input_text)\n",
    "decoded_bpe_lm = bpe_lm_encoder.ctc_decode(fake_logits)\n",
    "print(f\"Encoded: {encoded_bpe_lm}\")\n",
    "print(f\"Decoded: {decoded_bpe_lm}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
