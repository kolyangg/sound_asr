{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/sound_asr\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Downloading sentencepiece-0.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Preprocessing unigram file...\n",
      "[INFO] Preprocessed file saved to ./preprocessed_unigrams.txt\n",
      "[INFO] SentencePiece model saved to ./sentencepiece_model/librispeech_unigram_model.model and ./sentencepiece_model/librispeech_unigram_model.vocab\n",
      "Input Text: THIS IS AN EXAMPLE.\n",
      "Token IDs: [2, 1814, 2, 72, 2, 27, 2, 9119, 0]\n",
      "Tokens: ['▁', 'THIS', '▁', 'IS', '▁', 'AN', '▁', 'EXAMPLE', '.']\n",
      "Decoded Text: THIS IS AN EXAMPLE ⁇ \n",
      "First 50 tokens in vocab: ['<unk>\\t0\\n', '<pad>\\t0\\n', '▁\\t0\\n', 'E\\t-1.02664\\n', 'S\\t-2.36567\\n', \"'\\t-3.37786\\n\", 'D\\t-4.16412\\n', 'ING\\t-4.55007\\n', 'A\\t-4.61137\\n', 'Y\\t-4.67708\\n', 'T\\t-4.83491\\n', 'N\\t-4.92079\\n', 'L\\t-5.10186\\n', 'O\\t-5.15328\\n', 'ED\\t-5.15388\\n', 'R\\t-5.15845\\n', 'IN\\t-5.20676\\n', 'LY\\t-5.22307\\n', 'I\\t-5.32059\\n', 'UN\\t-5.52611\\n', 'ER\\t-5.68763\\n', 'M\\t-5.69725\\n', 'K\\t-5.96746\\n', 'TH\\t-6.08362\\n', 'AL\\t-6.10076\\n', 'MAN\\t-6.15482\\n', 'US\\t-6.19319\\n', 'AN\\t-6.20053\\n', 'NESS\\t-6.2134\\n', 'C\\t-6.21874\\n', 'B\\t-6.26467\\n', 'H\\t-6.26497\\n', 'G\\t-6.35667\\n', 'ON\\t-6.39152\\n', 'P\\t-6.43691\\n', 'Z\\t-6.5774\\n', 'TON\\t-6.62261\\n', 'MA\\t-6.70296\\n', 'AR\\t-6.72343\\n', 'F\\t-6.77772\\n', 'NA\\t-6.82191\\n', 'KA\\t-6.87335\\n', 'UM\\t-6.88978\\n', 'TA\\t-6.91962\\n', 'ERS\\t-6.93992\\n', 'CH\\t-6.94798\\n', 'LEY\\t-6.948\\n', 'CA\\t-6.95184\\n', 'LA\\t-6.96662\\n', 'V\\t-6.96788\\n']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ./preprocessed_unigrams.txt\n",
      "  input_format: \n",
      "  model_prefix: ./sentencepiece_model/librispeech_unigram_model\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 30000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 1\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  user_defined_symbols: ▁\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: -1\n",
      "  eos_id: -1\n",
      "  pad_id: 1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: identity\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: ./preprocessed_unigrams.txt\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 2000 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: ▁\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=1737588\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=27\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 2000 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=798875\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 230216 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 2000\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 2000\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 2000 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=95143 obj=4329.83 num_tokens=566899 num_tokens/piece=5.95839\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=82433 obj=4211.7 num_tokens=589363 num_tokens/piece=7.1496\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=61754 obj=4226.77 num_tokens=624396 num_tokens/piece=10.111\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=61456 obj=4207.71 num_tokens=625049 num_tokens/piece=10.1707\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=46091 obj=4208.76 num_tokens=654319 num_tokens/piece=14.1962\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=46085 obj=4193.3 num_tokens=654680 num_tokens/piece=14.2059\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=34563 obj=4230.93 num_tokens=685010 num_tokens/piece=19.8192\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=34562 obj=4254.65 num_tokens=685475 num_tokens/piece=19.8332\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=33000 obj=4194.64 num_tokens=690363 num_tokens/piece=20.9201\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=33000 obj=4209.71 num_tokens=690749 num_tokens/piece=20.9318\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: ./sentencepiece_model/librispeech_unigram_model.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: ./sentencepiece_model/librispeech_unigram_model.vocab\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "import os\n",
    "\n",
    "# Path to the unigram file and output directory\n",
    "unigram_file = \"librispeech-vocab.txt\"  # Replace with your unigram file path\n",
    "output_dir = \"./sentencepiece_model\"\n",
    "preprocessed_file = \"./preprocessed_unigrams.txt\"  # Temp file\n",
    "vocab_size = 30000  # Desired vocabulary size\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Preprocess the unigram file to create synthetic \"sentences\"\n",
    "print(\"[INFO] Preprocessing unigram file...\")\n",
    "with open(unigram_file, \"r\", encoding=\"utf-8\") as infile, open(preprocessed_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    batch_size = 100\n",
    "    batch = []\n",
    "    for line in infile:\n",
    "        word = line.strip()\n",
    "        if word:\n",
    "            batch.append(word)\n",
    "        if len(batch) >= batch_size:\n",
    "            outfile.write(\" \".join(batch) + \"\\n\")\n",
    "            batch = []\n",
    "    if batch:\n",
    "        outfile.write(\" \".join(batch) + \"\\n\")\n",
    "\n",
    "print(f\"[INFO] Preprocessed file saved to {preprocessed_file}\")\n",
    "\n",
    "# Train the SentencePiece model\n",
    "model_prefix = os.path.join(output_dir, \"librispeech_unigram_model\")\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input=preprocessed_file,\n",
    "    model_prefix=model_prefix,\n",
    "    vocab_size=vocab_size,\n",
    "    model_type=\"unigram\",\n",
    "    character_coverage=1.0,\n",
    "    bos_id=-1,\n",
    "    eos_id=-1,\n",
    "    unk_id=0,\n",
    "    pad_id=1,\n",
    "    user_defined_symbols=[\"▁\"],\n",
    "    shuffle_input_sentence=True,\n",
    "    normalization_rule_name=\"identity\"  # Disable normalization\n",
    ")\n",
    "\n",
    "print(f\"[INFO] SentencePiece model saved to {model_prefix}.model and {model_prefix}.vocab\")\n",
    "\n",
    "# Load and test the model\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(f\"{model_prefix}.model\")\n",
    "\n",
    "# Test encoding and decoding\n",
    "text = \"THIS IS AN EXAMPLE.\"  # Adjust case and spacing as needed\n",
    "token_ids = sp.encode(text, out_type=int)\n",
    "tokens = sp.encode(text, out_type=str)\n",
    "decoded_text = sp.decode(token_ids)\n",
    "\n",
    "print(f\"Input Text: {text}\")\n",
    "print(f\"Token IDs: {token_ids}\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Decoded Text: {decoded_text}\")\n",
    "\n",
    "# Inspect the vocab\n",
    "with open(f\"{model_prefix}.vocab\", \"r\", encoding=\"utf-8\") as f:\n",
    "    vocab = f.readlines()\n",
    "print(\"First 50 tokens in vocab:\", vocab[:50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Preprocessing unigram file...\n",
      "[INFO] Preprocessed file saved to ./preprocessed_unigrams.txt\n",
      "[INFO] SentencePiece model saved to ./sentencepiece_model/librispeech_unigram_model1000_new.model and ./sentencepiece_model/librispeech_unigram_model1000_new.vocab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ./preprocessed_unigrams.txt\n",
      "  input_format: \n",
      "  model_prefix: ./sentencepiece_model/librispeech_unigram_model1000_new\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 1000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 1\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  user_defined_symbols: .\n",
      "  user_defined_symbols: !\n",
      "  user_defined_symbols: ?\n",
      "  user_defined_symbols: ,\n",
      "  user_defined_symbols: '\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: -1\n",
      "  eos_id: -1\n",
      "  pad_id: 1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: identity\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: ./preprocessed_unigrams.txt\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 13256 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: .\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: !\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: ?\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: ,\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: '\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=24523638\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=27\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 13256 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=12441967\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 130970 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 13256\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 65987\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 65987 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=51285 obj=9.60683 num_tokens=123175 num_tokens/piece=2.40177\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=36958 obj=7.8114 num_tokens=123571 num_tokens/piece=3.34355\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=27715 obj=7.72961 num_tokens=129115 num_tokens/piece=4.65867\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=27699 obj=7.71437 num_tokens=129139 num_tokens/piece=4.66223\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=20773 obj=7.7365 num_tokens=143416 num_tokens/piece=6.90396\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=20773 obj=7.72704 num_tokens=143392 num_tokens/piece=6.90281\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=15579 obj=7.79453 num_tokens=160013 num_tokens/piece=10.2711\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=15579 obj=7.77807 num_tokens=159991 num_tokens/piece=10.2697\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=11684 obj=7.88858 num_tokens=176585 num_tokens/piece=15.1134\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=11684 obj=7.86467 num_tokens=176534 num_tokens/piece=15.109\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=8763 obj=8.02167 num_tokens=193130 num_tokens/piece=22.0393\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=8763 obj=7.99125 num_tokens=193104 num_tokens/piece=22.0363\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=6572 obj=8.18371 num_tokens=207689 num_tokens/piece=31.6021\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=6572 obj=8.14661 num_tokens=207638 num_tokens/piece=31.5943\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=4929 obj=8.38456 num_tokens=221543 num_tokens/piece=44.9468\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=4929 obj=8.34077 num_tokens=221555 num_tokens/piece=44.9493\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=3696 obj=8.61267 num_tokens=234303 num_tokens/piece=63.3937\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=3696 obj=8.56139 num_tokens=234250 num_tokens/piece=63.3793\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2772 obj=8.87995 num_tokens=246822 num_tokens/piece=89.0411\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2772 obj=8.82231 num_tokens=246838 num_tokens/piece=89.0469\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2079 obj=9.15895 num_tokens=258538 num_tokens/piece=124.357\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2079 obj=9.09731 num_tokens=258541 num_tokens/piece=124.358\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1559 obj=9.47426 num_tokens=269775 num_tokens/piece=173.044\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1559 obj=9.40756 num_tokens=269782 num_tokens/piece=173.048\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1169 obj=9.7921 num_tokens=281956 num_tokens/piece=241.194\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1169 obj=9.71792 num_tokens=281951 num_tokens/piece=241.19\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1100 obj=9.79929 num_tokens=285295 num_tokens/piece=259.359\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1100 obj=9.78322 num_tokens=285259 num_tokens/piece=259.326\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: ./sentencepiece_model/librispeech_unigram_model1000_new.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: ./sentencepiece_model/librispeech_unigram_model1000_new.vocab\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "import os\n",
    "\n",
    "# Path to the unigram file and output directory\n",
    "# unigram_file = \"librispeech-vocab.txt\"  # Replace with your unigram file path\n",
    "unigram_file = \"aggregated_result.txt\"  # Replace with your unigram file path\n",
    "output_dir = \"./sentencepiece_model\"\n",
    "preprocessed_file = \"./preprocessed_unigrams.txt\"  # Temp file for synthetic \"sentences\"\n",
    "# vocab_size = 10000  # Adjust to a smaller vocab size\n",
    "vocab_size = 1000  # Adjust to a smaller vocab size\n",
    "batch_size = 10  # Create more synthetic sentences by lowering batch size\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Preprocess the unigram file\n",
    "print(\"[INFO] Preprocessing unigram file...\")\n",
    "with open(unigram_file, \"r\", encoding=\"utf-8\") as infile, open(preprocessed_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    batch = []\n",
    "    for line in infile:\n",
    "        line = line.lower()\n",
    "        word = line.strip()\n",
    "        if word:\n",
    "            batch.append(word)\n",
    "        if len(batch) >= batch_size:\n",
    "            outfile.write(\" \".join(batch) + \"\\n\")\n",
    "            batch = []\n",
    "    if batch:\n",
    "        outfile.write(\" \".join(batch) + \"\\n\")\n",
    "\n",
    "print(f\"[INFO] Preprocessed file saved to {preprocessed_file}\")\n",
    "\n",
    "# Train the SentencePiece model\n",
    "model_prefix = os.path.join(output_dir, \"librispeech_unigram_model1000_new\")\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input=preprocessed_file,\n",
    "    model_prefix=model_prefix,\n",
    "    vocab_size=vocab_size,\n",
    "    model_type=\"unigram\",\n",
    "    character_coverage=1.0,\n",
    "    bos_id=-1,\n",
    "    eos_id=-1,\n",
    "    unk_id=0,\n",
    "    pad_id=1,\n",
    "    user_defined_symbols=[\".\", \"!\", \"?\", \",\", \"'\"],\n",
    "    shuffle_input_sentence=True,\n",
    "    normalization_rule_name=\"identity\"  # Disable normalization\n",
    ")\n",
    "\n",
    "print(f\"[INFO] SentencePiece model saved to {model_prefix}.model and {model_prefix}.vocab\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/sound_asr\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Preprocessing unigram file...\n",
      "[INFO] Preprocessed file saved to ./preprocessed_unigrams.txt\n",
      "[INFO] SentencePiece model saved to ./sentencepiece_model/librispeech_unigram_model1000_new2.model and ./sentencepiece_model/librispeech_unigram_model1000_new2.vocab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ./preprocessed_unigrams.txt\n",
      "  input_format: \n",
      "  model_prefix: ./sentencepiece_model/librispeech_unigram_model1000_new2\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 1000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 1\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: -1\n",
      "  eos_id: -1\n",
      "  pad_id: 1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: identity\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: ./preprocessed_unigrams.txt\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 13256 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=24480188\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=27\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 13256 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=12425442\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 131751 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 13256\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 64455\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 64455 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=51561 obj=9.32347 num_tokens=114531 num_tokens/piece=2.22127\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=37035 obj=7.52562 num_tokens=114846 num_tokens/piece=3.10101\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=27775 obj=7.44321 num_tokens=120108 num_tokens/piece=4.32432\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=27751 obj=7.42809 num_tokens=120103 num_tokens/piece=4.32788\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=20812 obj=7.46253 num_tokens=133910 num_tokens/piece=6.43427\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=20812 obj=7.45187 num_tokens=133889 num_tokens/piece=6.43326\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=15609 obj=7.52778 num_tokens=149504 num_tokens/piece=9.57806\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=15609 obj=7.51094 num_tokens=149492 num_tokens/piece=9.5773\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=11706 obj=7.63066 num_tokens=165268 num_tokens/piece=14.1182\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=11706 obj=7.60608 num_tokens=165280 num_tokens/piece=14.1193\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=8779 obj=7.76821 num_tokens=180682 num_tokens/piece=20.5812\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=8779 obj=7.73675 num_tokens=180699 num_tokens/piece=20.5831\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=6584 obj=7.93822 num_tokens=194774 num_tokens/piece=29.5829\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=6584 obj=7.89997 num_tokens=194784 num_tokens/piece=29.5844\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=4938 obj=8.14182 num_tokens=207638 num_tokens/piece=42.049\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=4938 obj=8.09724 num_tokens=207646 num_tokens/piece=42.0506\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=3703 obj=8.37483 num_tokens=219900 num_tokens/piece=59.3843\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=3703 obj=8.32246 num_tokens=219936 num_tokens/piece=59.394\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2777 obj=8.6406 num_tokens=232365 num_tokens/piece=83.6748\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2777 obj=8.58031 num_tokens=232399 num_tokens/piece=83.6871\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2082 obj=8.92395 num_tokens=243840 num_tokens/piece=117.118\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2082 obj=8.86128 num_tokens=243835 num_tokens/piece=117.116\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1561 obj=9.23654 num_tokens=254628 num_tokens/piece=163.119\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1561 obj=9.16608 num_tokens=254709 num_tokens/piece=163.17\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1170 obj=9.55937 num_tokens=265112 num_tokens/piece=226.591\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1170 obj=9.4815 num_tokens=265147 num_tokens/piece=226.621\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1100 obj=9.56497 num_tokens=268041 num_tokens/piece=243.674\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1100 obj=9.54752 num_tokens=268056 num_tokens/piece=243.687\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: ./sentencepiece_model/librispeech_unigram_model1000_new2.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: ./sentencepiece_model/librispeech_unigram_model1000_new2.vocab\n"
     ]
    }
   ],
   "source": [
    "### NEW\n",
    "\n",
    "import sentencepiece as spm\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Path to the unigram file and output directory\n",
    "unigram_file = \"aggregated_result.txt\"  # Replace with your unigram file path\n",
    "output_dir = \"./sentencepiece_model\"\n",
    "preprocessed_file = \"./preprocessed_unigrams.txt\"  # Temp file for synthetic \"sentences\"\n",
    "vocab_size = 1000  # Adjust to a smaller vocab size\n",
    "batch_size = 10  # Create more synthetic sentences by lowering batch size\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Preprocess the unigram file\n",
    "print(\"[INFO] Preprocessing unigram file...\")\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"\n",
    "    Remove all punctuation from text using regex.\n",
    "    \"\"\"\n",
    "    return re.sub(r\"[^\\w\\s]\", \"\", text)  # Retain only word characters and spaces\n",
    "\n",
    "with open(unigram_file, \"r\", encoding=\"utf-8\") as infile, open(preprocessed_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    batch = []\n",
    "    for line in infile:\n",
    "        line = line.lower()\n",
    "        word = remove_punctuation(line.strip())  # Remove punctuation\n",
    "        if word:\n",
    "            batch.append(word)\n",
    "        if len(batch) >= batch_size:\n",
    "            outfile.write(\" \".join(batch) + \"\\n\")\n",
    "            batch = []\n",
    "    if batch:\n",
    "        outfile.write(\" \".join(batch) + \"\\n\")\n",
    "\n",
    "print(f\"[INFO] Preprocessed file saved to {preprocessed_file}\")\n",
    "\n",
    "# Train the SentencePiece model\n",
    "model_prefix = os.path.join(output_dir, \"librispeech_unigram_model1000_new2\")\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input=preprocessed_file,\n",
    "    model_prefix=model_prefix,\n",
    "    vocab_size=vocab_size,\n",
    "    model_type=\"unigram\",\n",
    "    character_coverage=1.0,\n",
    "    bos_id=-1,\n",
    "    eos_id=-1,\n",
    "    unk_id=0,\n",
    "    pad_id=1,\n",
    "    shuffle_input_sentence=True,\n",
    "    normalization_rule_name=\"identity\"  # Disable normalization\n",
    ")\n",
    "\n",
    "print(f\"[INFO] SentencePiece model saved to {model_prefix}.model and {model_prefix}.vocab\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input Text: a b c d e f g\n",
      "Token IDs: [11, 9, 40, 62, 230, 131, 9, 47, 106]\n",
      "Tokens: ['▁a', '▁', 'b', '▁c', '▁d', '▁e', '▁', 'f', '▁g']\n",
      "Decoded Text: a b c d e f g\n",
      "\n",
      "Input Text: a'mighty's a'body a'court\n",
      "Token IDs: [11, 6, 30, 179, 67, 218, 6, 7, 11, 6, 452, 18, 23, 11, 6, 26, 370, 15]\n",
      "Tokens: ['▁a', \"'\", 'm', 'ig', 'h', 'ty', \"'\", 's', '▁a', \"'\", 'bo', 'd', 'y', '▁a', \"'\", 'c', 'our', 't']\n",
      "Decoded Text: a'mighty's a'body a'court\n",
      "\n",
      "Input Text: this is an example\n",
      "Token IDs: [77, 48, 103, 142, 36, 177, 41]\n",
      "Tokens: ['▁this', '▁is', '▁an', '▁ex', 'a', 'mp', 'le']\n",
      "Decoded Text: this is an example\n",
      "\n",
      "Input Text: this is a test sentence\n",
      "Token IDs: [77, 48, 11, 9, 15, 197, 587, 213]\n",
      "Tokens: ['▁this', '▁is', '▁a', '▁', 't', 'est', '▁sent', 'ence']\n",
      "Decoded Text: this is a test sentence\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "# Load the trained SentencePiece model\n",
    "sp = spm.SentencePieceProcessor()\n",
    "# sp.load(\"sentencepiece_model/librispeech_unigram_model.model\")\n",
    "sp.load(\"sentencepiece_model/librispeech_unigram_model1000_new.model\")\n",
    "\n",
    "# Test encoding and decoding\n",
    "test_sentences = [\n",
    "    \"A B C D E F G\",\n",
    "    \"A'MIGHTY'S A'BODY A'COURT\",\n",
    "    \"THIS IS AN EXAMPLE\",\n",
    "    \"THIS IS A TEST SENTENCE\"\n",
    "]\n",
    "\n",
    "for text in test_sentences:\n",
    "    text = text.lower()\n",
    "    token_ids = sp.encode(text, out_type=int)\n",
    "    tokens = sp.encode(text, out_type=str)\n",
    "    decoded_text = sp.decode(token_ids).strip()  # Remove leading/trailing spaces\n",
    "\n",
    "    print(f\"\\nInput Text: {text}\")\n",
    "    print(f\"Token IDs: {token_ids}\")\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    print(f\"Decoded Text: {decoded_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Token '▁' has ID: 9\n",
      "Token: '▁', ID: 9\n",
      "[INFO] Token 'a' has ID: 36\n",
      "Token: 'a', ID: 36\n",
      "[INFO] Token 'this' has ID: 0\n",
      "Token: 'this', ID: 0\n",
      "[INFO] Token 'unknown_token' has ID: 0\n",
      "Token: 'unknown_token', ID: 0\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "# Load the trained SentencePiece model\n",
    "sp = spm.SentencePieceProcessor()\n",
    "# sp.load(\"sentencepiece_model/librispeech_unigram_model.model\")\n",
    "sp.load(\"sentencepiece_model/librispeech_unigram_model1000_new.model\")\n",
    "\n",
    "# Function to get the token ID for a given token\n",
    "def get_token_id(token: str) -> int:\n",
    "    \"\"\"\n",
    "    Returns the token ID for the given token.\n",
    "    \n",
    "    Args:\n",
    "    - token (str): The token to look up.\n",
    "\n",
    "    Returns:\n",
    "    - int: Token ID if the token exists, -1 otherwise.\n",
    "    \"\"\"\n",
    "    token_id = sp.piece_to_id(token)\n",
    "    if token_id == -1:\n",
    "        print(f\"[INFO] Token '{token}' not found in vocabulary.\")\n",
    "    else:\n",
    "        print(f\"[INFO] Token '{token}' has ID: {token_id}\")\n",
    "    return token_id\n",
    "\n",
    "# Test cases\n",
    "tokens_to_lookup = [\"▁\", \"A\", \"THIS\", \"UNKNOWN_TOKEN\"]\n",
    "\n",
    "for token in tokens_to_lookup:\n",
    "    token = token.lower()\n",
    "    token_id = get_token_id(token)\n",
    "    print(f\"Token: '{token}', ID: {token_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Token 'it' has ID: 83\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = \"it\"\n",
    "get_token_id(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'te'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.decode_ids(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import sentencepiece as spm\n",
    "\n",
    "class CTCTextEncoder_Subword:\n",
    "    \"\"\"\n",
    "    A minimal CTC-compatible text encoder/decoder that uses a raw SentencePiece model.\n",
    "    - No Wav2Vec2CTCTokenizer or JSON vocab needed.\n",
    "    - Spaces are handled by the '▁' token in the trained SP model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        sp_model_path: str = \"librispeech_unigram_model.model\",\n",
    "        # Optional arguments if you want to do LM decoding, etc.\n",
    "        lm_path: str = None,\n",
    "        use_lm: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        sp_model_path: Path to the raw SentencePiece model file (.model) \n",
    "                       that you trained (or loaded).\n",
    "        lm_path:       Path to a KenLM .arpa or .bin if you want an LM-based decode. (Not used here)\n",
    "        use_lm:        Whether to eventually use an LM for beam-search decode. (Not used here)\n",
    "        \"\"\"\n",
    "        self.sp_model_path = sp_model_path\n",
    "        self.lm_path = lm_path\n",
    "        self.use_lm = use_lm\n",
    "\n",
    "        # Load SentencePiece\n",
    "        if not os.path.exists(sp_model_path):\n",
    "            raise FileNotFoundError(f\"SentencePiece model not found at: {sp_model_path}\")\n",
    "        self.sp = spm.SentencePieceProcessor()\n",
    "        self.sp.load(sp_model_path)\n",
    "\n",
    "        # Basic debug info\n",
    "        vocab_size = self.sp.get_piece_size()\n",
    "        print(f\"[INIT] Loaded SentencePiece model '{sp_model_path}' with vocab_size={vocab_size}\")\n",
    "        print(f\"       use_lm={use_lm}, lm_path={lm_path if lm_path else 'None'}\")\n",
    "\n",
    "    def encode(self, text: str) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Encode text -> subword token IDs using raw SentencePiece.\n",
    "        Returns a 2D torch.Tensor shape [1, seq_len].\n",
    "        \"\"\"\n",
    "        token_ids = self.sp.encode(text, out_type=int)\n",
    "\n",
    "        # For CTC training, we typically want shape [B, T], so wrap in batch dim.\n",
    "        # Debug\n",
    "        print(f\"\\n[ENCODE] text => '{text}'\")\n",
    "        print(f\"   => token_ids: {token_ids}\")\n",
    "        return torch.tensor([token_ids], dtype=torch.long)\n",
    "\n",
    "    def decode(self, token_ids) -> str:\n",
    "        \"\"\"\n",
    "        Decode subword token IDs -> text using raw SentencePiece.\n",
    "        \n",
    "        token_ids can be:\n",
    "          - a Python list of int IDs\n",
    "          - a 1D torch.Tensor of shape [seq_len]\n",
    "          - a 2D torch.Tensor of shape [1, seq_len]\n",
    "        Returns the fully reconstructed text (with spaces).\n",
    "        \"\"\"\n",
    "        if isinstance(token_ids, torch.Tensor):\n",
    "            token_ids = token_ids.squeeze().tolist()  # shape => list of int\n",
    "        elif isinstance(token_ids, list) and isinstance(token_ids[0], list):\n",
    "            # If shape is [B, T], pick the first batch\n",
    "            token_ids = token_ids[0]\n",
    "\n",
    "        # SentencePiece decode\n",
    "        text = self.sp.decode(token_ids)\n",
    "\n",
    "        # Debug\n",
    "        print(f\"[DECODE] token_ids => {token_ids}\")\n",
    "        print(f\"   => Decoded Text: {text}\")\n",
    "        return text\n",
    "\n",
    "    def decode_ctc_simple(self, token_ids) -> str:\n",
    "        \"\"\"\n",
    "        (Optional) If you have repeated tokens (typical in raw CTC argmax),\n",
    "        you can do a simple collapse of consecutive duplicates, ignoring a blank ID if you define one.\n",
    "\n",
    "        If your SentencePiece has a <pad> or <unk> ID for blank, you can skip them here.\n",
    "        \"\"\"\n",
    "        # This is only relevant if your model outputs repeated subword IDs \n",
    "        # or uses a specific blank token. For demonstration:\n",
    "        if isinstance(token_ids, torch.Tensor):\n",
    "            token_ids = token_ids.squeeze().tolist()\n",
    "\n",
    "        collapsed = []\n",
    "        prev_id = None\n",
    "        for tid in token_ids:\n",
    "            # Suppose <pad> or <unk> was ID=0, skip if you want to treat as blank\n",
    "            # if tid == 0:\n",
    "            #     continue\n",
    "\n",
    "            if tid != prev_id:\n",
    "                collapsed.append(tid)\n",
    "            prev_id = tid\n",
    "\n",
    "        # Now decode\n",
    "        return self.sp.decode(collapsed)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the size of the SentencePiece vocabulary.\"\"\"\n",
    "        return self.sp.get_piece_size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary saved to sentencepiece_model/librispeech_vocab.json\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "import json\n",
    "\n",
    "# Load the SentencePiece model\n",
    "sp_model_path = \"sentencepiece_model/librispeech_unigram_model.model\"\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(sp_model_path)\n",
    "\n",
    "# Export the vocabulary\n",
    "vocab = {sp.id_to_piece(i): i for i in range(sp.get_piece_size())}\n",
    "\n",
    "# Save as JSON\n",
    "vocab_json_path = \"sentencepiece_model/librispeech_vocab.json\"\n",
    "with open(vocab_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(vocab, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Vocabulary saved to {vocab_json_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Is space token ('▁') in vocabulary? True\n"
     ]
    }
   ],
   "source": [
    "# Load the SentencePiece model\n",
    "import json\n",
    "\n",
    "vocab_json_path = \"sentencepiece_model/librispeech_vocab.json\"\n",
    "with open(vocab_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    vocab = json.load(f)\n",
    "\n",
    "# Check if the space token ('▁') is present\n",
    "print(\"[INFO] Is space token ('▁') in vocabulary?\", \"▁\" in vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[SentencePiece]\n",
      "Input Text: this is a test sentence.\n",
      "Token IDs: [1003, 7, 8, 45, 8, 10, 2215, 8479, 9, 2]\n",
      "Tokens: ['▁thi', 's', '▁', 'is', '▁', 'a', '▁test', '▁sentenc', 'e', '.']\n",
      "Decoded Text: this is a test sentence.\n",
      "\n",
      "[SentencePiece]\n",
      "Input Text: another example with a'mighty.\n",
      "Token IDs: [8, 24, 2390, 8893, 2458, 8, 10, 6, 7291, 2]\n",
      "Tokens: ['▁', 'an', 'other', '▁example', '▁with', '▁', 'a', \"'\", 'mighty', '.']\n",
      "Decoded Text: another example with a'mighty.\n",
      "\n",
      "[SentencePiece]\n",
      "Input Text: hello world!\n",
      "Token IDs: [1707, 17, 8, 2425, 3]\n",
      "Tokens: ['▁hell', 'o', '▁', 'world', '!']\n",
      "Decoded Text: hello world!\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "sp_model_path = \"sentencepiece_model/librispeech_unigram_model.model\"\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(sp_model_path)\n",
    "\n",
    "# Test sentences\n",
    "test_sentences = [\n",
    "    \"THIS IS A TEST SENTENCE.\",\n",
    "    \"ANOTHER EXAMPLE WITH A'MIGHTY.\",\n",
    "    \"HELLO WORLD!\",\n",
    "]\n",
    "\n",
    "\n",
    "for text in test_sentences:\n",
    "    text = text.lower()\n",
    "    token_ids = sp.encode(text, out_type=int)\n",
    "    tokens = sp.encode(text, out_type=str)\n",
    "    decoded_text = sp.decode(token_ids)\n",
    "\n",
    "    print(f\"\\n[SentencePiece]\")\n",
    "    print(f\"Input Text: {text}\")\n",
    "    print(f\"Token IDs: {token_ids}\")\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    print(f\"Decoded Text: {decoded_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import torch\n",
    "import kenlm\n",
    "import numpy as np\n",
    "\n",
    "from typing import List, Tuple, Optional, Union\n",
    "from collections import defaultdict\n",
    "from transformers import Wav2Vec2Processor\n",
    "import sentencepiece as spm\n",
    "from pyctcdecode import build_ctcdecoder\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "class CTCTextEncoder:\n",
    "    def __init__(\n",
    "        self,\n",
    "        sp_model_path: Optional[str] = None,\n",
    "        arpa_path: Optional[str] = None,\n",
    "        binary_path: Optional[str] = \"4-gram_lc_correct.bin\",\n",
    "        unigram_path: Optional[str] = \"librispeech-vocab.txt\",\n",
    "        lm_weight: float = 0.5,\n",
    "        beam_size: int = 100,\n",
    "        use_lm: bool = False,\n",
    "        use_bpe: bool = True,\n",
    "        blank_token: str = \"<pad>\",\n",
    "        unk_token: str = \"<unk>\",\n",
    "        **kwargs\n",
    "    ):\n",
    "        print(\"\\n[INIT] CTCTextEncoder init:\")\n",
    "        self.beam_size = beam_size\n",
    "        self.lm_weight = lm_weight\n",
    "        self.arpa_path = arpa_path\n",
    "        self.binary_path = binary_path\n",
    "        self.blank_token = blank_token\n",
    "        self.unk_token = unk_token\n",
    "        self.use_lm = use_lm\n",
    "        self.use_bpe = use_bpe\n",
    "        self.sp_model_path = sp_model_path\n",
    "\n",
    "        print(\"  -> sp_model_path:\", sp_model_path)\n",
    "        print(\"  -> lm_weight:\", lm_weight)\n",
    "        print(\"  -> beam_size:\", beam_size)\n",
    "        print(\"  -> binary_path:\", binary_path)\n",
    "        print(\"  -> use_lm:\", use_lm)\n",
    "        print(\"  -> use_bpe:\", use_bpe)\n",
    "        print(\"  -> blank_token:\", blank_token)\n",
    "        print(\"  -> unk_token:\", unk_token)\n",
    "\n",
    "        self.printed_samples = 0\n",
    "        self.max_printed_samples = 5\n",
    "\n",
    "        # Load unigrams if provided (for pyctcdecode LM)\n",
    "        self.unigrams = None\n",
    "        if unigram_path and os.path.exists(unigram_path):\n",
    "            print(f\"[INIT] Loading unigrams from: {unigram_path}\")\n",
    "            with open(unigram_path, 'r', encoding='utf-8') as f:\n",
    "                self.unigrams = [line.strip().lower() for line in f if line.strip()]\n",
    "            print(f\"[INIT] Loaded {len(self.unigrams)} unigrams.\")\n",
    "\n",
    "        # Initialize vocabulary (tokenizer or char-based)\n",
    "        self._initialize_vocabulary()\n",
    "\n",
    "        # Map indices <-> tokens\n",
    "        self.ind2char = dict(enumerate(self.vocab))\n",
    "        self.char2ind = {v: k for k, v in self.ind2char.items()}\n",
    "        self.blank_index = self.char2ind.get(self.blank_token, None)\n",
    "\n",
    "        print(\"\\n[INIT] Final Vocabulary Information:\")\n",
    "        print(f\"  -> vocab size: {len(self.vocab)}\")\n",
    "        print(f\"  -> blank token: '{self.blank_token}' => index {self.blank_index}\")\n",
    "        sample_keys = list(self.ind2char.keys())[:20]\n",
    "        sample_ind2char = {k: self.ind2char[k] for k in sample_keys}\n",
    "        print(f\"  -> sample ind2char: {sample_ind2char}\")\n",
    "        sample_tokens = list(self.char2ind.keys())[:20]\n",
    "        sample_char2ind = {t: self.char2ind[t] for t in sample_tokens}\n",
    "        print(f\"  -> sample char2ind: {sample_char2ind}\")\n",
    "\n",
    "        # If using LM, init\n",
    "        if self.use_lm:\n",
    "            self._initialize_language_model()\n",
    "        else:\n",
    "            print(\"[INIT] Language model usage disabled.\")\n",
    "            self.lm = None\n",
    "            self.decoder = None\n",
    "\n",
    "        print(\"[INIT] CTCTextEncoder init done.\\n\")\n",
    "\n",
    "    def _initialize_vocabulary(self):\n",
    "        \"\"\"\n",
    "        Loads BPE-based vocab from SentencePiece model if use_bpe=True;\n",
    "        else uses simple char-based vocab.\n",
    "        \"\"\"\n",
    "        if self.use_bpe:\n",
    "            print(\"[VOCAB] use_bpe=True => loading SentencePiece model.\")\n",
    "            if not self.sp_model_path or not os.path.exists(self.sp_model_path):\n",
    "                raise ValueError(f\"[VOCAB] Invalid SentencePiece model path: {self.sp_model_path}\")\n",
    "\n",
    "            self.sp_model = spm.SentencePieceProcessor()\n",
    "            self.sp_model.load(self.sp_model_path)\n",
    "            self.vocab = [self.sp_model.id_to_piece(i) for i in range(self.sp_model.get_piece_size())]\n",
    "            print(f\"[VOCAB] Loaded SentencePiece model with vocab size {len(self.vocab)}\")\n",
    "        else:\n",
    "            print(\"[VOCAB] use_bpe=False => using simple character-based vocab.\")\n",
    "            chars = list(\"abcdefghijklmnopqrstuvwxyz \")\n",
    "            self.vocab = [self.blank_token] + chars\n",
    "            self.sp_model = None\n",
    "\n",
    "    def _initialize_language_model(self):\n",
    "        \"\"\"Initialize KenLM + pyctcdecode for beam search.\"\"\"\n",
    "        self.lm = None\n",
    "        self.decoder = None\n",
    "\n",
    "        model_path = self.binary_path if self.binary_path else self.arpa_path\n",
    "        print(f\"[LM] Attempting to load language model from path: {model_path}\")\n",
    "        if not model_path or not os.path.exists(model_path):\n",
    "            print(\"[LM] No valid LM path found. Skipping LM init.\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            self.lm = kenlm.Model(model_path)\n",
    "            print(f\"[LM] KenLM model loaded successfully: {model_path}\")\n",
    "\n",
    "            self.decoder = build_ctcdecoder(\n",
    "                labels=self.vocab,\n",
    "                kenlm_model_path=model_path,\n",
    "                alpha=self.lm_weight,\n",
    "                beta=0.1,\n",
    "                unk_score_offset=-10.0,\n",
    "            )\n",
    "\n",
    "            print(\"[LM] Successfully initialized pyctcdecode with LM support.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[LM] WARNING: Could not initialize LM: {str(e)}\")\n",
    "            self.decoder = None\n",
    "\n",
    "    def encode(self, text: str) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Convert text -> token IDs.\n",
    "        \"\"\"\n",
    "        if self.use_bpe and self.sp_model is not None:\n",
    "            text = \"▁\" + text.replace(\" \", \" ▁\").strip()\n",
    "            token_ids = self.sp_model.encode(text, out_type=int)\n",
    "            return torch.tensor(token_ids).unsqueeze(0)\n",
    "        else:\n",
    "            normalized_text = self.normalize_text(text)\n",
    "            token_indices = [\n",
    "                self.char2ind.get(char, self.char2ind.get(self.unk_token)) for char in normalized_text\n",
    "            ]\n",
    "            return torch.tensor(token_indices).unsqueeze(0)\n",
    "\n",
    "    def decode_simple(self, indices: List[int]) -> str:\n",
    "        \"\"\"\n",
    "        Greedy decode: remove blanks, collapse repeats.\n",
    "        \"\"\"\n",
    "        decoded_chars = []\n",
    "        prev_idx = None\n",
    "\n",
    "        for idx in indices:\n",
    "            if idx == self.blank_index:\n",
    "                prev_idx = idx\n",
    "                continue\n",
    "            if idx == prev_idx:\n",
    "                continue\n",
    "            if 0 <= idx < len(self.ind2char):\n",
    "                decoded_chars.append(self.ind2char[idx])\n",
    "            prev_idx = idx\n",
    "\n",
    "        text = \"\".join(decoded_chars).strip()\n",
    "        if self.use_bpe:\n",
    "            text = text.replace(\"▁\", \" \")\n",
    "        return text\n",
    "\n",
    "    def decode(self, indices: List[int]) -> str:\n",
    "        \"\"\"\n",
    "        Decode token indices to text.\n",
    "        \"\"\"\n",
    "        if self.decoder:\n",
    "            decoded_text = self.decoder.decode(indices)\n",
    "            return decoded_text.replace(\"▁\", \" \").strip()\n",
    "        else:\n",
    "            return self.decode_simple(indices).strip()\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_text(text: str) -> str:\n",
    "        \"\"\"Normalize input text.\"\"\"\n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"[^a-z ]\", \"\", text)\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xfe in position 99: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0mTraceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 9\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load the JSON vocabulary instead of the SentencePiece binary model\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# vocab_json_path = \"sentencepiece_model/librispeech_vocab.json\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m sp_model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentencepiece_model/librispeech_unigram_model.model\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 9\u001b[0m encoder \u001b[38;5;241m=\u001b[39m \u001b[43mCTCTextEncoder_Subword\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43msp_model_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msp_model_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use the JSON file here\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlm_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Path to KenLM model if available, or set to None\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocab_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpad_token\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m<pad>\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mword_delimiter_token\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# SentencePiece word boundary token\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munk_token\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m<unk>\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_lm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[64], line 24\u001b[0m, in \u001b[0;36mCTCTextEncoder_Subword.__init__\u001b[0;34m(self, sp_model_path, lm_path, alpha, beam_size, vocab_special_tokens, use_lm)\u001b[0m\n\u001b[1;32m     17\u001b[0m     vocab_special_tokens \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpad_token\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<pad>\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword_delimiter_token\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m▁\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# for SentencePiece 'underscores' as spaces\u001b[39;00m\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munk_token\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<unk>\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     21\u001b[0m     }\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# 1. Load the Wav2Vec2CTCTokenizer using the SentencePiece vocab\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mWav2Vec2CTCTokenizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43msp_model_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab_special_tokens\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpad_token\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43munk_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab_special_tokens\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munk_token\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mword_delimiter_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab_special_tokens\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mword_delimiter_token\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# 2. Feature extractor for real audio usage\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_extractor \u001b[38;5;241m=\u001b[39m Wav2Vec2FeatureExtractor(\n\u001b[1;32m     33\u001b[0m     feature_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     34\u001b[0m     sampling_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16000\u001b[39m,\n\u001b[1;32m     35\u001b[0m     do_normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     36\u001b[0m     return_attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     37\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/envs/project_env/lib/python3.12/site-packages/transformers/models/wav2vec2/tokenization_wav2vec2.py:169\u001b[0m, in \u001b[0;36mWav2Vec2CTCTokenizer.__init__\u001b[0;34m(self, vocab_file, bos_token, eos_token, unk_token, pad_token, word_delimiter_token, replace_word_delimiter_char, do_lower_case, target_lang, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_lang \u001b[38;5;241m=\u001b[39m target_lang\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(vocab_file, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m vocab_handle:\n\u001b[0;32m--> 169\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab_handle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# if target lang is defined vocab must be a nested dict\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# with each target lang being one vocabulary\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target_lang \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/envs/project_env/lib/python3.12/json/__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(fp, \u001b[38;5;241m*\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    275\u001b[0m         parse_int\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    276\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;124;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;124;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loads(\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    294\u001b[0m         \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m, object_hook\u001b[38;5;241m=\u001b[39mobject_hook,\n\u001b[1;32m    295\u001b[0m         parse_float\u001b[38;5;241m=\u001b[39mparse_float, parse_int\u001b[38;5;241m=\u001b[39mparse_int,\n\u001b[1;32m    296\u001b[0m         parse_constant\u001b[38;5;241m=\u001b[39mparse_constant, object_pairs_hook\u001b[38;5;241m=\u001b[39mobject_pairs_hook, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[0;32m<frozen codecs>:322\u001b[0m, in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xfe in position 99: invalid start byte"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor, Wav2Vec2Processor\n",
    "\n",
    "# Load the JSON vocabulary instead of the SentencePiece binary model\n",
    "vocab_json_path = \"sentencepiece_model/librispeech_vocab.json\"\n",
    "\n",
    "\n",
    "\n",
    "encoder = CTCTextEncoder_Subword(\n",
    "    sp_model_path=vocab_json_path,  # Use the JSON file here\n",
    "    lm_path=None,  # Path to KenLM model if available, or set to None\n",
    "    vocab_special_tokens={\n",
    "        \"pad_token\": \"<pad>\",\n",
    "        \"word_delimiter_token\": \" \",  # SentencePiece word boundary token\n",
    "        \"unk_token\": \"<unk>\",\n",
    "    },\n",
    "    use_lm=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INIT] Loaded SentencePiece model 'sentencepiece_model/librispeech_unigram_model.model' with vocab_size=10000\n",
      "       use_lm=False, lm_path=None\n",
      "\n",
      "[ENCODE] text => 'THIS IS A TEST SENTENCE.'\n",
      "   => token_ids: [1003, 7, 8, 45, 8, 10, 2215, 8479, 9, 2]\n",
      "[DECODE] token_ids => [1003, 7, 8, 45, 8, 10, 2215, 8479, 9, 2]\n",
      "   => Decoded Text: THIS IS A TEST SENTENCE.\n",
      "\n",
      "Original:  THIS IS A TEST SENTENCE.\n",
      "Reconstructed: THIS IS A TEST SENTENCE.\n",
      "\n",
      "[ENCODE] text => 'ANOTHER EXAMPLE WITH A'MIGHTY.'\n",
      "   => token_ids: [8, 24, 2390, 8893, 2458, 8, 10, 6, 7291, 2]\n",
      "[DECODE] token_ids => [8, 24, 2390, 8893, 2458, 8, 10, 6, 7291, 2]\n",
      "   => Decoded Text: ANOTHER EXAMPLE WITH A'MIGHTY.\n",
      "\n",
      "Original:  ANOTHER EXAMPLE WITH A'MIGHTY.\n",
      "Reconstructed: ANOTHER EXAMPLE WITH A'MIGHTY.\n",
      "\n",
      "[ENCODE] text => 'HELLO WORLD!'\n",
      "   => token_ids: [1707, 17, 8, 2425, 3]\n",
      "[DECODE] token_ids => [1707, 17, 8, 2425, 3]\n",
      "   => Decoded Text: HELLO WORLD!\n",
      "\n",
      "Original:  HELLO WORLD!\n",
      "Reconstructed: HELLO WORLD!\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "encoder = CTCTextEncoder_Subword(\n",
    "    sp_model_path=\"sentencepiece_model/librispeech_unigram_model.model\",\n",
    "    lm_path=None,\n",
    "    use_lm=False,\n",
    ")\n",
    "\n",
    "# Test some sentences\n",
    "test_sentences = [\n",
    "    \"THIS IS A TEST SENTENCE.\",\n",
    "    \"ANOTHER EXAMPLE WITH A'MIGHTY.\",\n",
    "    \"HELLO WORLD!\",\n",
    "]\n",
    "\n",
    "for text in test_sentences:\n",
    "    # 1. Encode\n",
    "    token_ids = encoder.encode(text)\n",
    "\n",
    "    # 2. Decode\n",
    "    decoded = encoder.decode(token_ids)\n",
    "\n",
    "    print(f\"\\nOriginal:  {text}\")\n",
    "    print(f\"Reconstructed: {decoded}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ENCODE] text => 'THIS IS A TEST SENTENCE.'\n",
      "   => token_ids: [[60, 45, 45, 10, 86, 7, 14, 0, 7, 76, 289, 2]]\n",
      "[DECODE] token_ids => [60, 45, 45, 10, 86, 7, 14, 0, 7, 76, 289, 2]\n",
      "   => Decoded Text (raw): THISATESTSENTENCE.\n",
      "   => Reconstructed Text: THISATESTSENTENCE.\n",
      "\n",
      "[ENCODE] text => 'THIS IS A TEST SENTENCE.'\n",
      "   => token_ids: [[60, 45, 45, 10, 86, 7, 14, 0, 7, 76, 289, 2]]\n",
      "[DECODE] token_ids => [60, 45, 45, 10, 86, 7, 14, 0, 7, 76, 289, 2]\n",
      "Tokens: ['TH', 'IS', 'IS', 'A', 'TE', 'S', 'T', '<unk>', 'S', 'ENT', 'ENCE', '.']\n",
      "Decoded Text (raw): THISATESTSENTENCE.\n",
      "Reconstructed Text: THISATESTSENTENCE.\n",
      "[ENCODE] text => 'ANOTHER EXAMPLE WITH A'MIGHTY.'\n",
      "   => token_ids: [[605, 211, 835, 77, 1537, 1636, 10, 6, 7291, 2]]\n",
      "[DECODE] token_ids => [605, 211, 835, 77, 1537, 1636, 10, 6, 7291, 2]\n",
      "   => Decoded Text (raw): ANOTHEREXAMPLEWITHA'MIGHTY.\n",
      "   => Reconstructed Text: ANOTHEREXAMPLEWITHA'MIGHTY.\n",
      "\n",
      "[ENCODE] text => 'ANOTHER EXAMPLE WITH A'MIGHTY.'\n",
      "   => token_ids: [[605, 211, 835, 77, 1537, 1636, 10, 6, 7291, 2]]\n",
      "[DECODE] token_ids => [605, 211, 835, 77, 1537, 1636, 10, 6, 7291, 2]\n",
      "Tokens: ['ANO', 'THER', 'EX', 'AM', 'PLE', 'WITH', 'A', \"'\", 'MIGHTY', '.']\n",
      "Decoded Text (raw): ANOTHEREXAMPLEWITHA'MIGHTY.\n",
      "Reconstructed Text: ANOTHEREXAMPLEWITHA'MIGHTY.\n",
      "[ENCODE] text => 'HELLO WORLD!'\n",
      "   => token_ids: [[1544, 128, 2425, 3]]\n",
      "[DECODE] token_ids => [1544, 128, 2425, 3]\n",
      "   => Decoded Text (raw): HELLOWORLD!\n",
      "   => Reconstructed Text: HELLOWORLD!\n",
      "\n",
      "[ENCODE] text => 'HELLO WORLD!'\n",
      "   => token_ids: [[1544, 128, 2425, 3]]\n",
      "[DECODE] token_ids => [1544, 128, 2425, 3]\n",
      "Tokens: ['HEL', 'LO', 'WORLD', '!']\n",
      "Decoded Text (raw): HELLOWORLD!\n",
      "Reconstructed Text: HELLOWORLD!\n"
     ]
    }
   ],
   "source": [
    "test_sentences = [\n",
    "    \"THIS IS A TEST SENTENCE.\",\n",
    "    \"ANOTHER EXAMPLE WITH A'MIGHTY.\",\n",
    "    \"HELLO WORLD!\",\n",
    "]\n",
    "\n",
    "for text in test_sentences:\n",
    "    # Encode the text\n",
    "    token_ids = encoder.encode(text)\n",
    "    \n",
    "    # Convert token IDs to tokens\n",
    "    tokens = encoder.tokenizer.convert_ids_to_tokens(token_ids.squeeze().tolist())\n",
    "    \n",
    "    # Decode back to text\n",
    "    decoded_text = encoder.decode(token_ids.squeeze().tolist())\n",
    "    \n",
    "    # Reconstruct spaces using '▁'\n",
    "    reconstructed_text = decoded_text.replace(\"▁\", \" \").strip()\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\n[ENCODE] text => '{text}'\")\n",
    "    print(f\"   => token_ids: {token_ids.tolist()}\")\n",
    "    print(f\"[DECODE] token_ids => {token_ids.squeeze().tolist()}\")\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    print(f\"Decoded Text (raw): {decoded_text}\")\n",
    "    print(f\"Reconstructed Text: {reconstructed_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ENCODE] text => '▁THIS ▁IS ▁A ▁TEST ▁SENTENCE.'\n",
      "   => token_ids: [1003, 7, 0, 8, 45, 8, 10, 2215, 8479, 9, 2]\n",
      "[DECODE] token_ids => [1003, 7, 0, 8, 45, 8, 10, 2215, 8479, 9, 2]\n",
      "\n",
      "[DEBUG] Preprocessed Encoding '▁THIS ▁IS ▁A ▁TEST ▁SENTENCE.'\n",
      "Token IDs: [[1003, 7, 0, 8, 45, 8, 10, 2215, 8479, 9, 2]]\n",
      "Tokens: ['▁THI', 'S', '<unk>', '▁', 'IS', '▁', 'A', '▁TEST', '▁SENTENC', 'E', '.']\n",
      "Decoded Text (raw): THIS IS A TEST SENTENCE.\n",
      "Reconstructed Text: THIS IS A TEST SENTENCE.\n",
      "[ENCODE] text => '▁ANOTHER ▁EXAMPLE ▁WITH ▁A'MIGHTY.'\n",
      "   => token_ids: [8, 605, 211, 8893, 2458, 8, 10, 6, 7291, 2]\n",
      "[DECODE] token_ids => [8, 605, 211, 8893, 2458, 8, 10, 6, 7291, 2]\n",
      "\n",
      "[DEBUG] Preprocessed Encoding '▁ANOTHER ▁EXAMPLE ▁WITH ▁A'MIGHTY.'\n",
      "Token IDs: [[8, 605, 211, 8893, 2458, 8, 10, 6, 7291, 2]]\n",
      "Tokens: ['▁', 'ANO', 'THER', '▁EXAMPLE', '▁WITH', '▁', 'A', \"'\", 'MIGHTY', '.']\n",
      "Decoded Text (raw): ANOTHER EXAMPLE WITH A'MIGHTY.\n",
      "Reconstructed Text: ANOTHER EXAMPLE WITH A'MIGHTY.\n",
      "[ENCODE] text => '▁HELLO ▁WORLD!'\n",
      "   => token_ids: [1707, 17, 1663, 31, 12, 3]\n",
      "[DECODE] token_ids => [1707, 17, 1663, 31, 12, 3]\n",
      "\n",
      "[DEBUG] Preprocessed Encoding '▁HELLO ▁WORLD!'\n",
      "Token IDs: [[1707, 17, 1663, 31, 12, 3]]\n",
      "Tokens: ['▁HELL', 'O', '▁WOR', 'L', 'D', '!']\n",
      "Decoded Text (raw): HELLO WORLD!\n",
      "Reconstructed Text: HELLO WORLD!\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text: str) -> str:\n",
    "    return \"▁\" + text.replace(\" \", \" ▁\").strip()\n",
    "\n",
    "# Preprocess test sentences\n",
    "preprocessed_sentences = [preprocess_text(text) for text in test_sentences]\n",
    "\n",
    "for text in preprocessed_sentences:\n",
    "    token_ids = encoder.encode(text)\n",
    "    tokens = encoder.tokenizer.convert_ids_to_tokens(token_ids.squeeze().tolist())\n",
    "    decoded_text = encoder.decode(token_ids.squeeze().tolist())\n",
    "    reconstructed_text = decoded_text.replace(\"▁\", \" \").strip()\n",
    "\n",
    "    print(f\"\\n[DEBUG] Preprocessed Encoding '{text}'\")\n",
    "    print(f\"Token IDs: {token_ids.tolist()}\")\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    print(f\"Decoded Text (raw): {decoded_text}\")\n",
    "    print(f\"Reconstructed Text: {reconstructed_text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_text_files(folders, output_file_name=\"aggregated_result.txt\"):\n",
    "    # Initialize an empty string to store the aggregated content\n",
    "    aggregated_content = \"\"\n",
    "\n",
    "    # Count total files for progress bar\n",
    "    total_files = sum(len(files) for folder_name in folders for _, _, files in os.walk(folder_name) if files)\n",
    "\n",
    "    # Initialize progress bar\n",
    "    with tqdm(total=total_files, desc=\"Processing files\", unit=\"file\") as pbar:\n",
    "        # Walk through each folder in the list\n",
    "        for folder_name in folders:\n",
    "            for root, _, files in os.walk(folder_name):\n",
    "                for file in files:\n",
    "                    if file.endswith(\".txt\"):  # Process only .txt files\n",
    "                        file_path = os.path.join(root, file)\n",
    "                        \n",
    "                        # Read and process each file\n",
    "                        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                            for line in f:\n",
    "                                # Remove leading numbers and dashes using regex\n",
    "                                processed_line = re.sub(r\"^[\\d-]+\\s*\", \"\", line)\n",
    "                                aggregated_content += processed_line\n",
    "\n",
    "                        pbar.update(1)\n",
    "\n",
    "    # Save the aggregated content to a new file in the current directory\n",
    "    with open(output_file_name, \"w\", encoding=\"utf-8\") as output_file:\n",
    "        output_file.write(aggregated_content)\n",
    "\n",
    "    print(f\"Aggregated content saved to {output_file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/sound_asr'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   2%| | 2682/135235 [00:00<00:2"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated content saved to aggregated_result.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "# Replace 'your_folder_names' with the list of folders containing the subfolders and .txt files\n",
    "folders = ['data/datasets/librispeech/train-clean-100', 'data/datasets/librispeech/train-clean-360', 'data/datasets/librispeech/train-clean-500']\n",
    "process_text_files(folders)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### PART 1\n",
    "\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_text_files(folders, output_file_name=\"aggregated_result.txt\"):\n",
    "    # Initialize an empty string to store the aggregated content\n",
    "    aggregated_content = \"\"\n",
    "\n",
    "    # Count total files for progress bar\n",
    "    total_files = sum(len(files) for folder_name in folders for _, _, files in os.walk(folder_name) if files)\n",
    "\n",
    "    # Initialize progress bar\n",
    "    with tqdm(total=total_files, desc=\"Processing files\", unit=\"file\") as pbar:\n",
    "        # Walk through each folder in the list\n",
    "        for folder_name in folders:\n",
    "            for root, _, files in os.walk(folder_name):\n",
    "                for file in files:\n",
    "                    if file.endswith(\".txt\"):  # Process only .txt files\n",
    "                        file_path = os.path.join(root, file)\n",
    "                        \n",
    "                        # Read and process each file\n",
    "                        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                            for line in f:\n",
    "                                # Remove leading numbers and dashes using regex\n",
    "                                processed_line = re.sub(r\"^[\\d-]+\\s*\", \"\", line)\n",
    "                                aggregated_content += processed_line\n",
    "\n",
    "                        pbar.update(1)\n",
    "\n",
    "    # Save the aggregated content to a new file in the current directory\n",
    "    with open(output_file_name, \"w\", encoding=\"utf-8\") as output_file:\n",
    "        output_file.write(aggregated_content)\n",
    "\n",
    "    print(f\"Aggregated content saved to {output_file_name}\")\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# Replace 'your_folder_names' with the list of folders containing the subfolders and .txt files\n",
    "folders = ['data/datasets/librispeech/train-clean-100', 'data/datasets/librispeech/train-clean-360', 'data/datasets/librispeech/train-clean-500']\n",
    "process_text_files(folders)\n",
    "\n",
    "\n",
    "\n",
    "##### PART 2\n",
    "\n",
    "import sentencepiece as spm\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Path to the unigram file and output directory\n",
    "unigram_file = \"aggregated_result.txt\"  # Replace with your unigram file path\n",
    "output_dir = \"./sentencepiece_model\"\n",
    "preprocessed_file = \"./preprocessed_unigrams.txt\"  # Temp file for synthetic \"sentences\"\n",
    "vocab_size = 1000  # Adjust to a smaller vocab size\n",
    "batch_size = 10  # Create more synthetic sentences by lowering batch size\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Preprocess the unigram file\n",
    "print(\"[INFO] Preprocessing unigram file...\")\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"\n",
    "    Remove all punctuation from text using regex.\n",
    "    \"\"\"\n",
    "    return re.sub(r\"[^\\w\\s]\", \"\", text)  # Retain only word characters and spaces\n",
    "\n",
    "with open(unigram_file, \"r\", encoding=\"utf-8\") as infile, open(preprocessed_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    batch = []\n",
    "    for line in infile:\n",
    "        line = line.lower()\n",
    "        word = remove_punctuation(line.strip())  # Remove punctuation\n",
    "        if word:\n",
    "            batch.append(word)\n",
    "        if len(batch) >= batch_size:\n",
    "            outfile.write(\" \".join(batch) + \"\\n\")\n",
    "            batch = []\n",
    "    if batch:\n",
    "        outfile.write(\" \".join(batch) + \"\\n\")\n",
    "\n",
    "print(f\"[INFO] Preprocessed file saved to {preprocessed_file}\")\n",
    "\n",
    "# Train the SentencePiece model\n",
    "model_prefix = os.path.join(output_dir, \"librispeech_unigram_model1000_new2\")\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input=preprocessed_file,\n",
    "    model_prefix=model_prefix,\n",
    "    vocab_size=vocab_size,\n",
    "    model_type=\"unigram\",\n",
    "    character_coverage=1.0,\n",
    "    bos_id=-1,\n",
    "    eos_id=-1,\n",
    "    unk_id=0,\n",
    "    pad_id=1,\n",
    "    shuffle_input_sentence=True,\n",
    "    normalization_rule_name=\"identity\"  # Disable normalization\n",
    ")\n",
    "\n",
    "print(f\"[INFO] SentencePiece model saved to {model_prefix}.model and {model_prefix}.vocab\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
