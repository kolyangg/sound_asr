{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer 21 Dec testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import kenlm\n",
    "from transformers import Wav2Vec2Processor, AutoProcessor, AutoTokenizer\n",
    "from pyctcdecode import build_ctcdecoder\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from typing import List, Tuple, Optional, Union\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "class CTCTextEncoder:\n",
    "    def __init__(\n",
    "        self,\n",
    "        arpa_path: Optional[str] = None,\n",
    "        binary_path: Optional[str] = None,\n",
    "        unigram_path: Optional[str] = None,\n",
    "        pretrained_tokenizer: str = \"facebook/wav2vec2-base-960h\",\n",
    "        lm_weight: float = 0.5,\n",
    "        beam_size: int = 100,\n",
    "        blank_token: str = \"[pad]\",  # Blank token as <pad> for Wav2Vec2\n",
    "        unk_token: str = \"[unk]\",     # UNK token\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize encoder with Wav2Vec2 tokenizer and beam search decoder.\n",
    "\n",
    "        Changes:\n",
    "        - Ensure normalization is strictly lowercase with only [a-z ].\n",
    "        \"\"\"\n",
    "        self.beam_size = beam_size\n",
    "        self.lm_weight = lm_weight\n",
    "        self.arpa_path = arpa_path\n",
    "        self.binary_path = binary_path\n",
    "        self.blank_token = blank_token\n",
    "        self.unk_token = unk_token\n",
    "        self.printed_samples = 0\n",
    "        self.max_printed_samples = 5\n",
    "        print('CTC Text Encoder:')\n",
    "        print('pretrained_tokenizer:', pretrained_tokenizer)\n",
    "        print('lm_weight:', lm_weight)\n",
    "        print('beam_size:', beam_size)\n",
    "        print('binary_path:', binary_path)\n",
    "\n",
    "        # unigram_path = None\n",
    "\n",
    "        # Load unigrams if provided\n",
    "        \n",
    "        self.unigrams = None\n",
    "        \n",
    "        # if unigram_path and os.path.exists(unigram_path):\n",
    "        #     print(f\"Loading unigrams from: {unigram_path}\")\n",
    "        #     with open(unigram_path, 'r', encoding='utf-8') as f:\n",
    "        #         self.unigrams = [line.strip().lower() for line in f if line.strip()]\n",
    "        #     print(f\"Loaded {len(self.unigrams)} unigrams\")\n",
    "        \n",
    "        if unigram_path and os.path.exists(unigram_path):\n",
    "            print(f\"Loading unigrams from: {unigram_path}\")\n",
    "            with open(unigram_path, 'r', encoding='utf-8') as f:\n",
    "                self.unigrams = [line.strip().lower() for line in f if line.strip()]\n",
    "                # self.unigrams = [line.strip().upper() for line in f if line.strip()]\n",
    "            print(f\"Loaded {len(self.unigrams)} unigrams\")\n",
    "\n",
    "\n",
    "        self._initialize_wav2vec2_tokenizer(pretrained_tokenizer)\n",
    "\n",
    "        # Create index mappings\n",
    "        self.ind2char = dict(enumerate(self.vocab))\n",
    "        self.char2ind = {v: k for k, v in self.ind2char.items()}\n",
    "        self.blank_index = self.char2ind[self.blank_token]\n",
    "\n",
    "        print(f\"\\nVocabulary Info:\")\n",
    "        print(f\"Size: {len(self.vocab)}\")\n",
    "        print(\"Full Vocabulary (up to first 50 tokens):\", self.vocab[:50])\n",
    "        print(f\"Blank token: {self.blank_token}, Blank index: {self.blank_index}\")\n",
    "\n",
    "        print(\"Sample ind2char mappings:\", {k: self.ind2char[k] for k in list(self.ind2char.keys())[:10]})\n",
    "        print(\"Sample char2ind mappings:\", {k: self.char2ind[k] for k in list(self.char2ind.keys())[:10]})\n",
    "\n",
    "        self._initialize_language_model()\n",
    "\n",
    "    # def _initialize_wav2vec2_tokenizer(self, pretrained_tokenizer: str):\n",
    "    #     \"\"\"Initialize vocabulary using Wav2Vec2 tokenizer.\"\"\"\n",
    "    #     self.processor = Wav2Vec2Processor.from_pretrained(pretrained_tokenizer)\n",
    "\n",
    "    #     # Add the unique blank token if not present\n",
    "    #     if self.blank_token not in self.tokenizer.get_vocab():\n",
    "    #         self.processor.tokenizer.add_tokens([self.blank_token])\n",
    "    #         print(f\"Added '{self.blank_token}' to the tokenizer's vocabulary.\")\n",
    "    #     else:\n",
    "    #         print(f\"'{self.blank_token}' no language exists in the tokenizer's vocabulary.\")\n",
    "\n",
    "    #     # Add the UNK token if not present\n",
    "    #     if self.unk_token not in self.processor.tokenizer.get_vocab():\n",
    "    #         self.processor.tokenizer.add_tokens([self.unk_token])\n",
    "    #         print(f\"Added '{self.unk_token}' to the tokenizer's vocabulary.\")\n",
    "    #     else:\n",
    "    #         print(f\"'{self.unk_token}' no lm path exists in the tokenizer's vocabulary.\")\n",
    "\n",
    "    #     # Get vocab, convert to lowercase and replace '|' with ' '\n",
    "    #     original_vocab = list(self.processor.tokenizer.get_vocab().keys())\n",
    "    #     self.vocab = [x.lower() for x in original_vocab]\n",
    "    #     self.vocab = [t.replace('|', ' ') for t in self.vocab]\n",
    "\n",
    "    #     # Debug: Print a few tokens after modification\n",
    "    #     print(\"Modified Vocabulary (first 20 tokens):\", self.vocab[:20])\n",
    "    \n",
    "\n",
    "    def _initialize_wav2vec2_tokenizer(self, pretrained_tokenizer: str):\n",
    "        \"\"\"Initialize vocabulary using Wav2Vec2 tokenizer.\"\"\"\n",
    "        # Initialize Wav2Vec2Processor\n",
    "        # self.processor = Wav2Vec2Processor.from_pretrained(pretrained_tokenizer)\n",
    "\n",
    "        \n",
    "\n",
    "        # # Add the UNK token if not present\n",
    "        # if self.unk_token not in self.processor.tokenizer.get_vocab():\n",
    "        #     self.processor.tokenizer.add_tokens([self.unk_token])\n",
    "        #     print(f\"Added '{self.unk_token}' to the tokenizer's vocabulary.\")\n",
    "        # else:\n",
    "        #     print(f\"'{self.unk_token}' already exists in the tokenizer's vocabulary.\")\n",
    "\n",
    "        # # Get vocab without altering the case\n",
    "        # self.vocab = list(self.processor.tokenizer.get_vocab().keys())\n",
    "\n",
    "        # vocab_dict = processor.tokenizer.get_vocab()\n",
    "        # sorted_vocab_dict = {k.lower(): v for k, v in sorted(vocab_dict.items(), key=lambda item: item[1])}\n",
    "        # labels_adj = list(sorted_vocab_dict.keys())\n",
    "\n",
    "        # self.vocab = [t.replace('|', ' ') for t in self.vocab]\n",
    "\n",
    "        # # Debugging: Inspect the tokenizer's vocabulary casing\n",
    "        # print(\"\\n--- Tokenizer Vocabulary Inspection ---\")\n",
    "        # sample_size = 100  # Adjust as needed\n",
    "        # sample_tokens = self.vocab[:sample_size]\n",
    "        # print(f\"First {sample_size} tokens in vocabulary:\")\n",
    "        # print(sample_tokens)\n",
    "\n",
    "        # # Save the full tokenizer vocabulary to a file for comparison\n",
    "        # with open(\"tokenizer_vocab.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        #     for token in self.vocab:\n",
    "        #         f.write(f\"{token}\\n\")\n",
    "        # print(\"Full tokenizer vocabulary saved to 'tokenizer_vocab.txt'.\")\n",
    "        # print(\"----------------------------------------\\n\")\n",
    "\n",
    "        self.processor = AutoProcessor.from_pretrained(\"hf-test/xls-r-300m-sv\")\n",
    "\n",
    "        vocab_dict = self.tokenizer.get_vocab()\n",
    "        sorted_vocab_dict = {k.lower(): v for k, v in sorted(vocab_dict.items(), key=lambda item: item[1])}\n",
    "\n",
    "        self.labels = list(sorted_vocab_dict.keys())\n",
    "\n",
    "        # Get vocab, convert to lowercase and replace '|' with ' '\n",
    "        original_vocab = list(self.tokenizer.get_vocab().keys())\n",
    "        # self.vocab = [x.lower() for x in original_vocab]\n",
    "        self.vocab = sorted_vocab_dict\n",
    "        self.vocab = [t.replace('|', ' ') for t in self.vocab]\n",
    "\n",
    "        # Add the unique blank token to the tokenizer's vocabulary if not present\n",
    "        if self.blank_token not in self.tokenizer.get_vocab():\n",
    "            self.tokenizer.add_tokens([self.blank_token])\n",
    "            print(f\"Added '{self.blank_token}' to the tokenizer's vocabulary.\")\n",
    "        else:\n",
    "            print(f\"'{self.blank_token}' already exists in the tokenizer's vocabulary.\")\n",
    "\n",
    "        # Debug: Print a few tokens after modification\n",
    "        print(\"Modified Vocabulary (first 20 tokens):\", self.vocab[:20])\n",
    "\n",
    "\n",
    "    def _initialize_language_model(self):\n",
    "        \"\"\"Initialize language model with explicit blank token handling.\"\"\"\n",
    "        self.lm = None\n",
    "        self.decoder = None\n",
    "\n",
    "        model_path = self.binary_path if self.binary_path else self.arpa_path\n",
    "        print('model_path: ', model_path)\n",
    "        if not model_path or not os.path.exists(model_path):\n",
    "            print(\"No language model path provided or file does not exist.\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            self.lm = kenlm.Model(model_path)\n",
    "            print(f\"Loaded {'binary' if self.binary_path else 'ARPA'} language model.\")\n",
    "\n",
    "            # labels = [self.blank_token] + [c for c in self.vocab if c != self.blank_token]\n",
    "\n",
    "            decoder_config = {\n",
    "                \"labels\": self.labels,\n",
    "                \"kenlm_model_path\": model_path,\n",
    "                \"alpha\": self.lm_weight,\n",
    "                \"beta\": 0.1,\n",
    "                \"unk_score_offset\": -10.0,\n",
    "            }\n",
    "\n",
    "            if self.unigrams:\n",
    "                print(\"\\n--- Unigrams List ---\")\n",
    "                # Save the unigrams to a file\n",
    "                with open(\"unigrams_list.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "                    for unigram in self.unigrams:\n",
    "                        f.write(f\"{unigram}\\n\")\n",
    "                print(f\"Unigrams list saved to 'unigrams_list.txt'. Total unigrams: {len(self.unigrams)}\")\n",
    "                print(\"----------------------\\n\")\n",
    "                decoder_config[\"unigrams\"] = self.unigrams\n",
    "\n",
    "            self.decoder = build_ctcdecoder(**decoder_config)\n",
    "            print(\"Successfully initialized language model and decoder.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Failed to initialize decoder: {str(e)}\")\n",
    "            self.decoder = None\n",
    "\n",
    "\n",
    "    def encode(self, text: str) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Encode text with Wav2Vec2 tokenizer.\n",
    "        \"\"\"\n",
    "        debug = False\n",
    "\n",
    "        if self.printed_samples < self.max_printed_samples:\n",
    "            original_text = text\n",
    "            text = self.normalize_text(text)\n",
    "            if debug:\n",
    "                print(f\"samples: {str(self.printed_samples)}\")\n",
    "                print(f\"\\nEncoding text:\")\n",
    "                print(f\" Original: '{original_text}'\")\n",
    "                print(f\" Normalized: '{text}'\")\n",
    "                for ch in text:\n",
    "                    print(ch, ord(ch))\n",
    "\n",
    "\n",
    "        try:\n",
    "            encoded = self.tokenizer(text, return_tensors=\"pt\", padding=False, truncation=False)\n",
    "            token_indices = encoded.input_ids[0].tolist()\n",
    "            if self.printed_samples < self.max_printed_samples:\n",
    "                # Convert indices to tokens from self.vocab\n",
    "                tokens = [self.vocab[idx] if 0 <= idx < len(self.vocab) else \"<invalid>\" for idx in token_indices]\n",
    "                # print(f\" Tokens (lowercased and '|'->' '): {tokens}\")\n",
    "                # print(f\" Token indices: {token_indices}\")\n",
    "                self.printed_samples += 1\n",
    "            return torch.tensor(token_indices).unsqueeze(0)\n",
    "        except KeyError as e:\n",
    "            unknown_tokens = set([token for token in text.split() if token not in self.char2ind])\n",
    "            raise Exception(f\"Unknown tokens: '{' '.join(unknown_tokens)}'\")\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Encoding error: {str(e)}\")\n",
    "\n",
    "    # latest ver\n",
    "    # def decode(self, indices: List[int]) -> str:\n",
    "    #     \"\"\"\n",
    "    #     Decode indices to text using beam search decoder if available.\n",
    "    #     \"\"\"\n",
    "    #     if self.decoder:\n",
    "    #         ctc_decode = self.decoder.decode(indices)\n",
    "    #         # convert to lower case\n",
    "    #         decoded_text = decoded_text.lower()\n",
    "    #         return decoded_text\n",
    "    #     else:\n",
    "    #         decoded_text = self.decode_simple(indices)\n",
    "    #         # convert to lower case\n",
    "    #         decoded_text = decoded_text.lower()\n",
    "    #         return\n",
    "    \n",
    "    def decode(self, indices: List[int]) -> str:\n",
    "        \"\"\"\n",
    "        Decode indices to text using beam search decoder if available.\n",
    "        \"\"\"\n",
    "        if self.decoder:\n",
    "            decoded_text = self.decoder.decode(indices)\n",
    "            # Convert to lower case\n",
    "            decoded_text = decoded_text.lower()\n",
    "            return decoded_text\n",
    "        else:\n",
    "            decoded_text = self.decode_simple(indices)\n",
    "            # Convert to lower case\n",
    "            decoded_text = decoded_text.lower()\n",
    "            return decoded_text  # Ensure the decoded text is returned\n",
    "\n",
    "    # latest ver\n",
    "    # def decode_simple(self, indices: List[int]) -> str:\n",
    "    #     \"\"\"\n",
    "    #     Simple CTC decoding without language model.\n",
    "    #     \"\"\"\n",
    "    #     valid_indices = [\n",
    "    #         idx for idx in indices\n",
    "    #         if idx != self.blank_index and 0 <= idx < len(self.ind2char)\n",
    "    #     ]\n",
    "    #     try:\n",
    "    #         tokens = [self.ind2char[idx] for idx in valid_indices]\n",
    "    #         text = \" \".join(tokens).strip().lower()\n",
    "    #         return self.processor.tokenizer.clean_up_tokenization(text)\n",
    "    #     except KeyError as e:\n",
    "    #         return \" \".join([self.ind2char[idx] for idx in valid_indices if idx in self.ind2char])\n",
    "\n",
    "    def decode_simple(self, indices: List[int]) -> str:\n",
    "        \"\"\"\n",
    "        Simple CTC decoding without language model.\n",
    "        Collapses consecutive duplicate tokens and removes blanks.\n",
    "        \"\"\"\n",
    "        decoded_chars = []\n",
    "\n",
    "        for idx in indices:\n",
    "            if idx == self.blank_index:\n",
    "                continue  # Skip blank tokens\n",
    "            if 0 <= idx < len(self.ind2char):\n",
    "                char = self.ind2char[idx]\n",
    "                decoded_chars.append(char)\n",
    "\n",
    "        # Join characters without collapsing duplicates and convert to lowercase\n",
    "        text = \"\".join(decoded_chars).strip().lower()\n",
    "\n",
    "        # Clean up tokenization using the tokenizer's method (if available)\n",
    "        return text\n",
    "\n",
    "\n",
    "    def decode_logits(self, logits: Union[torch.Tensor, List[List[float]], np.ndarray]) -> str:\n",
    "        \"\"\"\n",
    "        Decode logits using the decoder if available, otherwise use greedy decoding.\n",
    "        \"\"\"\n",
    "        if isinstance(logits, torch.Tensor):\n",
    "            logits = logits.cpu().numpy()\n",
    "        elif isinstance(logits, list):\n",
    "            logits = np.array(logits)\n",
    "        elif not isinstance(logits, np.ndarray):\n",
    "            raise TypeError(\"logits must be a torch.Tensor, list of lists, or numpy.ndarray\")\n",
    "\n",
    "        if logits.ndim == 3:\n",
    "            logits = logits[0]\n",
    "\n",
    "        if logits.ndim != 2:\n",
    "            raise ValueError(f\"Logits should be 2D (time_steps, vocab_size), got {logits.ndim}D\")\n",
    "\n",
    "        if self.decoder:\n",
    "            decoded_text = self.decoder.decode(logits)\n",
    "            return decoded_text\n",
    "        else:\n",
    "            predicted_indices = np.argmax(logits, axis=-1).tolist()\n",
    "            return self.decode_simple(predicted_indices)\n",
    "\n",
    "    def decode_indices(self, indices: Union[torch.Tensor, List[int], np.ndarray]) -> str:\n",
    "        \"\"\"\n",
    "        Decode token indices to text using simple decoding (no LM).\n",
    "        \"\"\"\n",
    "        if isinstance(indices, torch.Tensor):\n",
    "            indices = indices.squeeze().tolist()\n",
    "        elif isinstance(indices, np.ndarray):\n",
    "            indices = indices.tolist()\n",
    "        elif not isinstance(indices, list):\n",
    "            raise TypeError(\"decode_indices expects a list, torch.Tensor, or numpy.ndarray.\")\n",
    "\n",
    "        return self.decode_simple(indices)\n",
    "\n",
    "\n",
    "    # latest ver\n",
    "    def ctc_decode(self, logits: Union[torch.Tensor, List[int], np.ndarray]) -> str:\n",
    "        \"\"\"\n",
    "        Perform CTC decoding on logits.\n",
    "        \"\"\"\n",
    "        if isinstance(logits, np.ndarray):\n",
    "            logits = torch.from_numpy(logits)\n",
    "        elif isinstance(logits, list):\n",
    "            logits = torch.tensor(logits)\n",
    "\n",
    "        if logits.dim() == 3:\n",
    "            decoded_text = self.decode_logits(logits)\n",
    "            return decoded_text\n",
    "        elif logits.dim() == 2:\n",
    "            decoded_text = self.decode_logits(logits)\n",
    "            return decoded_text\n",
    "        elif logits.dim() == 1:\n",
    "            decoded_text = self.decode_indices(logits)\n",
    "            return decoded_text\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported logits shape: {logits.shape}. Expected 1D, 2D, or 3D.\")\n",
    "\n",
    "    def ctc_beam_search(self, probs, beam_size: int = 40,\n",
    "                       use_lm: bool = False, debug: bool = False) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Beam search with Wav2Vec2 support.\n",
    "        \"\"\"\n",
    "        beam_size = self.beam_size\n",
    "        debug = False\n",
    "\n",
    "        if use_lm and self.decoder is not None:\n",
    "            try:\n",
    "                if isinstance(probs, torch.Tensor):\n",
    "                    probs = probs.cpu().numpy()\n",
    "                elif isinstance(probs, list):\n",
    "                    probs = np.array(probs)\n",
    "                elif isinstance(probs, np.ndarray):\n",
    "                    pass\n",
    "                else:\n",
    "                    raise TypeError(\"probs must be a torch.Tensor, list, or numpy.ndarray\")\n",
    "\n",
    "                beams = self.decoder.decode_beams(\n",
    "                    probs,\n",
    "                    beam_prune_logp=-10.0,\n",
    "                    token_min_logp=-5.0,\n",
    "                    hotwords=[],\n",
    "                    hotword_weight=10.0,\n",
    "                )\n",
    "\n",
    "                formatted_beams = []\n",
    "                for beam in beams[:beam_size]:\n",
    "                    text = beam[0]\n",
    "                    acoustic_score = beam[3]\n",
    "                    lm_score = beam[4]\n",
    "\n",
    "                    text = self.tokenizer.clean_up_tokenization(text)\n",
    "                    text = text.lower().strip()\n",
    "\n",
    "                    combined_score = (1 - self.lm_weight) * acoustic_score + self.lm_weight * lm_score\n",
    "                    text_len = max(1, len(text.split()))\n",
    "                    normalized_score = combined_score / text_len\n",
    "\n",
    "                    formatted_beams.append((text, normalized_score))\n",
    "\n",
    "                if debug:\n",
    "                    print(\"\\nFormatted beam results with Wav2Vec2:\")\n",
    "                    for text, score in formatted_beams[:3]:\n",
    "                        print(f\"Text: '{text}', Score: {score:.4f}\")\n",
    "\n",
    "                if formatted_beams:\n",
    "                    return sorted(formatted_beams, key=lambda x: -x[1])\n",
    "                else:\n",
    "                    print(\"No valid beams found, falling back to standard beam search\")\n",
    "                    return self._standard_beam_search(probs, beam_size, debug)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Beam search with LM failed: {str(e)}, falling back to standard beam search\")\n",
    "                return self._standard_beam_search(probs, beam_size, debug)\n",
    "        else:\n",
    "            return self._standard_beam_search(probs, beam_size, debug)\n",
    "\n",
    "    def _standard_beam_search(self, probs, beam_size: int = 10, debug: bool = False) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Original beam search implementation with improved debugging\"\"\"\n",
    "        beam_size = self.beam_size\n",
    "\n",
    "        if isinstance(probs, np.ndarray):\n",
    "            probs = torch.from_numpy(probs)\n",
    "\n",
    "        if probs.device != torch.device('cpu'):\n",
    "            probs = probs.cpu()\n",
    "\n",
    "        dp = {(\"\", self.blank_token): 0.0}\n",
    "        log_probs = torch.log(probs + 1e-8)\n",
    "\n",
    "        if debug:\n",
    "            print(\"\\nStarting beam search with beam size:\", beam_size)\n",
    "\n",
    "        for t, prob in enumerate(log_probs):\n",
    "            new_dp = defaultdict(lambda: float('-inf'))\n",
    "            top_k = torch.topk(prob, k=min(beam_size, len(prob)))\n",
    "\n",
    "            if debug and t < self.max_printed_samples:\n",
    "                print(f\"\\nTimestep {t}:\")\n",
    "                print(\"Top tokens:\", [(self.ind2char[idx.item()], val.item()) \n",
    "                                    for val, idx in zip(top_k.values, top_k.indices)])\n",
    "\n",
    "            for val, ind in zip(top_k.values, top_k.indices):\n",
    "                curr_char = self.ind2char[ind.item()]\n",
    "                next_token_log_prob = val.item()\n",
    "\n",
    "                for (prefix, last_char), log_prob in dp.items():\n",
    "                    if last_char == curr_char and curr_char != \" \":\n",
    "                        new_prefix = prefix\n",
    "                    else:\n",
    "                        if curr_char != self.blank_token:\n",
    "                            if curr_char == \" \" and prefix.endswith(\" \"):\n",
    "                                continue\n",
    "                            new_prefix = prefix + curr_char\n",
    "                        else:\n",
    "                            new_prefix = prefix\n",
    "\n",
    "                    new_log_prob = log_prob + next_token_log_prob\n",
    "                    key = (new_prefix, curr_char)\n",
    "                    new_dp[key] = max(new_dp[key], new_log_prob)\n",
    "\n",
    "            if len(new_dp) > 0:\n",
    "                max_score = max(score for _, score in new_dp.items())\n",
    "                new_dp = {key: score - max_score for key, score in new_dp.items()}\n",
    "\n",
    "            dp = dict(sorted(new_dp.items(), key=lambda x: -x[1])[:beam_size])\n",
    "\n",
    "            if debug and t < 2:\n",
    "                print(\"\\nCurrent beam:\")\n",
    "                for (text, last_char), score in list(dp.items())[:3]:\n",
    "                    print(f\"Text: '{text}', Last: '{last_char}', Score: {score:.4f}\")\n",
    "\n",
    "        final_beams = []\n",
    "        for (text, _), score in dp.items():\n",
    "            text = self.tokenizer.clean_up_tokenization(text)\n",
    "            text = text.lower().strip()\n",
    "            text_len = max(1, len(text.split()))\n",
    "            normalized_score = score / text_len\n",
    "            final_beams.append((text, normalized_score))\n",
    "\n",
    "        final_beams.sort(key=lambda x: -x[1])\n",
    "        if not final_beams:\n",
    "            final_beams = [(\"\", float('-inf'))]\n",
    "\n",
    "        return final_beams[:beam_size]\n",
    "\n",
    "    def test_language_model(self):\n",
    "        \"\"\"Debug function to verify LM functionality\"\"\"\n",
    "        print(\"\\nTesting Language Model...\")\n",
    "\n",
    "        if self.lm is None:\n",
    "            print(\"Error: Language model is not loaded!\")\n",
    "            return\n",
    "\n",
    "        test_sentences = [\n",
    "            \"this is a good sentence\",\n",
    "            \"this is also a good sentence\",\n",
    "            \"thiss iss nott aa goodd sentencee\",\n",
    "            \"random word salad box cat\",\n",
    "            \"the cat sat on the mat\",\n",
    "            \"\",\n",
    "            \"a\",\n",
    "        ]\n",
    "\n",
    "        print(\"\\nTesting individual sentences:\")\n",
    "        for sentence in test_sentences:\n",
    "            score = self.score_with_lm(sentence)\n",
    "            print(f\"\\nText: '{sentence}'\")\n",
    "            print(f\"LM Score: {score:.4f}\")\n",
    "\n",
    "        test_prefixes = [\n",
    "            \"the quick brown\",\n",
    "            \"how are\",\n",
    "            \"thank\",\n",
    "            \"nice to\",\n",
    "        ]\n",
    "\n",
    "        print(\"\\nTesting word completions:\")\n",
    "        for prefix in test_prefixes:\n",
    "            print(f\"\\nPrefix: '{prefix}'\")\n",
    "            completions = [\n",
    "                prefix + \" \" + word for word in [\"you\", \"fox\", \"cat\", \"xyz\", \"meet\"]\n",
    "            ]\n",
    "            scores = [(completion, self.score_with_lm(completion)) \n",
    "                    for completion in completions]\n",
    "            scores.sort(key=lambda x: x[1], reverse=True)\n",
    "            print(\"Top completions by score:\")\n",
    "            for completion, score in scores[:3]:\n",
    "                print(f\"  '{completion}': {score:.4f}\")\n",
    "\n",
    "    def score_with_lm(self, text: str) -> float:\n",
    "        \"\"\"\n",
    "        Score text using language model, handling edge cases\n",
    "        \"\"\"\n",
    "        if self.lm is None:\n",
    "            return 0.0\n",
    "\n",
    "        if not text or len(text.strip()) == 0:\n",
    "            return float('-inf')\n",
    "\n",
    "        text = text.lower().strip()\n",
    "        return self.lm.score(text, bos=True, eos=True)\n",
    "\n",
    "    def _basic_ctc_decode(self, logits: np.ndarray, sequence_length: int) -> List[str]:\n",
    "        \"\"\"Basic CTC decoding without LM\"\"\"\n",
    "        argmax_indices = np.argmax(logits, axis=-1)\n",
    "\n",
    "        if len(argmax_indices.shape) == 0:\n",
    "            argmax_indices = np.array([argmax_indices])\n",
    "\n",
    "        if len(argmax_indices.shape) == 1:\n",
    "            argmax_indices = np.expand_dims(argmax_indices, axis=0)\n",
    "\n",
    "        predictions = []\n",
    "        for sequence in argmax_indices:\n",
    "            decoded = []\n",
    "            last_idx = None\n",
    "\n",
    "            for idx in sequence[:sequence_length]:\n",
    "                if idx != self.blank_index and idx != last_idx:\n",
    "                    decoded.append(self.ind2char[idx])\n",
    "                last_idx = idx\n",
    "\n",
    "            text = \"\".join(decoded)\n",
    "            if hasattr(self, 'processor') or self.use_bpe:\n",
    "                text = self.tokenizer.clean_up_tokenization(text)\n",
    "            predictions.append(text)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_text(text: str) -> str:\n",
    "        \"\"\"Normalize input text\"\"\"\n",
    "        # text = text.lower()\n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"[^a-z ]\", \"\", text)\n",
    "        # text = re.sub(r\"[^A-Z ]\", \"\", text)\n",
    "        return text\n",
    "\n",
    "\n",
    "    def test_decoder(self, sample_text: str = \"test decoder functionality\"):\n",
    "        \"\"\"Test the decoder setup\"\"\"\n",
    "        print(\"\\nTesting decoder configuration...\")\n",
    "\n",
    "        encoded = self.encode(sample_text)\n",
    "        decoded = self.decode(encoded[0].tolist())\n",
    "        print(f\"Original text: {sample_text}\")\n",
    "        print(f\"Basic decode: {decoded}\")\n",
    "\n",
    "        sequence_length = 50\n",
    "        vocab_size = len(self)\n",
    "        fake_logits = torch.randn(1, sequence_length, vocab_size)\n",
    "        fake_length = torch.tensor([sequence_length])\n",
    "\n",
    "        if self.decoder is not None:\n",
    "            print(\"\\nTesting pyctcdecode integration...\")\n",
    "            decoded_with_lm = self.ctc_decode(fake_logits)\n",
    "            print(f\"Decoded with LM: {decoded_with_lm}\")\n",
    "\n",
    "            print(f\"\\nBeam width: {self.beam_size}\")\n",
    "            print(f\"LM weight: {self.lm_weight}\")\n",
    "        else:\n",
    "            print(\"\\nNo language model loaded - using basic CTC decoding\")\n",
    "            basic_decoded = self._basic_ctc_decode(fake_logits.numpy(), fake_length)\n",
    "            print(f\"Basic CTC decoded: {basic_decoded[0]}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTC Text Encoder:\n",
      "pretrained_tokenizer: facebook/wav2vec2-base-960h\n",
      "lm_weight: 0.5\n",
      "beam_size: 100\n",
      "binary_path: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 4 files: 100%|█| 4/4 [00:00<00:00, 38746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added '[pad]' to the tokenizer's vocabulary.\n",
      "Modified Vocabulary (first 20 tokens): [' ', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's']\n",
      "\n",
      "Vocabulary Info:\n",
      "Size: 37\n",
      "Full Vocabulary (up to first 50 tokens): [' ', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'ä', 'å', 'é', 'ô', 'ö', 'ü', '[unk]', '[pad]', '<s>', '</s>']\n",
      "Blank token: [pad], Blank index: 34\n",
      "Sample ind2char mappings: {0: ' ', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i'}\n",
      "Sample char2ind mappings: {' ': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9}\n",
      "model_path:  None\n",
      "No language model path provided or file does not exist.\n",
      "Original text: hello world\n",
      "Encoded: tensor([[ 8,  5, 12, 12, 15,  0, 23, 15, 18, 12,  4]])\n",
      "Encoded tensor shape: torch.Size([1, 11])\n",
      "Decoded from logits: dbf</s>åévfüo\n",
      "Decoded: hello world\n",
      "Simple decoded: hello world\n",
      "\n",
      "Vocabulary and mappings check:\n",
      "Sample ind2char mappings: [(0, ' '), (1, 'a'), (2, 'b'), (3, 'c'), (4, 'd'), (5, 'e'), (6, 'f'), (7, 'g'), (8, 'h'), (9, 'i')]\n",
      "Sample char2ind mappings: [(' ', 0), ('a', 1), ('b', 2), ('c', 3), ('d', 4), ('e', 5), ('f', 6), ('g', 7), ('h', 8), ('i', 9)]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the encoder\n",
    "encoder = CTCTextEncoder(\n",
    "    pretrained_tokenizer=\"facebook/wav2vec2-base-960h\",\n",
    "    blank_token=\"[pad]\",\n",
    "    unk_token=\"[unk]\"\n",
    ")\n",
    "\n",
    "# Define test text\n",
    "test_text = \"hello world\"\n",
    "\n",
    "# Test encoding\n",
    "encoded = encoder.encode(test_text)\n",
    "print(f\"Original text: {test_text}\")\n",
    "print(f\"Encoded: {encoded}\")\n",
    "\n",
    "# Verify the structure of the encoded tensor\n",
    "print(f\"Encoded tensor shape: {encoded.shape}\")\n",
    "\n",
    "# Correct sequence length extraction\n",
    "sequence_length = encoded.size(1)  # Number of time steps in the encoded tensor\n",
    "vocab_size = len(encoder.vocab)\n",
    "\n",
    "# Create dummy logits for decoding\n",
    "logits = torch.randn(1, sequence_length, vocab_size)\n",
    "\n",
    "# Decode logits directly\n",
    "decoded_logits = encoder.decode_logits(logits)\n",
    "print(f\"Decoded from logits: {decoded_logits}\")\n",
    "\n",
    "# Test decoding from indices\n",
    "decoded = encoder.decode(encoded[0].tolist())\n",
    "print(f\"Decoded: {decoded}\")\n",
    "\n",
    "# Test simple decoding\n",
    "decoded_simple = encoder.decode_simple(encoded[0].tolist())\n",
    "print(f\"Simple decoded: {decoded_simple}\")\n",
    "\n",
    "# Check vocabulary alignment\n",
    "print(\"\\nVocabulary and mappings check:\")\n",
    "print(f\"Sample ind2char mappings: {list(encoder.ind2char.items())[:10]}\")\n",
    "print(f\"Sample char2ind mappings: {list(encoder.char2ind.items())[:10]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updating class for tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import kenlm\n",
    "from transformers import Wav2Vec2Processor, AutoProcessor\n",
    "from pyctcdecode import build_ctcdecoder\n",
    "import numpy as np\n",
    "import os\n",
    "from string import ascii_lowercase\n",
    "\n",
    "from typing import List, Tuple, Optional, Union\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "class CTCTextEncoder:\n",
    "    def __init__(\n",
    "        self,\n",
    "        arpa_path: Optional[str] = None,\n",
    "        # binary_path: Optional[str] = None,\n",
    "        # unigram_path: Optional[str] = None,\n",
    "        binary_path: Optional[str] = \"4-gram_lc_correct.bin\",\n",
    "        unigram_path: Optional[str] = \"librispeech-vocab.txt\",\n",
    "        pretrained_tokenizer: str = \"facebook/wav2vec2-base-960h\",\n",
    "        lm_weight: float = 0.5,\n",
    "        beam_size: int = 100,\n",
    "        use_lm: bool = False,     # **Added use_lm parameter**\n",
    "        use_bpe: bool = False,    # **Added use_bpe parameter**\n",
    "        blank_token: str = \"[pad]\",  # Blank token as <pad> for Wav2Vec2\n",
    "        unk_token: str = \"[unk]\",     # UNK token\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize encoder with conditional tokenizer/processor and language model.\n",
    "\n",
    "        Parameters:\n",
    "        - use_lm (bool): Whether to use the Language Model (LM) during decoding.\n",
    "        - use_bpe (bool): Whether to use Byte Pair Encoding (BPE) via tokenizer/processor.\n",
    "                           If False, perform character-based encoding/decoding without tokenizer.\n",
    "        \"\"\"\n",
    "        self.beam_size = beam_size\n",
    "        self.lm_weight = lm_weight\n",
    "        self.arpa_path = arpa_path\n",
    "        self.binary_path = binary_path\n",
    "        self.blank_token = blank_token\n",
    "        self.unk_token = unk_token\n",
    "        self.use_lm = use_lm # False\n",
    "        self.use_bpe = use_bpe # False MANUAL FOR NOW\n",
    "        # self.use_bpe = False # use_bpe # False # use_bpe\n",
    "        self.printed_samples = 0\n",
    "        self.max_printed_samples = 5\n",
    "        print('CTC Text Encoder:')\n",
    "        print('pretrained_tokenizer:', pretrained_tokenizer)\n",
    "        print('lm_weight:', lm_weight)\n",
    "        print('beam_size:', beam_size)\n",
    "        print('binary_path:', binary_path)\n",
    "        print('use_lm:', self.use_lm)\n",
    "        print('use_bpe:', self.use_bpe)\n",
    "\n",
    "        # Define blank token\n",
    "        if use_bpe:\n",
    "            self.blank_token = \"<pad>\"\n",
    "        else:\n",
    "            self.blank_token = \"\"\n",
    "        print(\"blank token: \", self.blank_token)\n",
    "\n",
    "        # Load unigrams if provided\n",
    "        self.unigrams = None\n",
    "        if unigram_path and os.path.exists(unigram_path):\n",
    "            print(f\"Loading unigrams from: {unigram_path}\")\n",
    "            with open(unigram_path, 'r', encoding='utf-8') as f:\n",
    "                self.unigrams = [line.strip().lower() for line in f if line.strip()]\n",
    "            print(f\"Loaded {len(self.unigrams)} unigrams\")\n",
    "\n",
    "        # Initialize the tokenizer or set up character-based vocab\n",
    "        self._initialize_vocabulary(pretrained_tokenizer)\n",
    "\n",
    "        # Create index mappings\n",
    "        self.ind2char = dict(enumerate(self.vocab))\n",
    "        self.char2ind = {v: k for k, v in self.ind2char.items()}\n",
    "        self.blank_index = self.char2ind.get(self.blank_token, None)\n",
    "\n",
    "        print(f\"\\nVocabulary Info:\")\n",
    "        print(f\"Size: {len(self.vocab)}\")\n",
    "        print(\"Full Vocabulary (up to first 50 tokens):\", self.vocab[:50])\n",
    "        print(f\"Blank token: {self.blank_token}, Blank index: {self.blank_index}\")\n",
    "\n",
    "        print(\"Sample ind2char mappings:\", {k: self.ind2char[k] for k in list(self.ind2char.keys())[:10]})\n",
    "        print(\"Sample char2ind mappings:\", {k: self.char2ind[k] for k in list(self.char2ind.keys())[:10]})\n",
    "\n",
    "        # **Conditionally initialize language model based on use_lm**\n",
    "        if self.use_lm:\n",
    "            self._initialize_language_model()\n",
    "            self.test_language_model()\n",
    "        else:\n",
    "            print(\"Language model usage is disabled.\")\n",
    "            self.lm = None\n",
    "            self.decoder = None\n",
    "\n",
    "    \n",
    "    def _initialize_vocabulary(self, pretrained_tokenizer: str):\n",
    "        \"\"\"\n",
    "        Initialize the vocabulary either using a tokenizer/processor (BPE) or character-based.\n",
    "\n",
    "        Parameters:\n",
    "        - pretrained_tokenizer (str): Name or path of the pretrained tokenizer.\n",
    "        \"\"\"\n",
    "        if self.use_bpe:\n",
    "            print(\"Initializing tokenizer and using BPE for encoding/decoding.\")\n",
    "            self.processor = Wav2Vec2Processor.from_pretrained(pretrained_tokenizer)\n",
    "\n",
    "            vocab_dict = self.processor.tokenizer.get_vocab()\n",
    "            sorted_vocab_dict = {k.lower(): v for k, v in sorted(vocab_dict.items(), key=lambda item: item[1])}\n",
    "\n",
    "            self.labels = list(sorted_vocab_dict.keys())\n",
    "            self.vocab = [token.replace('|', ' ') for token in self.labels]\n",
    "\n",
    "            if self.blank_token not in self.processor.tokenizer.get_vocab():\n",
    "                self.processor.tokenizer.add_tokens([self.blank_token])\n",
    "                print(f\"Added '{self.blank_token}' to the tokenizer's vocabulary.\")\n",
    "\n",
    "            if self.unk_token not in self.processor.tokenizer.get_vocab():\n",
    "                self.processor.tokenizer.add_tokens([self.unk_token])\n",
    "                print(f\"Added '{self.unk_token}' to the tokenizer's vocabulary.\")\n",
    "\n",
    "            self.ind2char = dict(enumerate(self.vocab))\n",
    "            self.char2ind = {v: k for k, v in self.ind2char.items()}\n",
    "            self.blank_index = self.char2ind.get(self.blank_token, None)\n",
    "        else:\n",
    "            print(\"Initializing character-based vocabulary without using tokenizer.\")\n",
    "\n",
    "            if self.unigrams and self.use_lm:\n",
    "                # Build alphabet from unigrams\n",
    "                # print(\"Building alphabets from unigrams\")\n",
    "                # alphabet_set = set(char for word in self.unigrams for char in word)\n",
    "                # alphabet_set.add(\" \")\n",
    "                # alphabet = sorted(list(alphabet_set))\n",
    "\n",
    "\n",
    "                print(\"Building default alphabet\")\n",
    "                alphabet = list(ascii_lowercase + \" \")\n",
    "                self.alphabet = alphabet\n",
    "\n",
    "                self.vocab = [self.blank_token] + list(alphabet)\n",
    "                print(f\"Loaded character vocabulary of size: {len(self.vocab)}\")\n",
    "            else:\n",
    "                # Default to lowercase letters and space\n",
    "                print(\"Building default alphabet\")\n",
    "                alphabet = list(ascii_lowercase + \" \")\n",
    "\n",
    "                self.alphabet = alphabet\n",
    "                # Insert blank token at the beginning of the vocabulary\n",
    "                self.vocab = [self.blank_token] + list(self.alphabet)\n",
    "                print(f\"Loaded character vocabulary of size: {len(self.vocab)}\")\n",
    "            \n",
    "            # self.vocab = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',\n",
    "            #             'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x',\n",
    "            #             'y', 'z', ' ']\n",
    "            \n",
    "            # self.vocab += [self.blank_token, self.unk_token]\n",
    "            self.ind2char = dict(enumerate(self.vocab))\n",
    "            self.char2ind = {v: k for k, v in self.ind2char.items()}\n",
    "            self.blank_index = self.char2ind.get(self.blank_token, None)\n",
    "            self.processor = None\n",
    "\n",
    "\n",
    "    def encode(self, text: str) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Encode text either using tokenizer/processor (BPE) or character-based encoding.\n",
    "\n",
    "        Parameters:\n",
    "        - text (str): The input text to encode.\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: Tensor of token indices.\n",
    "        \"\"\"\n",
    "        if self.use_bpe:\n",
    "            normalized_text = self.normalize_text(text)\n",
    "            encoded = self.processor.tokenizer(normalized_text, return_tensors=\"pt\", padding=False, truncation=False)\n",
    "            token_indices = encoded.input_ids[0].tolist()\n",
    "            return torch.tensor(token_indices).unsqueeze(0)\n",
    "        else:\n",
    "            normalized_text = self.normalize_text(text)\n",
    "            token_indices = [self.char2ind.get(char, self.char2ind.get(self.unk_token)) for char in normalized_text]\n",
    "            return torch.tensor(token_indices).unsqueeze(0)\n",
    "\n",
    "\n",
    "\n",
    "    def decode_simple(self, indices: List[int]) -> str:\n",
    "        \"\"\"\n",
    "        Simple CTC decoding without language model.\n",
    "        Collapses consecutive duplicate tokens and removes blanks.\n",
    "\n",
    "        Parameters:\n",
    "        - indices (List[int]): List of token indices.\n",
    "\n",
    "        Returns:\n",
    "        - str: Decoded text.\n",
    "        \"\"\"\n",
    "        decoded_chars = []\n",
    "        previous_idx = None\n",
    "\n",
    "        for idx in indices:\n",
    "            if idx == self.blank_index:\n",
    "                previous_idx = idx\n",
    "                continue  # Skip blank tokens\n",
    "            # if idx == previous_idx:\n",
    "            #     continue  # Skip duplicate tokens # TO CHECK IF THIS DELETION IS OK!!!\n",
    "            if 0 <= idx < len(self.ind2char):\n",
    "                char = self.ind2char[idx]\n",
    "                decoded_chars.append(char)\n",
    "            previous_idx = idx\n",
    "\n",
    "        # Join characters without spaces and convert to lowercase\n",
    "        text = \"\".join(decoded_chars).strip().lower()\n",
    "\n",
    "        if self.use_bpe and self.processor:\n",
    "            return self.processor.tokenizer.clean_up_tokenization(text)\n",
    "        else:\n",
    "            return text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _initialize_language_model(self):\n",
    "        \"\"\"Initialize language model with explicit blank token handling.\"\"\"\n",
    "        self.lm = None\n",
    "        self.decoder = None\n",
    "\n",
    "        model_path = self.binary_path if self.binary_path else self.arpa_path\n",
    "        print('model_path: ', model_path)\n",
    "        if not model_path or not os.path.exists(model_path):\n",
    "            print(\"No language model path provided or file does not exist.\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            if not self.use_bpe:\n",
    "                # Ensure blank token is at index 0\n",
    "                # self.labels = [self.blank_token] + [c for c in self.vocab if c != self.blank_token]\n",
    "                \n",
    "                # Get vocabulary without EMPTY_TOK\n",
    "                self.labels = [c for c in self.vocab if c != self.blank_token]\n",
    "                \n",
    "                print('DEBUG - labels:', self.labels) \n",
    "            \n",
    "            self.lm = kenlm.Model(model_path)\n",
    "            print(f\"Loaded {'binary' if self.binary_path else 'ARPA'} language model.\")\n",
    "\n",
    "            decoder_config = {\n",
    "                \"labels\": self.labels if self.use_bpe else self.vocab,\n",
    "                \"kenlm_model_path\": model_path,\n",
    "                \"alpha\": self.lm_weight,\n",
    "                \"beta\": 0.1,\n",
    "                \"unk_score_offset\": -10.0,\n",
    "            }\n",
    "\n",
    "            if self.unigrams:\n",
    "                print(\"\\n--- Unigrams List ---\")\n",
    "                # Save the unigrams to a file\n",
    "                with open(\"unigrams_list.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "                    for unigram in self.unigrams:\n",
    "                        f.write(f\"{unigram}\\n\")\n",
    "                print(f\"Unigrams list saved to 'unigrams_list.txt'. Total unigrams: {len(self.unigrams)}\")\n",
    "                print(\"----------------------\\n\")\n",
    "                decoder_config[\"unigrams\"] = self.unigrams\n",
    "\n",
    "            self.decoder = build_ctcdecoder(**decoder_config)\n",
    "            print(\"Successfully initialized language model and decoder.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Failed to initialize decoder: {str(e)}\")\n",
    "            self.decoder = None\n",
    "\n",
    "    \n",
    "    def decode(self, indices: List[int]) -> str:\n",
    "        \"\"\"\n",
    "        Decode indices to text using beam search decoder if available.\n",
    "\n",
    "        Parameters:\n",
    "        - indices (List[int]): List of token indices.\n",
    "\n",
    "        Returns:\n",
    "        - str: Decoded text.\n",
    "        \"\"\"\n",
    "        if self.decoder:\n",
    "            decoded_text = self.decoder.decode(indices)\n",
    "            # Convert to lower case\n",
    "            decoded_text = decoded_text.lower()\n",
    "            return decoded_text\n",
    "        else:\n",
    "            decoded_text = self.decode_simple(indices)\n",
    "            # Convert to lower case\n",
    "            decoded_text = decoded_text.lower()\n",
    "            return decoded_text  # Ensure the decoded text is returned\n",
    "\n",
    "    \n",
    "    def decode_logits(self, logits: Union[torch.Tensor, List[List[float]], np.ndarray]) -> str:\n",
    "        \"\"\"\n",
    "        Decode logits using the decoder if available, otherwise use greedy decoding.\n",
    "\n",
    "        Parameters:\n",
    "        - logits (Union[torch.Tensor, List[List[float]], np.ndarray]): Logits output from the model.\n",
    "\n",
    "        Returns:\n",
    "        - str: Decoded text.\n",
    "        \"\"\"\n",
    "        if isinstance(logits, torch.Tensor):\n",
    "            logits = logits.cpu().numpy()\n",
    "        elif isinstance(logits, list):\n",
    "            logits = np.array(logits)\n",
    "        elif not isinstance(logits, np.ndarray):\n",
    "            raise TypeError(\"logits must be a torch.Tensor, list of lists, or numpy.ndarray\")\n",
    "\n",
    "        if logits.ndim == 3:\n",
    "            logits = logits[0]\n",
    "\n",
    "        if logits.ndim != 2:\n",
    "            raise ValueError(f\"Logits should be 2D (time_steps, vocab_size), got {logits.ndim}D\")\n",
    "\n",
    "        if self.decoder:\n",
    "            decoded_text = self.decoder.decode(logits)\n",
    "            return decoded_text\n",
    "        else:\n",
    "            predicted_indices = np.argmax(logits, axis=-1).tolist()\n",
    "            return self.decode_simple(predicted_indices)\n",
    "\n",
    "    def decode_indices(self, indices: Union[torch.Tensor, List[int], np.ndarray]) -> str:\n",
    "        \"\"\"\n",
    "        Decode token indices to text using simple decoding (no LM).\n",
    "\n",
    "        Parameters:\n",
    "        - indices (Union[torch.Tensor, List[int], np.ndarray]): Token indices.\n",
    "\n",
    "        Returns:\n",
    "        - str: Decoded text.\n",
    "        \"\"\"\n",
    "        if isinstance(indices, torch.Tensor):\n",
    "            indices = indices.squeeze().tolist()\n",
    "        elif isinstance(indices, np.ndarray):\n",
    "            indices = indices.tolist()\n",
    "        elif not isinstance(indices, list):\n",
    "            raise TypeError(\"decode_indices expects a list, torch.Tensor, or numpy.ndarray.\")\n",
    "\n",
    "        return self.decode_simple(indices)\n",
    "\n",
    "    def ctc_decode(self, logits: Union[torch.Tensor, List[int], np.ndarray]) -> str:\n",
    "        \"\"\"\n",
    "        Perform CTC decoding on logits.\n",
    "\n",
    "        Parameters:\n",
    "        - logits (Union[torch.Tensor, List[int], np.ndarray]): Logits or token indices.\n",
    "\n",
    "        Returns:\n",
    "        - str: Decoded text.\n",
    "        \"\"\"\n",
    "        if isinstance(logits, np.ndarray):\n",
    "            logits = torch.from_numpy(logits)\n",
    "        elif isinstance(logits, list):\n",
    "            logits = torch.tensor(logits)\n",
    "\n",
    "        if logits.dim() == 3:\n",
    "            logits = logits[0]  # Reduce to 2D (sequence length, vocab size)\n",
    "\n",
    "        if self.use_bpe:\n",
    "            if self.use_lm and self.decoder:\n",
    "                # Use LM if available\n",
    "                return self.decoder.decode(logits)\n",
    "            else:\n",
    "                # Use tokenizer-based decoding\n",
    "                predicted_indices = torch.argmax(logits, axis=-1).tolist()\n",
    "                return self.decode(predicted_indices)\n",
    "        else:\n",
    "            # Use character-based decoding\n",
    "            predicted_indices = torch.argmax(logits, axis=-1).tolist()\n",
    "            return self.decode_simple(predicted_indices)\n",
    "\n",
    "    \n",
    "\n",
    "    def ctc_beam_search(self, probs, beam_size, use_lm = False, debug: bool = False) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Beam search with optional Language Model support.\n",
    "\n",
    "        Parameters:\n",
    "        - probs: Probability distributions over tokens.\n",
    "        - debug (bool): Whether to print debug information.\n",
    "\n",
    "        Returns:\n",
    "        - List[Tuple[str, float]]: List of decoded text with scores.\n",
    "        \"\"\"\n",
    "        beam_size = self.beam_size\n",
    "        debug = False\n",
    "\n",
    "        if self.use_lm and self.decoder is not None:\n",
    "            try:\n",
    "                if isinstance(probs, torch.Tensor):\n",
    "                    probs = probs.cpu().numpy()\n",
    "                elif isinstance(probs, list):\n",
    "                    probs = np.array(probs)\n",
    "                elif isinstance(probs, np.ndarray):\n",
    "                    pass\n",
    "                else:\n",
    "                    raise TypeError(\"probs must be a torch.Tensor, list, or numpy.ndarray\")\n",
    "\n",
    "                beams = self.decoder.decode_beams(\n",
    "                    probs,\n",
    "                    beam_prune_logp=-10.0,\n",
    "                    token_min_logp=-5.0,\n",
    "                    hotwords=[],\n",
    "                    hotword_weight=10.0,\n",
    "                )\n",
    "\n",
    "                formatted_beams = []\n",
    "                for beam in beams[:beam_size]:\n",
    "                    text = beam[0]\n",
    "                    acoustic_score = beam[3]\n",
    "                    lm_score = beam[4]\n",
    "\n",
    "                    if self.use_bpe and self.processor:\n",
    "                        text = self.processor.tokenizer.clean_up_tokenization(text)\n",
    "                    text = text.lower().strip()\n",
    "\n",
    "                    combined_score = (1 - self.lm_weight) * acoustic_score + self.lm_weight * lm_score\n",
    "                    text_len = max(1, len(text.split())) if self.use_bpe else max(1, len(text))\n",
    "                    normalized_score = combined_score / text_len\n",
    "\n",
    "                    formatted_beams.append((text, normalized_score))\n",
    "\n",
    "                if debug:\n",
    "                    print(\"\\nFormatted beam results with Language Model:\")\n",
    "                    for text, score in formatted_beams[:3]:\n",
    "                        print(f\"Text: '{text}', Score: {score:.4f}\")\n",
    "\n",
    "                if formatted_beams:\n",
    "                    return sorted(formatted_beams, key=lambda x: -x[1])\n",
    "                else:\n",
    "                    print(\"No valid beams found, falling back to standard beam search\")\n",
    "                    return self._standard_beam_search(probs, debug)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Beam search with LM failed: {str(e)}, falling back to standard beam search\")\n",
    "                return self._standard_beam_search(probs, debug)\n",
    "        else:\n",
    "            return self._standard_beam_search(probs, debug)\n",
    "\n",
    "    # def _standard_beam_search(self, probs, debug: bool = False) -> List[Tuple[str, float]]:\n",
    "    #     \"\"\"\n",
    "    #     Original beam search implementation without Language Model.\n",
    "\n",
    "    #     Parameters:\n",
    "    #     - probs: Probability distributions over tokens.\n",
    "    #     - debug (bool): Whether to print debug information.\n",
    "\n",
    "    #     Returns:\n",
    "    #     - List[Tuple[str, float]]: List of decoded text with scores.\n",
    "    #     \"\"\"\n",
    "    #     beam_size = self.beam_size\n",
    "\n",
    "    #     debug = True # TEMP\n",
    "\n",
    "    #     if isinstance(probs, np.ndarray):\n",
    "    #         probs = torch.from_numpy(probs)\n",
    "\n",
    "    #     if probs.device != torch.device('cpu'):\n",
    "    #         probs = probs.cpu()\n",
    "\n",
    "    #     dp = {(\"\", self.blank_token): 0.0}\n",
    "    #     log_probs = torch.log(probs + 1e-8)\n",
    "\n",
    "    #     if debug:\n",
    "    #         print(\"\\nStarting beam search with beam size:\", beam_size)\n",
    "\n",
    "    #     for t, prob in enumerate(log_probs):\n",
    "    #         new_dp = defaultdict(lambda: float('-inf'))\n",
    "    #         top_k = torch.topk(prob, k=min(beam_size, len(prob)))\n",
    "\n",
    "    #         if debug and t < self.max_printed_samples:\n",
    "    #             print(f\"\\nTimestep {t}:\")\n",
    "    #             print(\"Top tokens:\", [(self.ind2char[idx.item()], val.item()) \n",
    "    #                                 for val, idx in zip(top_k.values, top_k.indices)])\n",
    "\n",
    "    #         for val, ind in zip(top_k.values, top_k.indices):\n",
    "    #             curr_char = self.ind2char[ind.item()]\n",
    "    #             next_token_log_prob = val.item()\n",
    "\n",
    "    #             for (prefix, last_char), log_prob in dp.items():\n",
    "    #                 if last_char == curr_char and curr_char != \" \":\n",
    "    #                     new_prefix = prefix\n",
    "    #                 else:\n",
    "    #                     if curr_char != self.blank_token:\n",
    "    #                         if curr_char == \" \" and prefix.endswith(\" \"):\n",
    "    #                             continue\n",
    "    #                         new_prefix = prefix + curr_char\n",
    "    #                     else:\n",
    "    #                         new_prefix = prefix\n",
    "\n",
    "    #                 new_log_prob = log_prob + next_token_log_prob\n",
    "    #                 key = (new_prefix, curr_char)\n",
    "    #                 new_dp[key] = max(new_dp[key], new_log_prob)\n",
    "\n",
    "    #         if len(new_dp) > 0:\n",
    "    #             max_score = max(score for _, score in new_dp.items())\n",
    "    #             new_dp = {key: score - max_score for key, score in new_dp.items()}\n",
    "\n",
    "    #         dp = dict(sorted(new_dp.items(), key=lambda x: -x[1])[:beam_size])\n",
    "\n",
    "    #         if debug and t < 2:\n",
    "    #             print(\"\\nCurrent beam:\")\n",
    "    #             for (text, last_char), score in list(dp.items())[:3]:\n",
    "    #                 print(f\"Text: '{text}', Last: '{last_char}', Score: {score:.4f}\")\n",
    "\n",
    "    #     final_beams = []\n",
    "    #     for (text, _), score in dp.items():\n",
    "    #         if self.use_bpe and self.processor:\n",
    "    #             text = self.processor.tokenizer.clean_up_tokenization(text)\n",
    "    #         text = text.lower().strip()\n",
    "    #         text_len = max(1, len(text.split())) if self.use_bpe else max(1, len(text))\n",
    "    #         normalized_score = score / text_len\n",
    "    #         final_beams.append((text, normalized_score))\n",
    "\n",
    "    #     final_beams.sort(key=lambda x: -x[1])\n",
    "    #     if not final_beams:\n",
    "    #         final_beams = [(\"\", float('-inf'))]\n",
    "\n",
    "    #     return final_beams[:beam_size]\n",
    "\n",
    "\n",
    "    # def _standard_beam_search(self, probs, beam_size: int = 10, debug: bool = False) -> List[Tuple[str, float]]:\n",
    "    #     \"\"\"Original beam search implementation with improved debugging\"\"\"\n",
    "    #     # Convert input to torch tensor if needed\n",
    "        \n",
    "    #     # beam_size = self.beam_size\n",
    "    #     beam_size = 10\n",
    "        \n",
    "    #     if isinstance(probs, np.ndarray):\n",
    "    #         probs = torch.from_numpy(probs)\n",
    "\n",
    "    #     # Ensure probs is on CPU\n",
    "    #     if probs.device != torch.device('cpu'):\n",
    "    #         probs = probs.cpu()\n",
    "\n",
    "    #     # Initialize beam with empty string\n",
    "    #     dp = {(\"\", self.blank_token): 0.0}  # Using log probs\n",
    "\n",
    "    #     # Convert to log probabilities\n",
    "    #     log_probs = torch.log(probs + 1e-8)\n",
    "        \n",
    "    #     if debug:\n",
    "    #         print(\"\\nStarting beam search with beam size:\", beam_size)\n",
    "        \n",
    "    #     for t, prob in enumerate(log_probs):\n",
    "    #         new_dp = defaultdict(lambda: float('-inf'))\n",
    "            \n",
    "    #         # Get top-k tokens for this timestep\n",
    "    #         top_k = torch.topk(prob, k=min(beam_size, len(prob)))\n",
    "            \n",
    "    #         if debug and t < 2:  # Print first two timesteps\n",
    "    #             print(f\"\\nTimestep {t}:\")\n",
    "    #             print(\"Top tokens:\", [(self.ind2char[idx.item()], val.item()) \n",
    "    #                                 for val, idx in zip(top_k.values, top_k.indices)])\n",
    "            \n",
    "    #         # Only expand using top-k tokens\n",
    "    #         for val, ind in zip(top_k.values, top_k.indices):\n",
    "    #             curr_char = self.ind2char[ind.item()]\n",
    "    #             next_token_log_prob = val.item()\n",
    "                \n",
    "    #             for (prefix, last_char), log_prob in dp.items():\n",
    "    #                 # Skip repeated characters (except spaces)\n",
    "    #                 if last_char == curr_char and curr_char != \" \":\n",
    "    #                     new_prefix = prefix\n",
    "    #                 else:\n",
    "    #                     if curr_char != self.blank_token:\n",
    "    #                         # Handle spaces better\n",
    "    #                         if curr_char == \" \" and prefix.endswith(\" \"):\n",
    "    #                             continue\n",
    "    #                         new_prefix = prefix + curr_char\n",
    "    #                     else:\n",
    "    #                         new_prefix = prefix\n",
    "                    \n",
    "    #                 # Update score\n",
    "    #                 new_log_prob = log_prob + next_token_log_prob\n",
    "    #                 key = (new_prefix, curr_char)\n",
    "    #                 new_dp[key] = max(new_dp[key], new_log_prob)\n",
    "            \n",
    "    #         # Normalize scores\n",
    "    #         if len(new_dp) > 0:\n",
    "    #             max_score = max(score for _, score in new_dp.items())\n",
    "    #             new_dp = {key: score - max_score for key, score in new_dp.items()}\n",
    "            \n",
    "    #         # Truncate beams\n",
    "    #         dp = dict(sorted(new_dp.items(), key=lambda x: -x[1])[:beam_size])\n",
    "            \n",
    "    #         if debug and t < 2:  # Print beam state for first two timesteps\n",
    "    #             print(\"\\nCurrent beam:\")\n",
    "    #             for (text, last_char), score in list(dp.items())[:3]:\n",
    "    #                 print(f\"Text: '{text}', Last: '{last_char}', Score: {score:.4f}\")\n",
    "        \n",
    "    #     # Format final results\n",
    "    #     final_beams = []\n",
    "    #     for (text, _), score in dp.items():\n",
    "    #         # Clean up text\n",
    "    #         if self.use_bpe:\n",
    "    #             text = self.tokenizer.clean_up_tokenization(text)\n",
    "    #         else:\n",
    "    #             text = ' '.join(text.split())\n",
    "                \n",
    "    #         if not text.strip():  # Skip empty results\n",
    "    #             continue\n",
    "                \n",
    "    #         # Length normalization\n",
    "    #         text_len = max(1, len(text.split()))\n",
    "    #         normalized_score = score / text_len\n",
    "            \n",
    "    #         final_beams.append((text, normalized_score))\n",
    "            \n",
    "    #     # Sort and ensure we have results\n",
    "    #     final_beams.sort(key=lambda x: -x[1])\n",
    "    #     if not final_beams:\n",
    "    #         final_beams = [(\"\", float('-inf'))]\n",
    "        \n",
    "    #     return final_beams[:beam_size]\n",
    "\n",
    "    def _standard_beam_search(self, probs, beam_size: int = 10, debug: bool = False) -> List[Tuple[str, float]]:\n",
    "        beam_size = self.beam_size\n",
    "        beam_size = 100\n",
    "        \n",
    "        if isinstance(probs, np.ndarray):\n",
    "            probs = torch.from_numpy(probs)\n",
    "        if probs.device != torch.device('cpu'):\n",
    "            probs = probs.cpu()\n",
    "\n",
    "        # Initialize beam with empty string\n",
    "        dp = {(\"\", self.blank_token): 0.0}\n",
    "        \n",
    "        # Convert to log probabilities more carefully\n",
    "        log_probs = torch.log(torch.clamp(probs, min=1e-8))\n",
    "        \n",
    "        for t, prob in enumerate(log_probs):\n",
    "            new_dp = defaultdict(lambda: float('-inf'))\n",
    "            \n",
    "            # Consider all tokens, not just top-k\n",
    "            token_indices = range(len(prob))\n",
    "            token_log_probs = prob\n",
    "            \n",
    "            for ind in token_indices:\n",
    "                curr_char = self.ind2char[ind]\n",
    "                next_token_log_prob = token_log_probs[ind].item()\n",
    "                \n",
    "                for (prefix, last_char), log_prob in dp.items():\n",
    "                    # Modified prefix handling\n",
    "                    if curr_char == self.blank_token:\n",
    "                        # Blank token: keep prefix unchanged\n",
    "                        new_prefix = prefix\n",
    "                    elif last_char == curr_char and curr_char != \" \":\n",
    "                        # Repeated char: merge only if not space\n",
    "                        new_prefix = prefix\n",
    "                    else:\n",
    "                        # New char: add to prefix\n",
    "                        new_prefix = prefix + curr_char\n",
    "                    \n",
    "                    # Update score without aggressive normalization\n",
    "                    new_log_prob = log_prob + next_token_log_prob\n",
    "                    key = (new_prefix, curr_char)\n",
    "                    new_dp[key] = max(new_dp[key], new_log_prob)\n",
    "            \n",
    "            # Less aggressive score normalization\n",
    "            if len(new_dp) > 0:\n",
    "                max_score = max(score for _, score in new_dp.items())\n",
    "                new_dp = {key: score - max_score/2 for key, score in new_dp.items()}\n",
    "            \n",
    "            # Keep top beams\n",
    "            dp = dict(sorted(new_dp.items(), key=lambda x: -x[1])[:beam_size])\n",
    "            \n",
    "            if debug and t < 2:\n",
    "                print(f\"\\nTimestep {t}:\")\n",
    "                top_beams = list(dp.items())[:3]\n",
    "                for (text, last_char), score in top_beams:\n",
    "                    print(f\"Text: '{text}', Last: '{last_char}', Score: {score:.4f}\")\n",
    "        \n",
    "        # Modified final scoring\n",
    "        final_beams = []\n",
    "        for (text, _), score in dp.items():\n",
    "            if self.use_bpe:\n",
    "                text = self.tokenizer.clean_up_tokenization(text)\n",
    "            else:\n",
    "                text = ' '.join(text.split())\n",
    "                \n",
    "            if not text.strip():\n",
    "                continue\n",
    "                \n",
    "            # Modified length normalization\n",
    "            text_len = max(1, len(text))  # Use character length instead of word length\n",
    "            normalized_score = score / (text_len ** 0.5)  # Square root length normalization\n",
    "            \n",
    "            final_beams.append((text, normalized_score))\n",
    "        \n",
    "        final_beams.sort(key=lambda x: -x[1])\n",
    "        if not final_beams:\n",
    "            final_beams = [(\"\", float('-inf'))]\n",
    "        \n",
    "        return final_beams[:beam_size]\n",
    "\n",
    "\n",
    "    # def _standard_beam_search(self, probs, beam_size=50, debug=False):\n",
    "    #     \"\"\"\n",
    "    #     Beam search implementation without Language Model (use_lm=False).\n",
    "        \n",
    "    #     Parameters:\n",
    "    #     - probs: Probability distributions over tokens (can be numpy array or tensor).\n",
    "    #     - beam_size: Maximum number of beams to keep at each timestep.\n",
    "    #     - debug (bool): Whether to print debug information.\n",
    "\n",
    "    #     Returns:\n",
    "    #     - List[Tuple[str, float]]: Decoded beams with their scores.\n",
    "    #     \"\"\"\n",
    "    #     dp = {(\"\", self.blank_token): 0.0}  # Initialize with log probabilities\n",
    "\n",
    "    #     # Convert probabilities to a PyTorch tensor if needed\n",
    "    #     if isinstance(probs, np.ndarray):\n",
    "    #         probs = torch.tensor(probs)\n",
    "\n",
    "    #     log_probs = torch.log(probs + 1e-8)  # Add epsilon to avoid log(0)\n",
    "\n",
    "    #     for t, next_token_log_probs in enumerate(log_probs):\n",
    "    #         if debug:\n",
    "    #             print(f\"\\nTimestep {t}:\")\n",
    "    #             top_k = torch.topk(torch.exp(next_token_log_probs), k=5)  # Convert back to prob for debugging\n",
    "    #             print(\"Top 5 tokens and probs:\")\n",
    "    #             for i, (p, idx) in enumerate(zip(top_k.values, top_k.indices)):\n",
    "    #                 token = self.ind2char[idx.item()]\n",
    "    #                 print(f\"{token}: {p:.4f}\")\n",
    "\n",
    "    #         # Expand and merge paths\n",
    "    #         dp = self.expand_and_merge_path(dp, next_token_log_probs)\n",
    "\n",
    "    #         # Normalize scores periodically to prevent underflow\n",
    "    #         if len(dp) > 0:\n",
    "    #             max_score = max(score for _, score in dp.items())\n",
    "    #             dp = {key: score - max_score for key, score in dp.items()}\n",
    "\n",
    "    #         # Prune to keep top beams\n",
    "    #         dp = self.truncate_paths(dp, beam_size)\n",
    "\n",
    "    #         if debug:\n",
    "    #             print(\"\\nCurrent beam state:\")\n",
    "    #             for (text, last_char), score in list(dp.items())[:5]:\n",
    "    #                 print(f\"Text: '{text}', Last: '{last_char}', Score: {score:.4f}\")\n",
    "\n",
    "    #     # Prepare final results with length normalization\n",
    "    #     final_beams = []\n",
    "    #     for (text, _), score in sorted(dp.items(), key=lambda x: -x[1])[:beam_size]:\n",
    "    #         # Clean up text\n",
    "    #         text = ' '.join(text.split())\n",
    "    #         # Length normalization\n",
    "    #         text_len = max(1, len(text.split()))\n",
    "    #         normalized_score = score / text_len\n",
    "    #         final_beams.append((text, normalized_score))\n",
    "\n",
    "    #     if debug:\n",
    "    #         print(\"\\nFinal beams:\")\n",
    "    #         for text, score in final_beams[:3]:\n",
    "    #             print(f\"Text: '{text}', Score: {score:.4f}\")\n",
    "\n",
    "    #     return final_beams\n",
    "\n",
    "    # def truncate_paths(self, dp, beam_size):\n",
    "    #     \"\"\"Regular beam search truncation\"\"\"\n",
    "    #     return dict(sorted(dp.items(), key=lambda x: -x[1])[:beam_size])    \n",
    "\n",
    "    # def expand_and_merge_path(self, dp, next_token_log_probs):\n",
    "    #     \"\"\"\n",
    "    #     Expand and merge paths for CTC decoding without LM.\n",
    "\n",
    "    #     Parameters:\n",
    "    #     - dp: Current beams with their scores.\n",
    "    #     - next_token_log_probs: Log probabilities of the next tokens.\n",
    "\n",
    "    #     Returns:\n",
    "    #     - Updated beams with scores.\n",
    "    #     \"\"\"\n",
    "    #     new_dp = defaultdict(lambda: float('-inf'))  # Initialize with log-prob -inf\n",
    "\n",
    "    #     for ind, next_token_log_prob in enumerate(next_token_log_probs):\n",
    "    #         if ind >= len(self.ind2char):  # Skip invalid indices\n",
    "    #             continue\n",
    "    #         curr_char = self.ind2char[ind]  # Get character corresponding to the index\n",
    "\n",
    "    #         for (prefix, last_char), log_prob in dp.items():\n",
    "    #             # Avoid consecutive duplicates unless blank token\n",
    "    #             if last_char == curr_char and curr_char != self.blank_token:\n",
    "    #                 new_prefix = prefix\n",
    "    #             else:\n",
    "    #                 if curr_char != self.blank_token:\n",
    "    #                     if curr_char == \" \" and prefix.endswith(\" \"):  # Avoid consecutive spaces\n",
    "    #                         continue\n",
    "    #                     new_prefix = prefix + curr_char\n",
    "    #                 else:\n",
    "    #                     new_prefix = prefix\n",
    "\n",
    "    #             # Update score for the new prefix\n",
    "    #             new_log_prob = log_prob + next_token_log_prob\n",
    "    #             new_dp[(new_prefix, curr_char)] = max(new_dp[(new_prefix, curr_char)], new_log_prob)\n",
    "\n",
    "    #     return new_dp\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def test_language_model(self):\n",
    "        \"\"\"Debug function to verify LM functionality\"\"\"\n",
    "        print(\"\\nTesting Language Model...\")\n",
    "\n",
    "        if self.lm is None:\n",
    "            print(\"Error: Language model is not loaded!\")\n",
    "            return\n",
    "\n",
    "        test_sentences = [\n",
    "            \"this is a good sentence\",\n",
    "            \"this is also a good sentence\",\n",
    "            \"thiss iss nott aa goodd sentencee\",\n",
    "            \"random word salad box cat\",\n",
    "            \"the cat sat on the mat\",\n",
    "            \"\",\n",
    "            \"a\",\n",
    "        ]\n",
    "\n",
    "        print(\"\\nTesting individual sentences:\")\n",
    "        for sentence in test_sentences:\n",
    "            score = self.score_with_lm(sentence)\n",
    "            print(f\"\\nText: '{sentence}'\")\n",
    "            print(f\"LM Score: {score:.4f}\")\n",
    "\n",
    "        test_prefixes = [\n",
    "            \"the quick brown\",\n",
    "            \"how are\",\n",
    "            \"thank\",\n",
    "            \"nice to\",\n",
    "        ]\n",
    "\n",
    "        print(\"\\nTesting word completions:\")\n",
    "        for prefix in test_prefixes:\n",
    "            print(f\"\\nPrefix: '{prefix}'\")\n",
    "            completions = [\n",
    "                prefix + \" \" + word for word in [\"you\", \"fox\", \"cat\", \"xyz\", \"meet\"]\n",
    "            ]\n",
    "            scores = [(completion, self.score_with_lm(completion)) \n",
    "                    for completion in completions]\n",
    "            scores.sort(key=lambda x: x[1], reverse=True)\n",
    "            print(\"Top completions by score:\")\n",
    "            for completion, score in scores[:3]:\n",
    "                print(f\"  '{completion}': {score:.4f}\")\n",
    "\n",
    "    def score_with_lm(self, text: str) -> float:\n",
    "        \"\"\"\n",
    "        Score text using language model, handling edge cases.\n",
    "\n",
    "        Parameters:\n",
    "        - text (str): The input text to score.\n",
    "\n",
    "        Returns:\n",
    "        - float: LM score.\n",
    "        \"\"\"\n",
    "        if self.lm is None:\n",
    "            return 0.0\n",
    "\n",
    "        if not text or len(text.strip()) == 0:\n",
    "            return float('-inf')\n",
    "\n",
    "        text = text.lower().strip()\n",
    "        return self.lm.score(text, bos=True, eos=True)\n",
    "\n",
    "    def _basic_ctc_decode(self, logits: np.ndarray, sequence_length: int) -> List[str]:\n",
    "        \"\"\"\n",
    "        Basic CTC decoding without LM.\n",
    "\n",
    "        Parameters:\n",
    "        - logits (np.ndarray): Logits from the model.\n",
    "        - sequence_length (int): Length of the sequence to decode.\n",
    "\n",
    "        Returns:\n",
    "        - List[str]: Decoded text.\n",
    "        \"\"\"\n",
    "        argmax_indices = np.argmax(logits, axis=-1)\n",
    "\n",
    "        if len(argmax_indices.shape) == 0:\n",
    "            argmax_indices = np.array([argmax_indices])\n",
    "\n",
    "        if len(argmax_indices.shape) == 1:\n",
    "            argmax_indices = np.expand_dims(argmax_indices, axis=0)\n",
    "\n",
    "        predictions = []\n",
    "        for sequence in argmax_indices:\n",
    "            decoded = []\n",
    "            last_idx = None\n",
    "\n",
    "            for idx in sequence[:sequence_length]:\n",
    "                if idx != self.blank_index and idx != last_idx:\n",
    "                    decoded.append(self.ind2char[idx])\n",
    "                last_idx = idx\n",
    "\n",
    "            text = \"\".join(decoded)\n",
    "            if self.use_bpe and self.processor:\n",
    "                text = self.processor.tokenizer.clean_up_tokenization(text)\n",
    "            predictions.append(text)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_text(text: str) -> str:\n",
    "        \"\"\"Normalize input text.\"\"\"\n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"[^a-z ]\", \"\", text)\n",
    "        return text\n",
    "\n",
    "    def test_decoder(self, sample_text: str = \"test decoder functionality\"):\n",
    "        \"\"\"Test the decoder setup.\"\"\"\n",
    "        print(\"\\nTesting decoder configuration...\")\n",
    "\n",
    "        encoded = self.encode(sample_text)\n",
    "        decoded = self.decode(encoded[0].tolist())\n",
    "        print(f\"Original text: {sample_text}\")\n",
    "        print(f\"Basic decode: {decoded}\")\n",
    "\n",
    "        sequence_length = 50\n",
    "        vocab_size = len(self)\n",
    "        fake_logits = torch.randn(1, sequence_length, vocab_size)\n",
    "        fake_length = torch.tensor([sequence_length])\n",
    "\n",
    "        if self.decoder is not None:\n",
    "            print(\"\\nTesting pyctcdecode integration...\")\n",
    "            decoded_with_lm = self.ctc_decode(fake_logits)\n",
    "            print(f\"Decoded with LM: {decoded_with_lm}\")\n",
    "\n",
    "            print(f\"\\nBeam width: {self.beam_size}\")\n",
    "            print(f\"LM weight: {self.lm_weight}\")\n",
    "        else:\n",
    "            print(\"\\nNo language model loaded - using basic CTC decoding\")\n",
    "            basic_decoded = self._basic_ctc_decode(fake_logits.numpy(), sequence_length)\n",
    "            print(f\"Basic CTC decoded: {basic_decoded[0]}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the size of the vocabulary.\"\"\"\n",
    "        return len(self.vocab)\n",
    "\n",
    "    def ctc_decode(self, logits: Union[torch.Tensor, List[int], np.ndarray]) -> str:\n",
    "        \"\"\"\n",
    "        Perform CTC decoding on logits.\n",
    "\n",
    "        Parameters:\n",
    "        - logits (Union[torch.Tensor, List[int], np.ndarray]): Logits or token indices.\n",
    "\n",
    "        Returns:\n",
    "        - str: Decoded text.\n",
    "        \"\"\"\n",
    "        if isinstance(logits, np.ndarray):\n",
    "            logits = torch.from_numpy(logits)\n",
    "        elif isinstance(logits, list):\n",
    "            logits = torch.tensor(logits)\n",
    "\n",
    "        if logits.dim() == 3:\n",
    "            decoded_text = self.decode_logits(logits)\n",
    "            return decoded_text\n",
    "        elif logits.dim() == 2:\n",
    "            decoded_text = self.decode_logits(logits)\n",
    "            return decoded_text\n",
    "        elif logits.dim() == 1:\n",
    "            decoded_text = self.decode_indices(logits)\n",
    "            return decoded_text\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported logits shape: {logits.shape}. Expected 1D, 2D, or 3D.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# import re\n",
    "# from collections import defaultdict\n",
    "# import torch\n",
    "# import kenlm\n",
    "# from transformers import Wav2Vec2Processor, AutoProcessor\n",
    "# from pyctcdecode import build_ctcdecoder\n",
    "# import numpy as np\n",
    "# import os\n",
    "\n",
    "# from typing import List, Tuple, Optional, Union\n",
    "\n",
    "# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# class CTCTextEncoder:\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         arpa_path: Optional[str] = None,\n",
    "#         binary_path: Optional[str] = None,\n",
    "#         unigram_path: Optional[str] = None,\n",
    "#         pretrained_tokenizer: str = \"facebook/wav2vec2-base-960h\",\n",
    "#         lm_weight: float = 0.5,\n",
    "#         beam_size: int = 100,\n",
    "#         use_lm: bool = False,     # **Added use_lm parameter**\n",
    "#         use_bpe: bool = False,    # **Added use_bpe parameter**\n",
    "#         blank_token: str = \"[pad]\",  # Blank token as <pad> for Wav2Vec2\n",
    "#         unk_token: str = \"[unk]\",     # UNK token\n",
    "#         **kwargs\n",
    "#     ):\n",
    "#         \"\"\"\n",
    "#         Initialize encoder with conditional tokenizer/processor and language model.\n",
    "\n",
    "#         Parameters:\n",
    "#         - use_lm (bool): Whether to use the Language Model (LM) during decoding.\n",
    "#         - use_bpe (bool): Whether to use Byte Pair Encoding (BPE) via tokenizer/processor.\n",
    "#                            If False, perform character-based encoding/decoding without tokenizer.\n",
    "#         \"\"\"\n",
    "#         self.beam_size = beam_size\n",
    "#         self.lm_weight = lm_weight\n",
    "#         self.arpa_path = arpa_path\n",
    "#         self.binary_path = binary_path\n",
    "#         self.blank_token = blank_token\n",
    "#         self.unk_token = unk_token\n",
    "#         self.use_lm = use_lm # False\n",
    "#         self.use_bpe = False # False MANUAL FOR NOW\n",
    "#         # self.use_bpe = False # use_bpe # False # use_bpe\n",
    "#         self.printed_samples = 0\n",
    "#         self.max_printed_samples = 5\n",
    "#         print('CTC Text Encoder:')\n",
    "#         print('pretrained_tokenizer:', pretrained_tokenizer)\n",
    "#         print('lm_weight:', lm_weight)\n",
    "#         print('beam_size:', beam_size)\n",
    "#         print('binary_path:', binary_path)\n",
    "#         print('use_lm:', self.use_lm)\n",
    "#         print('use_bpe:', self.use_bpe)\n",
    "\n",
    "#         # Load unigrams if provided\n",
    "#         self.unigrams = None\n",
    "#         if unigram_path and os.path.exists(unigram_path):\n",
    "#             print(f\"Loading unigrams from: {unigram_path}\")\n",
    "#             with open(unigram_path, 'r', encoding='utf-8') as f:\n",
    "#                 self.unigrams = [line.strip().lower() for line in f if line.strip()]\n",
    "#             print(f\"Loaded {len(self.unigrams)} unigrams\")\n",
    "\n",
    "#         # Initialize the tokenizer or set up character-based vocab\n",
    "#         self._initialize_vocabulary(pretrained_tokenizer)\n",
    "\n",
    "#         # Create index mappings\n",
    "#         self.ind2char = dict(enumerate(self.vocab))\n",
    "#         self.char2ind = {v: k for k, v in self.ind2char.items()}\n",
    "#         self.blank_index = self.char2ind.get(self.blank_token, None)\n",
    "\n",
    "#         print(f\"\\nVocabulary Info:\")\n",
    "#         print(f\"Size: {len(self.vocab)}\")\n",
    "#         print(\"Full Vocabulary (up to first 50 tokens):\", self.vocab[:50])\n",
    "#         print(f\"Blank token: {self.blank_token}, Blank index: {self.blank_index}\")\n",
    "\n",
    "#         print(\"Sample ind2char mappings:\", {k: self.ind2char[k] for k in list(self.ind2char.keys())[:10]})\n",
    "#         print(\"Sample char2ind mappings:\", {k: self.char2ind[k] for k in list(self.char2ind.keys())[:10]})\n",
    "\n",
    "#         # **Conditionally initialize language model based on use_lm**\n",
    "#         if self.use_lm:\n",
    "#             self._initialize_language_model()\n",
    "#         else:\n",
    "#             print(\"Language model usage is disabled.\")\n",
    "#             self.lm = None\n",
    "#             self.decoder = None\n",
    "\n",
    "#     def _initialize_vocabulary(self, pretrained_tokenizer: str):\n",
    "#         \"\"\"\n",
    "#         Initialize the vocabulary either using a tokenizer/processor (BPE) or character-based.\n",
    "\n",
    "#         Parameters:\n",
    "#         - pretrained_tokenizer (str): Name or path of the pretrained tokenizer.\n",
    "#         \"\"\"\n",
    "#         if self.use_bpe:\n",
    "#             print(\"Initializing tokenizer and using BPE for encoding/decoding.\")\n",
    "#             # Initialize AutoProcessor (you can switch to Wav2Vec2Processor if preferred)\n",
    "#             # self.processor = AutoProcessor.from_pretrained(pretrained_tokenizer)\n",
    "#             self.processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "#             # Get the vocabulary from tokenizer and process it\n",
    "#             vocab_dict = self.processor.tokenizer.get_vocab()\n",
    "#             sorted_vocab_dict = {k.lower(): v for k, v in sorted(vocab_dict.items(), key=lambda item: item[1])}\n",
    "\n",
    "#             self.labels = list(sorted_vocab_dict.keys())\n",
    "\n",
    "#             # Replace '|' with ' ' in vocab\n",
    "#             original_vocab = list(sorted_vocab_dict.keys())\n",
    "#             self.vocab = [t.replace('|', ' ') for t in original_vocab]\n",
    "\n",
    "#             # Add the unique blank token to the tokenizer's vocabulary if not present\n",
    "#             if self.blank_token not in self.processor.tokenizer.get_vocab():\n",
    "#                 self.processor.tokenizer.add_tokens([self.blank_token])\n",
    "#                 print(f\"Added '{self.blank_token}' to the tokenizer's vocabulary.\")\n",
    "#             else:\n",
    "#                 print(f\"'{self.blank_token}' already exists in the tokenizer's vocabulary.\")\n",
    "\n",
    "#             # Add the UNK token if not present\n",
    "#             if self.unk_token not in self.processor.tokenizer.get_vocab():\n",
    "#                 self.processor.tokenizer.add_tokens([self.unk_token])\n",
    "#                 print(f\"Added '{self.unk_token}' to the tokenizer's vocabulary.\")\n",
    "#             else:\n",
    "#                 print(f\"'{self.unk_token}' already exists in the tokenizer's vocabulary.\")\n",
    "\n",
    "#             # **Update ind2char and char2ind after adding tokens**\n",
    "#             self.ind2char = dict(enumerate(self.vocab))\n",
    "#             self.char2ind = {v: k for k, v in self.ind2char.items()}\n",
    "#             self.blank_index = self.char2ind.get(self.blank_token, None)\n",
    "\n",
    "#             # Debug: Print a few tokens after modification\n",
    "#             print(\"Modified Vocabulary (first 20 tokens):\", self.vocab[:20])\n",
    "#         else:\n",
    "#             print(\"Initializing character-based vocabulary without using tokenizer.\")\n",
    "#             # Define a simple character-based vocabulary: a-z and space\n",
    "#             self.vocab = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',\n",
    "#                          'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x',\n",
    "#                          'y', 'z', ' ']\n",
    "#             # Optionally, add special tokens\n",
    "#             self.vocab += [self.blank_token, self.unk_token]\n",
    "\n",
    "#             # **Create index mappings**\n",
    "#             self.ind2char = dict(enumerate(self.vocab))\n",
    "#             self.char2ind = {v: k for k, v in self.ind2char.items()}\n",
    "#             self.blank_index = self.char2ind.get(self.blank_token, None)\n",
    "\n",
    "#             # No processor is used in this mode\n",
    "#             self.processor = None\n",
    "\n",
    "#     def _initialize_language_model(self):\n",
    "#         \"\"\"Initialize language model with explicit blank token handling.\"\"\"\n",
    "#         self.lm = None\n",
    "#         self.decoder = None\n",
    "\n",
    "#         model_path = self.binary_path if self.binary_path else self.arpa_path\n",
    "#         print('model_path: ', model_path)\n",
    "#         if not model_path or not os.path.exists(model_path):\n",
    "#             print(\"No language model path provided or file does not exist.\")\n",
    "#             return\n",
    "\n",
    "#         try:\n",
    "#             self.lm = kenlm.Model(model_path)\n",
    "#             print(f\"Loaded {'binary' if self.binary_path else 'ARPA'} language model.\")\n",
    "\n",
    "#             decoder_config = {\n",
    "#                 \"labels\": self.labels if self.use_bpe else self.vocab,\n",
    "#                 \"kenlm_model_path\": model_path,\n",
    "#                 \"alpha\": self.lm_weight,\n",
    "#                 \"beta\": 0.1,\n",
    "#                 \"unk_score_offset\": -10.0,\n",
    "#             }\n",
    "\n",
    "#             if self.unigrams:\n",
    "#                 print(\"\\n--- Unigrams List ---\")\n",
    "#                 # Save the unigrams to a file\n",
    "#                 with open(\"unigrams_list.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "#                     for unigram in self.unigrams:\n",
    "#                         f.write(f\"{unigram}\\n\")\n",
    "#                 print(f\"Unigrams list saved to 'unigrams_list.txt'. Total unigrams: {len(self.unigrams)}\")\n",
    "#                 print(\"----------------------\\n\")\n",
    "#                 decoder_config[\"unigrams\"] = self.unigrams\n",
    "\n",
    "#             self.decoder = build_ctcdecoder(**decoder_config)\n",
    "#             print(\"Successfully initialized language model and decoder.\")\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"Warning: Failed to initialize decoder: {str(e)}\")\n",
    "#             self.decoder = None\n",
    "\n",
    "#     def encode(self, text: str) -> torch.Tensor:\n",
    "#         \"\"\"\n",
    "#         Encode text either using tokenizer/processor (BPE) or character-based encoding.\n",
    "\n",
    "#         Parameters:\n",
    "#         - text (str): The input text to encode.\n",
    "\n",
    "#         Returns:\n",
    "#         - torch.Tensor: Tensor of token indices.\n",
    "#         \"\"\"\n",
    "#         debug = False\n",
    "\n",
    "#         if self.printed_samples < self.max_printed_samples:\n",
    "#             original_text = text\n",
    "#             text = self.normalize_text(text)\n",
    "#             if debug:\n",
    "#                 print(f\"samples: {str(self.printed_samples)}\")\n",
    "#                 print(f\"\\nEncoding text:\")\n",
    "#                 print(f\" Original: '{original_text}'\")\n",
    "#                 print(f\" Normalized: '{text}'\")\n",
    "#                 for ch in text:\n",
    "#                     print(ch, ord(ch))\n",
    "\n",
    "#         if self.use_bpe:\n",
    "#             try:\n",
    "#                 encoded = self.processor.tokenizer(text, return_tensors=\"pt\", padding=False, truncation=False)\n",
    "#                 token_indices = encoded.input_ids[0].tolist()\n",
    "#                 if self.printed_samples < self.max_printed_samples:\n",
    "#                     # Convert indices to tokens from self.vocab\n",
    "#                     tokens = [self.vocab[idx] if 0 <= idx < len(self.vocab) else \"<invalid>\" for idx in token_indices]\n",
    "#                     # Optionally print tokens for debugging\n",
    "#                     # print(f\" Tokens (lowercased and '|'->' '): {tokens}\")\n",
    "#                     # print(f\" Token indices: {token_indices}\")\n",
    "#                     self.printed_samples += 1\n",
    "#                 return torch.tensor(token_indices).unsqueeze(0)\n",
    "#             except KeyError as e:\n",
    "#                 unknown_tokens = set([token for token in text.split() if token not in self.char2ind])\n",
    "#                 raise Exception(f\"Unknown tokens: '{' '.join(unknown_tokens)}'\")\n",
    "#             except Exception as e:\n",
    "#                 raise Exception(f\"Encoding error: {str(e)}\")\n",
    "#         else:\n",
    "#             # Character-based encoding\n",
    "#             try:\n",
    "#                 normalized_text = self.normalize_text(text)\n",
    "#                 token_indices = [self.char2ind.get(char, self.char2ind.get(self.unk_token)) for char in normalized_text]\n",
    "#                 return torch.tensor(token_indices).unsqueeze(0)\n",
    "#             except Exception as e:\n",
    "#                 raise Exception(f\"Encoding error: {str(e)}\")\n",
    "\n",
    "#     def decode(self, indices: List[int]) -> str:\n",
    "#         \"\"\"\n",
    "#         Decode indices to text using beam search decoder if available.\n",
    "\n",
    "#         Parameters:\n",
    "#         - indices (List[int]): List of token indices.\n",
    "\n",
    "#         Returns:\n",
    "#         - str: Decoded text.\n",
    "#         \"\"\"\n",
    "#         if self.decoder:\n",
    "#             decoded_text = self.decoder.decode(indices)\n",
    "#             # Convert to lower case\n",
    "#             decoded_text = decoded_text.lower()\n",
    "#             return decoded_text\n",
    "#         else:\n",
    "#             decoded_text = self.decode_simple(indices)\n",
    "#             # Convert to lower case\n",
    "#             decoded_text = decoded_text.lower()\n",
    "#             return decoded_text  # Ensure the decoded text is returned\n",
    "\n",
    "#     def decode_simple(self, indices: List[int]) -> str:\n",
    "#         \"\"\"\n",
    "#         Simple CTC decoding without language model.\n",
    "#         Collapses consecutive duplicate tokens and removes blanks.\n",
    "\n",
    "#         Parameters:\n",
    "#         - indices (List[int]): List of token indices.\n",
    "\n",
    "#         Returns:\n",
    "#         - str: Decoded text.\n",
    "#         \"\"\"\n",
    "#         decoded_chars = []\n",
    "#         previous_idx = None\n",
    "\n",
    "#         for idx in indices:\n",
    "#             if idx == self.blank_index:\n",
    "#                 previous_idx = idx\n",
    "#                 continue  # Skip blank tokens\n",
    "#             if idx == previous_idx:\n",
    "#                 continue  # Skip duplicate tokens\n",
    "#             if 0 <= idx < len(self.ind2char):\n",
    "#                 char = self.ind2char[idx]\n",
    "#                 decoded_chars.append(char)\n",
    "#             previous_idx = idx\n",
    "\n",
    "#         # Join characters without spaces and convert to lowercase\n",
    "#         text = \"\".join(decoded_chars).strip().lower()\n",
    "\n",
    "#         if self.use_bpe and self.processor:\n",
    "#             # Clean up tokenization using the tokenizer's method\n",
    "#             return self.processor.tokenizer.clean_up_tokenization(text)\n",
    "#         else:\n",
    "#             # For character-based decoding, additional cleanup can be added if necessary\n",
    "#             return text\n",
    "\n",
    "#     def decode_logits(self, logits: Union[torch.Tensor, List[List[float]], np.ndarray]) -> str:\n",
    "#         \"\"\"\n",
    "#         Decode logits using the decoder if available, otherwise use greedy decoding.\n",
    "\n",
    "#         Parameters:\n",
    "#         - logits (Union[torch.Tensor, List[List[float]], np.ndarray]): Logits output from the model.\n",
    "\n",
    "#         Returns:\n",
    "#         - str: Decoded text.\n",
    "#         \"\"\"\n",
    "#         if isinstance(logits, torch.Tensor):\n",
    "#             logits = logits.cpu().numpy()\n",
    "#         elif isinstance(logits, list):\n",
    "#             logits = np.array(logits)\n",
    "#         elif not isinstance(logits, np.ndarray):\n",
    "#             raise TypeError(\"logits must be a torch.Tensor, list of lists, or numpy.ndarray\")\n",
    "\n",
    "#         if logits.ndim == 3:\n",
    "#             logits = logits[0]\n",
    "\n",
    "#         if logits.ndim != 2:\n",
    "#             raise ValueError(f\"Logits should be 2D (time_steps, vocab_size), got {logits.ndim}D\")\n",
    "\n",
    "#         if self.decoder:\n",
    "#             decoded_text = self.decoder.decode(logits)\n",
    "#             return decoded_text\n",
    "#         else:\n",
    "#             predicted_indices = np.argmax(logits, axis=-1).tolist()\n",
    "#             return self.decode_simple(predicted_indices)\n",
    "\n",
    "#     def decode_indices(self, indices: Union[torch.Tensor, List[int], np.ndarray]) -> str:\n",
    "#         \"\"\"\n",
    "#         Decode token indices to text using simple decoding (no LM).\n",
    "\n",
    "#         Parameters:\n",
    "#         - indices (Union[torch.Tensor, List[int], np.ndarray]): Token indices.\n",
    "\n",
    "#         Returns:\n",
    "#         - str: Decoded text.\n",
    "#         \"\"\"\n",
    "#         if isinstance(indices, torch.Tensor):\n",
    "#             indices = indices.squeeze().tolist()\n",
    "#         elif isinstance(indices, np.ndarray):\n",
    "#             indices = indices.tolist()\n",
    "#         elif not isinstance(indices, list):\n",
    "#             raise TypeError(\"decode_indices expects a list, torch.Tensor, or numpy.ndarray.\")\n",
    "\n",
    "#         return self.decode_simple(indices)\n",
    "\n",
    "#     def ctc_decode(self, logits: Union[torch.Tensor, List[int], np.ndarray]) -> str:\n",
    "#         \"\"\"\n",
    "#         Perform CTC decoding on logits.\n",
    "\n",
    "#         Parameters:\n",
    "#         - logits (Union[torch.Tensor, List[int], np.ndarray]): Logits or token indices.\n",
    "\n",
    "#         Returns:\n",
    "#         - str: Decoded text.\n",
    "#         \"\"\"\n",
    "#         if isinstance(logits, np.ndarray):\n",
    "#             logits = torch.from_numpy(logits)\n",
    "#         elif isinstance(logits, list):\n",
    "#             logits = torch.tensor(logits)\n",
    "\n",
    "#         if logits.dim() == 3:\n",
    "#             decoded_text = self.decode_logits(logits)\n",
    "#             return decoded_text\n",
    "#         elif logits.dim() == 2:\n",
    "#             decoded_text = self.decode_logits(logits)\n",
    "#             return decoded_text\n",
    "#         elif logits.dim() == 1:\n",
    "#             decoded_text = self.decode_indices(logits)\n",
    "#             return decoded_text\n",
    "#         else:\n",
    "#             raise ValueError(f\"Unsupported logits shape: {logits.shape}. Expected 1D, 2D, or 3D.\")\n",
    "\n",
    "#     def ctc_beam_search(self, probs, beam_size, use_lm = False, debug: bool = False) -> List[Tuple[str, float]]:\n",
    "#         \"\"\"\n",
    "#         Beam search with optional Language Model support.\n",
    "\n",
    "#         Parameters:\n",
    "#         - probs: Probability distributions over tokens.\n",
    "#         - debug (bool): Whether to print debug information.\n",
    "\n",
    "#         Returns:\n",
    "#         - List[Tuple[str, float]]: List of decoded text with scores.\n",
    "#         \"\"\"\n",
    "#         beam_size = self.beam_size\n",
    "#         debug = False\n",
    "\n",
    "#         if self.use_lm and self.decoder is not None:\n",
    "#             try:\n",
    "#                 if isinstance(probs, torch.Tensor):\n",
    "#                     probs = probs.cpu().numpy()\n",
    "#                 elif isinstance(probs, list):\n",
    "#                     probs = np.array(probs)\n",
    "#                 elif isinstance(probs, np.ndarray):\n",
    "#                     pass\n",
    "#                 else:\n",
    "#                     raise TypeError(\"probs must be a torch.Tensor, list, or numpy.ndarray\")\n",
    "\n",
    "#                 beams = self.decoder.decode_beams(\n",
    "#                     probs,\n",
    "#                     beam_prune_logp=-10.0,\n",
    "#                     token_min_logp=-5.0,\n",
    "#                     hotwords=[],\n",
    "#                     hotword_weight=10.0,\n",
    "#                 )\n",
    "\n",
    "#                 formatted_beams = []\n",
    "#                 for beam in beams[:beam_size]:\n",
    "#                     text = beam[0]\n",
    "#                     acoustic_score = beam[3]\n",
    "#                     lm_score = beam[4]\n",
    "\n",
    "#                     if self.use_bpe and self.processor:\n",
    "#                         text = self.processor.tokenizer.clean_up_tokenization(text)\n",
    "#                     text = text.lower().strip()\n",
    "\n",
    "#                     combined_score = (1 - self.lm_weight) * acoustic_score + self.lm_weight * lm_score\n",
    "#                     text_len = max(1, len(text.split())) if self.use_bpe else max(1, len(text))\n",
    "#                     normalized_score = combined_score / text_len\n",
    "\n",
    "#                     formatted_beams.append((text, normalized_score))\n",
    "\n",
    "#                 if debug:\n",
    "#                     print(\"\\nFormatted beam results with Language Model:\")\n",
    "#                     for text, score in formatted_beams[:3]:\n",
    "#                         print(f\"Text: '{text}', Score: {score:.4f}\")\n",
    "\n",
    "#                 if formatted_beams:\n",
    "#                     return sorted(formatted_beams, key=lambda x: -x[1])\n",
    "#                 else:\n",
    "#                     print(\"No valid beams found, falling back to standard beam search\")\n",
    "#                     return self._standard_beam_search(probs, debug)\n",
    "\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Beam search with LM failed: {str(e)}, falling back to standard beam search\")\n",
    "#                 return self._standard_beam_search(probs, debug)\n",
    "#         else:\n",
    "#             return self._standard_beam_search(probs, debug)\n",
    "\n",
    "#     def _standard_beam_search(self, probs, debug: bool = False) -> List[Tuple[str, float]]:\n",
    "#         \"\"\"\n",
    "#         Original beam search implementation without Language Model.\n",
    "\n",
    "#         Parameters:\n",
    "#         - probs: Probability distributions over tokens.\n",
    "#         - debug (bool): Whether to print debug information.\n",
    "\n",
    "#         Returns:\n",
    "#         - List[Tuple[str, float]]: List of decoded text with scores.\n",
    "#         \"\"\"\n",
    "#         beam_size = self.beam_size\n",
    "\n",
    "#         if isinstance(probs, np.ndarray):\n",
    "#             probs = torch.from_numpy(probs)\n",
    "\n",
    "#         if probs.device != torch.device('cpu'):\n",
    "#             probs = probs.cpu()\n",
    "\n",
    "#         dp = {(\"\", self.blank_token): 0.0}\n",
    "#         log_probs = torch.log(probs + 1e-8)\n",
    "\n",
    "#         if debug:\n",
    "#             print(\"\\nStarting beam search with beam size:\", beam_size)\n",
    "\n",
    "#         for t, prob in enumerate(log_probs):\n",
    "#             new_dp = defaultdict(lambda: float('-inf'))\n",
    "#             top_k = torch.topk(prob, k=min(beam_size, len(prob)))\n",
    "\n",
    "#             if debug and t < self.max_printed_samples:\n",
    "#                 print(f\"\\nTimestep {t}:\")\n",
    "#                 print(\"Top tokens:\", [(self.ind2char[idx.item()], val.item()) \n",
    "#                                     for val, idx in zip(top_k.values, top_k.indices)])\n",
    "\n",
    "#             for val, ind in zip(top_k.values, top_k.indices):\n",
    "#                 curr_char = self.ind2char[ind.item()]\n",
    "#                 next_token_log_prob = val.item()\n",
    "\n",
    "#                 for (prefix, last_char), log_prob in dp.items():\n",
    "#                     if last_char == curr_char and curr_char != \" \":\n",
    "#                         new_prefix = prefix\n",
    "#                     else:\n",
    "#                         if curr_char != self.blank_token:\n",
    "#                             if curr_char == \" \" and prefix.endswith(\" \"):\n",
    "#                                 continue\n",
    "#                             new_prefix = prefix + curr_char\n",
    "#                         else:\n",
    "#                             new_prefix = prefix\n",
    "\n",
    "#                     new_log_prob = log_prob + next_token_log_prob\n",
    "#                     key = (new_prefix, curr_char)\n",
    "#                     new_dp[key] = max(new_dp[key], new_log_prob)\n",
    "\n",
    "#             if len(new_dp) > 0:\n",
    "#                 max_score = max(score for _, score in new_dp.items())\n",
    "#                 new_dp = {key: score - max_score for key, score in new_dp.items()}\n",
    "\n",
    "#             dp = dict(sorted(new_dp.items(), key=lambda x: -x[1])[:beam_size])\n",
    "\n",
    "#             if debug and t < 2:\n",
    "#                 print(\"\\nCurrent beam:\")\n",
    "#                 for (text, last_char), score in list(dp.items())[:3]:\n",
    "#                     print(f\"Text: '{text}', Last: '{last_char}', Score: {score:.4f}\")\n",
    "\n",
    "#         final_beams = []\n",
    "#         for (text, _), score in dp.items():\n",
    "#             if self.use_bpe and self.processor:\n",
    "#                 text = self.processor.tokenizer.clean_up_tokenization(text)\n",
    "#             text = text.lower().strip()\n",
    "#             text_len = max(1, len(text.split())) if self.use_bpe else max(1, len(text))\n",
    "#             normalized_score = score / text_len\n",
    "#             final_beams.append((text, normalized_score))\n",
    "\n",
    "#         final_beams.sort(key=lambda x: -x[1])\n",
    "#         if not final_beams:\n",
    "#             final_beams = [(\"\", float('-inf'))]\n",
    "\n",
    "#         return final_beams[:beam_size]\n",
    "\n",
    "#     def test_language_model(self):\n",
    "#         \"\"\"Debug function to verify LM functionality\"\"\"\n",
    "#         print(\"\\nTesting Language Model...\")\n",
    "\n",
    "#         if self.lm is None:\n",
    "#             print(\"Error: Language model is not loaded!\")\n",
    "#             return\n",
    "\n",
    "#         test_sentences = [\n",
    "#             \"this is a good sentence\",\n",
    "#             \"this is also a good sentence\",\n",
    "#             \"thiss iss nott aa goodd sentencee\",\n",
    "#             \"random word salad box cat\",\n",
    "#             \"the cat sat on the mat\",\n",
    "#             \"\",\n",
    "#             \"a\",\n",
    "#         ]\n",
    "\n",
    "#         print(\"\\nTesting individual sentences:\")\n",
    "#         for sentence in test_sentences:\n",
    "#             score = self.score_with_lm(sentence)\n",
    "#             print(f\"\\nText: '{sentence}'\")\n",
    "#             print(f\"LM Score: {score:.4f}\")\n",
    "\n",
    "#         test_prefixes = [\n",
    "#             \"the quick brown\",\n",
    "#             \"how are\",\n",
    "#             \"thank\",\n",
    "#             \"nice to\",\n",
    "#         ]\n",
    "\n",
    "#         print(\"\\nTesting word completions:\")\n",
    "#         for prefix in test_prefixes:\n",
    "#             print(f\"\\nPrefix: '{prefix}'\")\n",
    "#             completions = [\n",
    "#                 prefix + \" \" + word for word in [\"you\", \"fox\", \"cat\", \"xyz\", \"meet\"]\n",
    "#             ]\n",
    "#             scores = [(completion, self.score_with_lm(completion)) \n",
    "#                     for completion in completions]\n",
    "#             scores.sort(key=lambda x: x[1], reverse=True)\n",
    "#             print(\"Top completions by score:\")\n",
    "#             for completion, score in scores[:3]:\n",
    "#                 print(f\"  '{completion}': {score:.4f}\")\n",
    "\n",
    "#     def score_with_lm(self, text: str) -> float:\n",
    "#         \"\"\"\n",
    "#         Score text using language model, handling edge cases.\n",
    "\n",
    "#         Parameters:\n",
    "#         - text (str): The input text to score.\n",
    "\n",
    "#         Returns:\n",
    "#         - float: LM score.\n",
    "#         \"\"\"\n",
    "#         if self.lm is None:\n",
    "#             return 0.0\n",
    "\n",
    "#         if not text or len(text.strip()) == 0:\n",
    "#             return float('-inf')\n",
    "\n",
    "#         text = text.lower().strip()\n",
    "#         return self.lm.score(text, bos=True, eos=True)\n",
    "\n",
    "#     def _basic_ctc_decode(self, logits: np.ndarray, sequence_length: int) -> List[str]:\n",
    "#         \"\"\"\n",
    "#         Basic CTC decoding without LM.\n",
    "\n",
    "#         Parameters:\n",
    "#         - logits (np.ndarray): Logits from the model.\n",
    "#         - sequence_length (int): Length of the sequence to decode.\n",
    "\n",
    "#         Returns:\n",
    "#         - List[str]: Decoded text.\n",
    "#         \"\"\"\n",
    "#         argmax_indices = np.argmax(logits, axis=-1)\n",
    "\n",
    "#         if len(argmax_indices.shape) == 0:\n",
    "#             argmax_indices = np.array([argmax_indices])\n",
    "\n",
    "#         if len(argmax_indices.shape) == 1:\n",
    "#             argmax_indices = np.expand_dims(argmax_indices, axis=0)\n",
    "\n",
    "#         predictions = []\n",
    "#         for sequence in argmax_indices:\n",
    "#             decoded = []\n",
    "#             last_idx = None\n",
    "\n",
    "#             for idx in sequence[:sequence_length]:\n",
    "#                 if idx != self.blank_index and idx != last_idx:\n",
    "#                     decoded.append(self.ind2char[idx])\n",
    "#                 last_idx = idx\n",
    "\n",
    "#             text = \"\".join(decoded)\n",
    "#             if self.use_bpe and self.processor:\n",
    "#                 text = self.processor.tokenizer.clean_up_tokenization(text)\n",
    "#             predictions.append(text)\n",
    "\n",
    "#         return predictions\n",
    "\n",
    "#     @staticmethod\n",
    "#     def normalize_text(text: str) -> str:\n",
    "#         \"\"\"Normalize input text.\"\"\"\n",
    "#         text = text.lower()\n",
    "#         text = re.sub(r\"[^a-z ]\", \"\", text)\n",
    "#         return text\n",
    "\n",
    "#     def test_decoder(self, sample_text: str = \"test decoder functionality\"):\n",
    "#         \"\"\"Test the decoder setup.\"\"\"\n",
    "#         print(\"\\nTesting decoder configuration...\")\n",
    "\n",
    "#         encoded = self.encode(sample_text)\n",
    "#         decoded = self.decode(encoded[0].tolist())\n",
    "#         print(f\"Original text: {sample_text}\")\n",
    "#         print(f\"Basic decode: {decoded}\")\n",
    "\n",
    "#         sequence_length = 50\n",
    "#         vocab_size = len(self)\n",
    "#         fake_logits = torch.randn(1, sequence_length, vocab_size)\n",
    "#         fake_length = torch.tensor([sequence_length])\n",
    "\n",
    "#         if self.decoder is not None:\n",
    "#             print(\"\\nTesting pyctcdecode integration...\")\n",
    "#             decoded_with_lm = self.ctc_decode(fake_logits)\n",
    "#             print(f\"Decoded with LM: {decoded_with_lm}\")\n",
    "\n",
    "#             print(f\"\\nBeam width: {self.beam_size}\")\n",
    "#             print(f\"LM weight: {self.lm_weight}\")\n",
    "#         else:\n",
    "#             print(\"\\nNo language model loaded - using basic CTC decoding\")\n",
    "#             basic_decoded = self._basic_ctc_decode(fake_logits.numpy(), sequence_length)\n",
    "#             print(f\"Basic CTC decoded: {basic_decoded[0]}\")\n",
    "\n",
    "#     def __len__(self):\n",
    "#         \"\"\"Return the size of the vocabulary.\"\"\"\n",
    "#         return len(self.vocab)\n",
    "\n",
    "#     def ctc_decode(self, logits: Union[torch.Tensor, List[int], np.ndarray]) -> str:\n",
    "#         \"\"\"\n",
    "#         Perform CTC decoding on logits.\n",
    "\n",
    "#         Parameters:\n",
    "#         - logits (Union[torch.Tensor, List[int], np.ndarray]): Logits or token indices.\n",
    "\n",
    "#         Returns:\n",
    "#         - str: Decoded text.\n",
    "#         \"\"\"\n",
    "#         if isinstance(logits, np.ndarray):\n",
    "#             logits = torch.from_numpy(logits)\n",
    "#         elif isinstance(logits, list):\n",
    "#             logits = torch.tensor(logits)\n",
    "\n",
    "#         if logits.dim() == 3:\n",
    "#             decoded_text = self.decode_logits(logits)\n",
    "#             return decoded_text\n",
    "#         elif logits.dim() == 2:\n",
    "#             decoded_text = self.decode_logits(logits)\n",
    "#             return decoded_text\n",
    "#         elif logits.dim() == 1:\n",
    "#             decoded_text = self.decode_indices(logits)\n",
    "#             return decoded_text\n",
    "#         else:\n",
    "#             raise ValueError(f\"Unsupported logits shape: {logits.shape}. Expected 1D, 2D, or 3D.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Processor\n",
    "\n",
    "\n",
    "pretrained_tokenizer = \"facebook/wav2vec2-base-960h\"\n",
    "processor = Wav2Vec2Processor.from_pretrained(pretrained_tokenizer)\n",
    "\n",
    "vocab_dict = processor.tokenizer.get_vocab()\n",
    "sorted_vocab_dict = {k.lower(): v for k, v in sorted(vocab_dict.items(), key=lambda item: item[1])}\n",
    "\n",
    "labels = list(sorted_vocab_dict.keys())\n",
    "vocab = [token.replace('|', ' ') for token in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added '[PAD]' to the tokenizer's vocabulary.\n",
      "Added '[UNK]' to the tokenizer's vocabulary.\n"
     ]
    }
   ],
   "source": [
    "blank_token = \"[PAD]\"\n",
    "unk_token = \"[UNK]\"\n",
    "\n",
    "# pretrained_tokenizer = \"hf-test/xls-r-300m-sv\"\n",
    "# pretrained_tokenizer = \"bert-base-uncased\"\n",
    "pretrained_tokenizer =  \"facebook/wav2vec2-base-960h\"\n",
    "\n",
    "if pretrained_tokenizer == \"facebook/wav2vec2-base-960h\":\n",
    "    tokenizer = Wav2Vec2Processor.from_pretrained(pretrained_tokenizer)\n",
    "    # tokenizer.tokenizer = tokenizer\n",
    "    vocab_dict = tokenizer.tokenizer.get_vocab()\n",
    "    sorted_vocab_dict = {k.lower(): v for k, v in sorted(vocab_dict.items(), key=lambda item: item[1])}\n",
    "    labels = list(sorted_vocab_dict.keys())\n",
    "\n",
    "else:\n",
    "    tokenizer = None\n",
    "    tokenizer = AutoTokenizer.from_pretrained(pretrained_tokenizer)\n",
    "    vocab_dict = tokenizer.vocab\n",
    "    labels = list(vocab_dict.keys())\n",
    "    \n",
    "\n",
    "vocab = [token.replace('|', ' ') for token in labels]\n",
    "\n",
    "# if blank_token not in tokenizer.get_vocab():\n",
    "#     tokenizer.add_tokens([blank_token])\n",
    "#     print(f\"Added '{blank_token}' to the tokenizer's vocabulary.\")\n",
    "\n",
    "# if unk_token not in tokenizer.get_vocab():\n",
    "#     tokenizer.add_tokens([unk_token])\n",
    "#     print(f\"Added '{unk_token}' to the tokenizer's vocabulary.\")\n",
    "\n",
    "if blank_token not in tokenizer.tokenizer.get_vocab():\n",
    "    tokenizer.tokenizer.add_tokens([blank_token])\n",
    "    print(f\"Added '{blank_token}' to the tokenizer's vocabulary.\")\n",
    "\n",
    "if unk_token not in tokenizer.tokenizer.get_vocab():\n",
    "    tokenizer.tokenizer.add_tokens([unk_token])\n",
    "    print(f\"Added '{unk_token}' to the tokenizer's vocabulary.\")\n",
    "\n",
    "\n",
    "ind2char = dict(enumerate(vocab))\n",
    "char2ind = {v: k for k, v in ind2char.items()}\n",
    "blank_index = char2ind.get(blank_token, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenizer.total_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert\n",
    "\n",
    "pretrained_tokenizer = \"bert-base-uncased\"\n",
    "\n",
    "tokenizer_bert = AutoTokenizer.from_pretrained(pretrained_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bye': 9061,\n",
       " 'infections': 15245,\n",
       " 'potsdam': 26554,\n",
       " 'therese': 25598,\n",
       " 'projections': 21796,\n",
       " 'grassroots': 23299,\n",
       " '##tham': 22536,\n",
       " 'stirring': 18385,\n",
       " 'organise': 22933,\n",
       " '[unused973]': 978,\n",
       " '##chman': 19944,\n",
       " 'ambient': 17093,\n",
       " 'edouard': 21627,\n",
       " 'fragments': 10341,\n",
       " 'reviewer': 12027,\n",
       " '##yck': 28377,\n",
       " 'struck': 4930,\n",
       " 'utility': 9710,\n",
       " 'cane': 11942,\n",
       " 'vital': 8995,\n",
       " '##ull': 18083,\n",
       " '##dar': 7662,\n",
       " 'please': 3531,\n",
       " 'woody': 13703,\n",
       " 'warden': 13745,\n",
       " 'available': 2800,\n",
       " 'toulon': 27160,\n",
       " 'belong': 7141,\n",
       " '##eon': 10242,\n",
       " '##cture': 14890,\n",
       " 'yankees': 11081,\n",
       " 'nectar': 24816,\n",
       " 'observer': 9718,\n",
       " '##folk': 29284,\n",
       " 'packaging': 14793,\n",
       " '##〈': 30163,\n",
       " 'codes': 9537,\n",
       " 'thoughts': 4301,\n",
       " 'signage': 29404,\n",
       " '有': 1873,\n",
       " 'multiple': 3674,\n",
       " 'scar': 11228,\n",
       " 'hiv': 9820,\n",
       " 'enthusiasts': 20305,\n",
       " 'plagued': 17808,\n",
       " 'portuguese': 5077,\n",
       " 'decca': 21079,\n",
       " 'ligand': 27854,\n",
       " 'syndrome': 8715,\n",
       " 'rode': 8469,\n",
       " '##ব': 29904,\n",
       " 'nap': 18996,\n",
       " 'avenge': 24896,\n",
       " 'optic': 22816,\n",
       " 'seduction': 26962,\n",
       " 'readers': 8141,\n",
       " 'anonymous': 10812,\n",
       " 'stanza': 29509,\n",
       " 'fred': 5965,\n",
       " 'industrialist': 21691,\n",
       " 'tim': 5199,\n",
       " 'books': 2808,\n",
       " 'reza': 26323,\n",
       " 'influences': 8092,\n",
       " 'squadron': 3704,\n",
       " 'copa': 10613,\n",
       " 'cherokee': 13796,\n",
       " '##gi': 5856,\n",
       " 'avalon': 18973,\n",
       " '海': 1902,\n",
       " 'harder': 6211,\n",
       " '##room': 9954,\n",
       " '[unused887]': 892,\n",
       " 'tung': 27079,\n",
       " '##bara': 20709,\n",
       " 'celaena': 29252,\n",
       " 'blizzard': 21689,\n",
       " 'rhyme': 20622,\n",
       " 'herself': 2841,\n",
       " '##edge': 24225,\n",
       " 'linebacker': 15674,\n",
       " 'ninja': 14104,\n",
       " '##hen': 10222,\n",
       " '##thor': 27844,\n",
       " '##urn': 14287,\n",
       " 'escapes': 12976,\n",
       " 'lime': 14123,\n",
       " 'impacts': 14670,\n",
       " '##zell': 24085,\n",
       " 'indicated': 5393,\n",
       " 'petitioned': 22527,\n",
       " 'villains': 16219,\n",
       " 'effectively': 6464,\n",
       " 'pl': 20228,\n",
       " 'widening': 17973,\n",
       " 'gilded': 23880,\n",
       " 'temperature': 4860,\n",
       " 'め': 1680,\n",
       " '##isi': 17417,\n",
       " 'jug': 26536,\n",
       " '##well': 4381,\n",
       " 'explained': 4541,\n",
       " 'amber': 8994,\n",
       " '##24': 18827,\n",
       " 'clawed': 22544,\n",
       " '##mad': 25666,\n",
       " '9th': 6280,\n",
       " 'cups': 10268,\n",
       " 'estimation': 24155,\n",
       " 'rising': 4803,\n",
       " '##gall': 22263,\n",
       " 'dover': 13985,\n",
       " '##mg': 24798,\n",
       " 'colors': 6087,\n",
       " '##aid': 14326,\n",
       " 'geoff': 14915,\n",
       " '[unused357]': 362,\n",
       " 'nemesis': 21363,\n",
       " 'william': 2520,\n",
       " '₩': 1573,\n",
       " 'gently': 5251,\n",
       " '##onzo': 29452,\n",
       " '106': 10114,\n",
       " 'bays': 13933,\n",
       " 'hears': 14994,\n",
       " 'internet': 4274,\n",
       " 'nt': 23961,\n",
       " 'presenters': 25588,\n",
       " '2002': 2526,\n",
       " 'rainbow': 10098,\n",
       " 'ruler': 7786,\n",
       " 'ড': 1360,\n",
       " '##nta': 12380,\n",
       " 'ο': 1169,\n",
       " '03': 6021,\n",
       " 'falling': 4634,\n",
       " 'placement': 11073,\n",
       " 'reduces': 13416,\n",
       " 'ryu': 19367,\n",
       " 'economics': 5543,\n",
       " 'magazine': 2932,\n",
       " 'targeting': 14126,\n",
       " '##uter': 19901,\n",
       " '##illo': 10486,\n",
       " 'revoked': 22837,\n",
       " '##月': 30398,\n",
       " 'contests': 15795,\n",
       " 'uprising': 10138,\n",
       " 'designate': 24414,\n",
       " 'bright': 4408,\n",
       " '##ท': 29948,\n",
       " 'renal': 25125,\n",
       " 'ʿ': 1148,\n",
       " '##ury': 13098,\n",
       " 'lab': 6845,\n",
       " 'cabinets': 20053,\n",
       " 'choke': 16769,\n",
       " 'georgian': 9166,\n",
       " '##owe': 29385,\n",
       " '##cous': 27199,\n",
       " 'reggie': 17963,\n",
       " 'safeguard': 28805,\n",
       " '##phate': 24556,\n",
       " 'counties': 5721,\n",
       " 'lady': 3203,\n",
       " 'lucrative': 21115,\n",
       " 'realising': 27504,\n",
       " 'madly': 29179,\n",
       " '##uber': 21436,\n",
       " 'brilliant': 8235,\n",
       " 'corpses': 18113,\n",
       " '##nee': 24045,\n",
       " 'cesare': 26708,\n",
       " 'napoleon': 8891,\n",
       " 'ski': 8301,\n",
       " '1640': 21533,\n",
       " 'accents': 24947,\n",
       " 'vanity': 18736,\n",
       " '##bate': 20179,\n",
       " 'bourgeois': 22846,\n",
       " 'retorted': 24056,\n",
       " 'chartered': 12443,\n",
       " '##ding': 4667,\n",
       " 'mold': 18282,\n",
       " 'shrieked': 22383,\n",
       " 'oscar': 7436,\n",
       " '##arm': 27292,\n",
       " 'requests': 11186,\n",
       " 'carlson': 22226,\n",
       " 'imaginative': 28575,\n",
       " '、': 1635,\n",
       " 'bleeding': 9524,\n",
       " 'marjorie': 21562,\n",
       " 'refined': 15514,\n",
       " '##mbled': 23931,\n",
       " '##uy': 26230,\n",
       " '##hdi': 22960,\n",
       " '##ook': 14659,\n",
       " 'nightingale': 21771,\n",
       " 'masonic': 21152,\n",
       " 'soothe': 28384,\n",
       " '##eger': 26320,\n",
       " 'digital': 3617,\n",
       " 'mathias': 20494,\n",
       " 'politely': 16954,\n",
       " 'myspace': 24927,\n",
       " 'antennas': 26315,\n",
       " 'games': 2399,\n",
       " 'secondary': 3905,\n",
       " '##gle': 9354,\n",
       " 'disciplined': 28675,\n",
       " 'whitney': 9809,\n",
       " 'solemn': 19487,\n",
       " '##gold': 21270,\n",
       " 'browne': 15005,\n",
       " 'commentaries': 21241,\n",
       " 'figuring': 23218,\n",
       " 'seldom': 15839,\n",
       " 'rca': 12639,\n",
       " 'tides': 22487,\n",
       " 'bitten': 19026,\n",
       " 'gnu': 27004,\n",
       " 'უ': 1454,\n",
       " '247': 23380,\n",
       " 'muster': 20327,\n",
       " 'backpack': 13383,\n",
       " 'comb': 22863,\n",
       " 'yang': 8675,\n",
       " '##ming': 6562,\n",
       " 'fashionable': 19964,\n",
       " 'les': 4649,\n",
       " 'supervising': 21238,\n",
       " 'cupping': 25076,\n",
       " '##codes': 23237,\n",
       " 'stars': 3340,\n",
       " 'newer': 10947,\n",
       " 'holster': 29447,\n",
       " 'illicit': 25049,\n",
       " 'longer': 2936,\n",
       " 'spears': 13957,\n",
       " 'edna': 21051,\n",
       " '##pressing': 24128,\n",
       " 'georgie': 20280,\n",
       " 'ม': 1415,\n",
       " 'ﬂ': 1985,\n",
       " 'punk': 7196,\n",
       " 'below': 2917,\n",
       " '[unused20]': 21,\n",
       " '##croft': 14716,\n",
       " 'ton': 10228,\n",
       " 'پ': 1302,\n",
       " 'mason': 6701,\n",
       " 'sutra': 26567,\n",
       " 'por': 18499,\n",
       " 'sire': 15785,\n",
       " 'hence': 6516,\n",
       " 'hurriedly': 23878,\n",
       " 'fourth': 2959,\n",
       " 'arose': 10375,\n",
       " 'cain': 11557,\n",
       " 'fascist': 14870,\n",
       " '1917': 4585,\n",
       " 'fears': 10069,\n",
       " 'snort': 26759,\n",
       " 'vr': 27830,\n",
       " 'lighter': 9442,\n",
       " 'willis': 12688,\n",
       " 'cesar': 14923,\n",
       " 'reins': 19222,\n",
       " 'landmarks': 16209,\n",
       " 'faintly': 18587,\n",
       " 'translator': 11403,\n",
       " '●': 1619,\n",
       " 'damian': 19507,\n",
       " 'chaplain': 14011,\n",
       " 'madison': 7063,\n",
       " '1963': 3699,\n",
       " 'kamen': 22099,\n",
       " 'between': 2090,\n",
       " 'settle': 7392,\n",
       " 'burroughs': 25991,\n",
       " 'coefficient': 19064,\n",
       " '##nbc': 28957,\n",
       " 'milo': 20359,\n",
       " 'snap': 10245,\n",
       " '##や': 30208,\n",
       " 'separating': 14443,\n",
       " 'underwood': 22751,\n",
       " '##ri': 3089,\n",
       " 'turned': 2357,\n",
       " '##dded': 19520,\n",
       " '##12': 12521,\n",
       " '##oise': 23565,\n",
       " '##iano': 15668,\n",
       " 'greenwich': 13861,\n",
       " 'ltd': 5183,\n",
       " '##vyn': 23487,\n",
       " 'chase': 5252,\n",
       " 'cheyenne': 17778,\n",
       " 'committee': 2837,\n",
       " '[unused317]': 322,\n",
       " 'resource': 7692,\n",
       " 'crimea': 21516,\n",
       " 'sweeney': 21178,\n",
       " 'ineffective': 20694,\n",
       " 'notified': 19488,\n",
       " '##rled': 27501,\n",
       " '##body': 23684,\n",
       " 'helmet': 10412,\n",
       " '‐': 1513,\n",
       " 'flotilla': 17150,\n",
       " 'hesitated': 9369,\n",
       " 'magnificent': 12047,\n",
       " '##g': 2290,\n",
       " '[unused723]': 728,\n",
       " '[unused108]': 113,\n",
       " 'rent': 9278,\n",
       " 'customized': 28749,\n",
       " '01': 5890,\n",
       " '##oria': 11069,\n",
       " 'foster': 6469,\n",
       " 'weren': 4694,\n",
       " '##xin': 20303,\n",
       " 'itv': 11858,\n",
       " 'ewing': 24023,\n",
       " '##ght': 13900,\n",
       " 'gubernatorial': 19100,\n",
       " 'aunt': 5916,\n",
       " 'positions': 4460,\n",
       " 'sic': 14387,\n",
       " '92': 6227,\n",
       " 'undertook': 12543,\n",
       " '1999': 2639,\n",
       " 'jump': 5376,\n",
       " '1986': 3069,\n",
       " 'land': 2455,\n",
       " 'power': 2373,\n",
       " 'winged': 14462,\n",
       " '##rke': 25074,\n",
       " 'skies': 15717,\n",
       " 'embroidery': 29507,\n",
       " 'embarrassed': 10339,\n",
       " 'monde': 23117,\n",
       " '##not': 17048,\n",
       " 'ɒ': 1111,\n",
       " 'uncertain': 9662,\n",
       " 'suggesting': 9104,\n",
       " 'goin': 17249,\n",
       " 'playboy': 18286,\n",
       " 'rhetoric': 17871,\n",
       " 'intensity': 8015,\n",
       " 'prisoners': 5895,\n",
       " 'parking': 5581,\n",
       " '##los': 10483,\n",
       " 'coat': 5435,\n",
       " 'southernmost': 21787,\n",
       " 'uniformly': 27423,\n",
       " 'dim': 11737,\n",
       " 'kendra': 13812,\n",
       " 'heroic': 14779,\n",
       " 'sous': 27411,\n",
       " 'transverse': 18323,\n",
       " 'prior': 3188,\n",
       " 'engravings': 28611,\n",
       " '##aine': 18175,\n",
       " '##vat': 22879,\n",
       " '##worth': 5172,\n",
       " 'vocalist': 8032,\n",
       " 'stuff': 4933,\n",
       " 'ه': 1297,\n",
       " 'markets': 6089,\n",
       " 'assured': 8916,\n",
       " 'evacuate': 22811,\n",
       " 'contributed': 5201,\n",
       " '/': 1013,\n",
       " 'ache': 12336,\n",
       " 'crisp': 15594,\n",
       " 'schultz': 22378,\n",
       " '##dication': 25027,\n",
       " '##nst': 23808,\n",
       " 'thought': 2245,\n",
       " '⇌': 1590,\n",
       " '##anga': 18222,\n",
       " 'darius': 14861,\n",
       " 'siege': 6859,\n",
       " '##sion': 10992,\n",
       " 'electromagnetic': 17225,\n",
       " '–': 1516,\n",
       " 'galloway': 22372,\n",
       " 'campus': 3721,\n",
       " 'coats': 15695,\n",
       " 'breaths': 12938,\n",
       " 'saxony': 13019,\n",
       " 'intact': 10109,\n",
       " 'patents': 13979,\n",
       " 'trigger': 9495,\n",
       " 'snatch': 23365,\n",
       " 'supplied': 8127,\n",
       " '##int': 18447,\n",
       " 'pushed': 3724,\n",
       " 'seal': 7744,\n",
       " 'combo': 25025,\n",
       " 'khz': 17737,\n",
       " 'occurrence': 14404,\n",
       " 'mike': 3505,\n",
       " 'religion': 4676,\n",
       " 'investigative': 15025,\n",
       " 'translation': 5449,\n",
       " 'devotion': 13347,\n",
       " '355': 26271,\n",
       " 'eagle': 6755,\n",
       " 'motorcycles': 18580,\n",
       " 'miscellaneous': 25408,\n",
       " 'ukraine': 5924,\n",
       " '##factory': 21450,\n",
       " 'sant': 15548,\n",
       " 'arguing': 9177,\n",
       " '本': 1876,\n",
       " '##stituting': 21532,\n",
       " '##ս': 29781,\n",
       " 'rochdale': 26109,\n",
       " 'spoke': 3764,\n",
       " 'alison': 12684,\n",
       " '##hoff': 17896,\n",
       " '##note': 22074,\n",
       " 'austria': 5118,\n",
       " 'sounds': 4165,\n",
       " 'dorset': 15367,\n",
       " 'realization': 12393,\n",
       " 'minimum': 6263,\n",
       " '##љ': 29762,\n",
       " 'forts': 15421,\n",
       " 'going': 2183,\n",
       " 'slip': 7540,\n",
       " 'telangana': 23764,\n",
       " 'strawberry': 16876,\n",
       " 'conservatives': 11992,\n",
       " 'from': 2013,\n",
       " 'surrounded': 5129,\n",
       " 'yong': 18999,\n",
       " 'multiplayer': 17762,\n",
       " 'existed': 5839,\n",
       " 'ramon': 12716,\n",
       " 'pounder': 20091,\n",
       " '##umb': 25438,\n",
       " 'frowns': 29303,\n",
       " 'rafe': 15819,\n",
       " 'maid': 10850,\n",
       " '14': 2403,\n",
       " '[unused106]': 111,\n",
       " 'hallway': 6797,\n",
       " 'sw': 25430,\n",
       " '1960': 3624,\n",
       " 'schumacher': 22253,\n",
       " '##eering': 20550,\n",
       " '21': 2538,\n",
       " '##powering': 23948,\n",
       " '##9': 2683,\n",
       " 'satisfy': 13225,\n",
       " 'correction': 18140,\n",
       " '##oit': 28100,\n",
       " '@': 1030,\n",
       " '##omic': 22026,\n",
       " 'asia': 4021,\n",
       " '##ands': 29560,\n",
       " '##las': 8523,\n",
       " 'laying': 10201,\n",
       " 'everywhere': 7249,\n",
       " 'extensions': 14305,\n",
       " 'evicted': 25777,\n",
       " 'clever': 12266,\n",
       " 'hebrew': 6836,\n",
       " 'etudes': 25041,\n",
       " 'mobile': 4684,\n",
       " 'robe': 11111,\n",
       " 'bermuda': 13525,\n",
       " 'lynn': 9399,\n",
       " 'decreasing': 16922,\n",
       " 'excuses': 21917,\n",
       " 'diameter': 6705,\n",
       " 'signaling': 14828,\n",
       " 'travellers': 19284,\n",
       " 'jaya': 24120,\n",
       " 'perkins': 13601,\n",
       " 'sandstone': 11694,\n",
       " '##iary': 17302,\n",
       " '1925': 4849,\n",
       " '[unused949]': 954,\n",
       " 'age': 2287,\n",
       " 'kam': 27829,\n",
       " 'tasmania': 12343,\n",
       " 'chronology': 17873,\n",
       " '##ᵍ': 30036,\n",
       " 'arrived': 3369,\n",
       " 'templar': 27850,\n",
       " 'transparent': 13338,\n",
       " '##pine': 19265,\n",
       " 'having': 2383,\n",
       " 'global': 3795,\n",
       " 'elk': 18995,\n",
       " 'alf': 24493,\n",
       " 'punishment': 7750,\n",
       " '##vite': 25217,\n",
       " 'hasn': 8440,\n",
       " '000': 2199,\n",
       " '##jing': 29518,\n",
       " '##த': 29921,\n",
       " 'shadow': 5192,\n",
       " 'certificates': 17987,\n",
       " 'arnold': 7779,\n",
       " '##gn': 16206,\n",
       " '##ο': 29730,\n",
       " 'but': 2021,\n",
       " 'hatch': 11300,\n",
       " '108': 10715,\n",
       " 'interpreter': 19555,\n",
       " 'tenant': 16713,\n",
       " 'wat': 28194,\n",
       " 'julio': 15090,\n",
       " '##pit': 23270,\n",
       " 'letter': 3661,\n",
       " 'siren': 19558,\n",
       " 'abduction': 23415,\n",
       " 'versa': 18601,\n",
       " '##chment': 22729,\n",
       " '##uni': 19496,\n",
       " '##ße': 17499,\n",
       " 'bulgaria': 8063,\n",
       " 'nate': 8253,\n",
       " 'selfish': 14337,\n",
       " 'gdp': 14230,\n",
       " 'nhl': 7097,\n",
       " '[unused67]': 68,\n",
       " 'cyclone': 11609,\n",
       " 'almighty': 26668,\n",
       " 'albion': 13392,\n",
       " 'prolific': 12807,\n",
       " 'visa': 9425,\n",
       " 'panther': 15133,\n",
       " 'look': 2298,\n",
       " 'adherents': 25712,\n",
       " 'degraded': 26131,\n",
       " 'fleetwood': 23866,\n",
       " 'topographic': 24254,\n",
       " 'deserve': 10107,\n",
       " 'mutation': 16221,\n",
       " 'slid': 4934,\n",
       " '区': 1782,\n",
       " 'vegetables': 11546,\n",
       " 'flawless': 27503,\n",
       " 'connectivity': 20831,\n",
       " '##ieg': 28872,\n",
       " '##分': 30301,\n",
       " 'classrooms': 12463,\n",
       " 'bicycle': 10165,\n",
       " '##kind': 18824,\n",
       " 'assemblies': 17720,\n",
       " '##stad': 16917,\n",
       " '1643': 25534,\n",
       " '##nel': 11877,\n",
       " 'sank': 7569,\n",
       " 'picket': 28972,\n",
       " 'purchasing': 13131,\n",
       " 'batteries': 10274,\n",
       " 'carmel': 19443,\n",
       " 'included': 2443,\n",
       " 'abraham': 8181,\n",
       " 'aristotle': 17484,\n",
       " 'sliced': 15920,\n",
       " 'rancho': 18123,\n",
       " 'crisis': 5325,\n",
       " 'references': 7604,\n",
       " '##amo': 22591,\n",
       " 'burning': 5255,\n",
       " 'coup': 8648,\n",
       " 'alsace': 24922,\n",
       " 'arm': 2849,\n",
       " 'deer': 8448,\n",
       " 'miss': 3335,\n",
       " '##mans': 15154,\n",
       " 'ignacio': 22988,\n",
       " 'murmur': 20227,\n",
       " 'ghanaian': 27202,\n",
       " 'institut': 17126,\n",
       " 'reluctantly': 11206,\n",
       " 'solution': 5576,\n",
       " 'clerks': 26706,\n",
       " 'better': 2488,\n",
       " 'francoise': 28979,\n",
       " '##uez': 29488,\n",
       " 'missed': 4771,\n",
       " 'cara': 14418,\n",
       " '263': 25246,\n",
       " 'ruin': 10083,\n",
       " 'fierce': 9205,\n",
       " 'je': 15333,\n",
       " 'gallons': 18501,\n",
       " 'capacities': 21157,\n",
       " '##mania': 27010,\n",
       " '##borg': 11755,\n",
       " 'survivors': 8643,\n",
       " 'ry': 29431,\n",
       " '##→': 30113,\n",
       " 'intent': 7848,\n",
       " 'difficulty': 7669,\n",
       " 'phylogenetic': 23192,\n",
       " 'straw': 13137,\n",
       " 'arrows': 12563,\n",
       " 'crashed': 8007,\n",
       " 'documented': 8832,\n",
       " 'marquis': 13410,\n",
       " 'staged': 9813,\n",
       " '##her': 5886,\n",
       " '##llan': 19036,\n",
       " '##も': 30207,\n",
       " 'monsoon': 19183,\n",
       " 'expect': 5987,\n",
       " '##skaya': 23070,\n",
       " 'similarity': 14402,\n",
       " 'ministerial': 18645,\n",
       " '##enstein': 21819,\n",
       " 'з': 1187,\n",
       " 'suited': 10897,\n",
       " 'wah': 22894,\n",
       " '##স': 29912,\n",
       " '##cens': 19023,\n",
       " '##nac': 18357,\n",
       " 'glint': 25263,\n",
       " 'taxonomy': 25274,\n",
       " '高': 1981,\n",
       " 'burundi': 28836,\n",
       " 'onto': 3031,\n",
       " '118': 12963,\n",
       " '[unused469]': 474,\n",
       " 'effects': 3896,\n",
       " '##age': 4270,\n",
       " 'pastoral': 13645,\n",
       " 'folder': 19622,\n",
       " 'bulldog': 28628,\n",
       " 'aim': 6614,\n",
       " 'constructions': 21913,\n",
       " '##ர': 29927,\n",
       " 'paranoid': 19810,\n",
       " 'disturb': 22995,\n",
       " 'felony': 24648,\n",
       " 'militant': 16830,\n",
       " 'ponce': 21085,\n",
       " '530': 23523,\n",
       " 'sr': 5034,\n",
       " 'announcement': 8874,\n",
       " 'dudley': 12648,\n",
       " 'trusts': 20278,\n",
       " '##bil': 14454,\n",
       " 'crimson': 11466,\n",
       " 'fordham': 27302,\n",
       " 'geese': 28519,\n",
       " 'vishnu': 17647,\n",
       " 'worth': 4276,\n",
       " 'bust': 13950,\n",
       " 'evolved': 7964,\n",
       " 'erosion': 14173,\n",
       " 'skeletal': 20415,\n",
       " '##ake': 13808,\n",
       " 'loving': 8295,\n",
       " 'deficiency': 18888,\n",
       " 'quincy': 16141,\n",
       " 'ep': 4958,\n",
       " 'breakdown': 12554,\n",
       " '##raine': 26456,\n",
       " '155': 14168,\n",
       " 'sounding': 9391,\n",
       " 'bargain': 17113,\n",
       " 'reassuring': 21093,\n",
       " 'effortlessly': 29483,\n",
       " 'degree': 3014,\n",
       " 'twain': 24421,\n",
       " 'vida': 19830,\n",
       " 'ruins': 8435,\n",
       " '[unused944]': 949,\n",
       " 'sundance': 20140,\n",
       " '[unused91]': 92,\n",
       " 'struggled': 6915,\n",
       " 'sen': 12411,\n",
       " 'sidney': 11430,\n",
       " 'reviewed': 8182,\n",
       " ';': 1025,\n",
       " 'ivan': 7332,\n",
       " '##ero': 10624,\n",
       " '##elles': 22869,\n",
       " '##chi': 5428,\n",
       " '##lance': 23078,\n",
       " '##cian': 14483,\n",
       " 'assisted': 7197,\n",
       " 'seater': 23392,\n",
       " '##mani': 20799,\n",
       " 'successes': 14152,\n",
       " '##rdon': 28176,\n",
       " 'plata': 19534,\n",
       " '##ɪ': 29685,\n",
       " 'thursday': 9432,\n",
       " 'pri': 26927,\n",
       " 'gaston': 18572,\n",
       " 'majesty': 9995,\n",
       " 'azure': 24296,\n",
       " 'ी': 1342,\n",
       " 'oriented': 8048,\n",
       " '光': 1770,\n",
       " '##we': 8545,\n",
       " '##llins': 26655,\n",
       " 'sable': 23492,\n",
       " 'claimed': 3555,\n",
       " 'sculpted': 19921,\n",
       " 'gospel': 8036,\n",
       " 'wheelbase': 29141,\n",
       " 'momentary': 29089,\n",
       " 'atmosphere': 7224,\n",
       " '##yana': 16811,\n",
       " '##≡': 30134,\n",
       " 'doug': 8788,\n",
       " 'gee': 20277,\n",
       " '[unused253]': 258,\n",
       " 'tires': 13310,\n",
       " 'scenic': 12916,\n",
       " 'bruises': 18438,\n",
       " 'descent': 6934,\n",
       " 'shadowy': 22801,\n",
       " 'solid': 5024,\n",
       " 'encourage': 8627,\n",
       " 'hope': 3246,\n",
       " 'resided': 12427,\n",
       " 'mercury': 8714,\n",
       " '##q': 4160,\n",
       " '##uven': 27346,\n",
       " '294': 28135,\n",
       " 'sharpe': 22147,\n",
       " 'teaches': 12011,\n",
       " 'neck': 3300,\n",
       " 'fielding': 15452,\n",
       " '[unused437]': 442,\n",
       " 'callum': 15229,\n",
       " 'wear': 4929,\n",
       " 'ley': 25866,\n",
       " 'mer': 21442,\n",
       " '##ola': 6030,\n",
       " '53': 5187,\n",
       " '[unused843]': 848,\n",
       " 'apologetic': 29352,\n",
       " 'deprived': 17676,\n",
       " 'filming': 7467,\n",
       " 'mathematicians': 29374,\n",
       " 'tack': 26997,\n",
       " '1959': 3851,\n",
       " 'comune': 21130,\n",
       " '##香': 30505,\n",
       " 'adolph': 28564,\n",
       " '[unused578]': 583,\n",
       " 'symbolism': 22050,\n",
       " '[unused442]': 447,\n",
       " 'lebanon': 8341,\n",
       " 'patrolling': 24248,\n",
       " 'argue': 7475,\n",
       " 'genes': 9165,\n",
       " 'founders': 8759,\n",
       " 'clouded': 26761,\n",
       " '##aus': 20559,\n",
       " 'cinematic': 21014,\n",
       " 'visuals': 26749,\n",
       " 'recreational': 10517,\n",
       " 'restore': 9239,\n",
       " '##entes': 26933,\n",
       " 'govt': 22410,\n",
       " 'manchester': 5087,\n",
       " 'hey': 4931,\n",
       " 'earthly': 29520,\n",
       " 'obscure': 14485,\n",
       " 'procurement': 21423,\n",
       " 'profiles': 17879,\n",
       " 'modifications': 12719,\n",
       " 'madden': 24890,\n",
       " 'loads': 15665,\n",
       " 'expose': 14451,\n",
       " 'guerrero': 16938,\n",
       " 'revival': 6308,\n",
       " 'mountains': 4020,\n",
       " '[unused158]': 163,\n",
       " 'salts': 23480,\n",
       " '##year': 29100,\n",
       " 'obsessed': 15896,\n",
       " 'assertion': 23617,\n",
       " 'starts': 4627,\n",
       " 'scholastic': 24105,\n",
       " 'clumsy': 22902,\n",
       " 'hydrogen': 9732,\n",
       " '##itung': 28813,\n",
       " 'bland': 20857,\n",
       " 'interrupting': 22602,\n",
       " 'malicious': 24391,\n",
       " 'seed': 6534,\n",
       " '##tet': 22513,\n",
       " 'ventilation': 19536,\n",
       " 'puck': 22900,\n",
       " 'followed': 2628,\n",
       " 'lisa': 7059,\n",
       " 'fork': 9292,\n",
       " 'inquiries': 27050,\n",
       " 'antennae': 28624,\n",
       " 'parks': 6328,\n",
       " 'patrons': 13497,\n",
       " '##liner': 20660,\n",
       " '##ve': 3726,\n",
       " 'also': 2036,\n",
       " 'blade': 6085,\n",
       " '1846': 9244,\n",
       " 'joanne': 23459,\n",
       " '龍': 1982,\n",
       " 'purity': 18433,\n",
       " 'ぬ': 1669,\n",
       " 'circumstance': 25652,\n",
       " 'crowded': 10789,\n",
       " 'slams': 25967,\n",
       " '##tea': 27058,\n",
       " 'prentice': 23429,\n",
       " 'tunes': 13281,\n",
       " 'residency': 14079,\n",
       " 'weighed': 12781,\n",
       " 'disqualified': 14209,\n",
       " 'rosemary': 18040,\n",
       " 'saxophone': 8283,\n",
       " 'brooklyn': 6613,\n",
       " 'ornamental': 18200,\n",
       " 'north': 2167,\n",
       " 'comedies': 22092,\n",
       " '##dial': 27184,\n",
       " '##پ': 29839,\n",
       " 'integral': 9897,\n",
       " '##eau': 10207,\n",
       " 'geologic': 22125,\n",
       " 'coordinate': 13530,\n",
       " '1889': 6478,\n",
       " 'blending': 23293,\n",
       " 'illustrates': 24899,\n",
       " 'progressively': 20519,\n",
       " 'vampire': 4393,\n",
       " '前': 1776,\n",
       " '##lun': 26896,\n",
       " '07': 5718,\n",
       " 'tourism': 6813,\n",
       " '1758': 16832,\n",
       " 'featuring': 3794,\n",
       " 'hair': 2606,\n",
       " '##rton': 11715,\n",
       " 'faded': 8105,\n",
       " 'bromwich': 27888,\n",
       " '##ج': 29819,\n",
       " '[unused943]': 948,\n",
       " 'realises': 25924,\n",
       " 'congo': 9030,\n",
       " 'silvery': 21666,\n",
       " 'vince': 12159,\n",
       " '28': 2654,\n",
       " 'superhero': 16251,\n",
       " 'ʼ': 1146,\n",
       " 'adorable': 23677,\n",
       " 'chores': 27091,\n",
       " 'sailing': 8354,\n",
       " '##cm': 27487,\n",
       " '##ivate': 21466,\n",
       " '##公': 30298,\n",
       " 'availability': 11343,\n",
       " '##ina': 3981,\n",
       " 'brussels': 9371,\n",
       " 'curve': 7774,\n",
       " 'llc': 11775,\n",
       " 'pagoda': 27387,\n",
       " 'life': 2166,\n",
       " 'residing': 7154,\n",
       " 'spilling': 18054,\n",
       " 'bud': 13007,\n",
       " '##nish': 24014,\n",
       " '1914': 4554,\n",
       " '1840s': 19308,\n",
       " '##dre': 16200,\n",
       " 'hawkins': 13835,\n",
       " 'sane': 22856,\n",
       " 'pauline': 15595,\n",
       " 'tons': 6197,\n",
       " 'commented': 7034,\n",
       " '[unused644]': 649,\n",
       " 'jets': 9924,\n",
       " 'tribe': 5917,\n",
       " 'shannon': 10881,\n",
       " '##国': 30325,\n",
       " 'interactive': 9123,\n",
       " 'inheritance': 12839,\n",
       " 'functional': 8360,\n",
       " 'completed': 2949,\n",
       " 'carnegie': 11298,\n",
       " '##tica': 22723,\n",
       " '##セ': 30234,\n",
       " 'disbanded': 8532,\n",
       " 'thanking': 28638,\n",
       " 'reigning': 16323,\n",
       " 'museums': 9941,\n",
       " '##berto': 21201,\n",
       " 'sheets': 8697,\n",
       " 'す': 1658,\n",
       " 'airlift': 20019,\n",
       " 'grammatical': 24402,\n",
       " 'horsemen': 25431,\n",
       " 'mab': 26661,\n",
       " '##woman': 10169,\n",
       " '##grin': 24860,\n",
       " 'beacon': 14400,\n",
       " 'data': 2951,\n",
       " 'adolf': 12500,\n",
       " 'capabilities': 9859,\n",
       " 'projecting': 18398,\n",
       " 'rae': 14786,\n",
       " '##ள': 29929,\n",
       " 'got': 2288,\n",
       " '130': 7558,\n",
       " '##ship': 9650,\n",
       " 'liberal': 4314,\n",
       " 'haas': 22996,\n",
       " 'pointed': 4197,\n",
       " 'stake': 8406,\n",
       " 'flea': 26735,\n",
       " '##rath': 27362,\n",
       " 'inputs': 20407,\n",
       " 'gulf': 6084,\n",
       " 'social': 2591,\n",
       " 'softly': 5238,\n",
       " 'method': 4118,\n",
       " 'vulnerability': 18130,\n",
       " 'cousin': 5542,\n",
       " '##idad': 27893,\n",
       " 'afb': 16909,\n",
       " 'attending': 7052,\n",
       " '##lr': 20974,\n",
       " 'provided': 3024,\n",
       " 'patients': 5022,\n",
       " 'realistic': 12689,\n",
       " '##ち': 30188,\n",
       " 'unspecified': 25851,\n",
       " '##held': 24850,\n",
       " 'regina': 12512,\n",
       " 'allies': 6956,\n",
       " '##word': 18351,\n",
       " 'chao': 22455,\n",
       " '##jun': 19792,\n",
       " 'oversight': 15709,\n",
       " '##nesia': 21509,\n",
       " 'mayoral': 23578,\n",
       " '##rchy': 29389,\n",
       " 'petite': 20146,\n",
       " 'thanksgiving': 15060,\n",
       " 'costumes': 12703,\n",
       " 'idea': 2801,\n",
       " 'monstrous': 21668,\n",
       " '##hn': 7295,\n",
       " 'offers': 4107,\n",
       " 'statements': 8635,\n",
       " '##neil': 27276,\n",
       " 'extraction': 14676,\n",
       " 'provence': 19923,\n",
       " '##oped': 24174,\n",
       " 'cannot': 3685,\n",
       " 'colt': 9110,\n",
       " 'loosen': 29476,\n",
       " 'yamaha': 24031,\n",
       " 'yugoslavia': 8936,\n",
       " 'vacation': 10885,\n",
       " '久': 1748,\n",
       " 'enigma': 26757,\n",
       " 'sergey': 22703,\n",
       " '##pace': 15327,\n",
       " '[unused898]': 903,\n",
       " '##bies': 20536,\n",
       " 'doing': 2725,\n",
       " 'quiz': 19461,\n",
       " 'modernized': 27140,\n",
       " '##rant': 17884,\n",
       " 'fucking': 8239,\n",
       " 'vaccine': 17404,\n",
       " 'war': 2162,\n",
       " 'hague': 14575,\n",
       " 'divers': 18612,\n",
       " 'inference': 28937,\n",
       " 'browns': 13240,\n",
       " 'eyed': 7168,\n",
       " '##iac': 20469,\n",
       " '##ika': 7556,\n",
       " 'raf': 7148,\n",
       " 'celebrities': 12330,\n",
       " 'of': 1997,\n",
       " 'attacker': 17346,\n",
       " 'diocese': 5801,\n",
       " 'jurisdictions': 17370,\n",
       " '##kill': 15872,\n",
       " 'enemies': 6716,\n",
       " ...}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_bert.vocab_size\n",
    "tokenizer_bert.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " 'ä',\n",
       " 'å',\n",
       " 'é',\n",
       " 'ô',\n",
       " 'ö',\n",
       " 'ü',\n",
       " ' ',\n",
       " '[UNK]',\n",
       " '[PAD]']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<pad>': 0,\n",
       " '<s>': 1,\n",
       " '</s>': 2,\n",
       " '<unk>': 3,\n",
       " '|': 4,\n",
       " 'E': 5,\n",
       " 'T': 6,\n",
       " 'A': 7,\n",
       " 'O': 8,\n",
       " 'N': 9,\n",
       " 'I': 10,\n",
       " 'H': 11,\n",
       " 'S': 12,\n",
       " 'R': 13,\n",
       " 'D': 14,\n",
       " 'L': 15,\n",
       " 'U': 16,\n",
       " 'M': 17,\n",
       " 'W': 18,\n",
       " 'C': 19,\n",
       " 'F': 20,\n",
       " 'G': 21,\n",
       " 'Y': 22,\n",
       " 'P': 23,\n",
       " 'B': 24,\n",
       " 'V': 25,\n",
       " 'K': 26,\n",
       " \"'\": 27,\n",
       " 'X': 28,\n",
       " 'J': 29,\n",
       " 'Q': 30,\n",
       " 'Z': 31}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/sound_asr\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/workspace/sound_asr'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %cd ..\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing with use_bpe=False and use_lm=False ---\n",
      "CTC Text Encoder:\n",
      "pretrained_tokenizer: facebook/wav2vec2-base-960h\n",
      "lm_weight: 0.5\n",
      "beam_size: 100\n",
      "binary_path: 4-gram_lc_correct.bin\n",
      "use_lm: False\n",
      "use_bpe: False\n",
      "Loading unigrams from: librispeech-vocab.txt\n",
      "Loaded 200000 unigrams\n",
      "Initializing character-based vocabulary without using tokenizer.\n",
      "\n",
      "Vocabulary Info:\n",
      "Size: 29\n",
      "Full Vocabulary (up to first 50 tokens): ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', ' ', '[pad]', '[unk]']\n",
      "Blank token: [pad], Blank index: 27\n",
      "Sample ind2char mappings: {0: 'a', 1: 'b', 2: 'c', 3: 'd', 4: 'e', 5: 'f', 6: 'g', 7: 'h', 8: 'i', 9: 'j'}\n",
      "Sample char2ind mappings: {'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': 7, 'i': 8, 'j': 9}\n",
      "Language model usage is disabled.\n",
      "Encoded: tensor([[ 7,  4, 11, 11, 14, 26, 22, 14, 17, 11,  3]])\n",
      "Decoded: hello world\n",
      "Decoded without LM: ojxqzbu es\n",
      "\n",
      "--- Testing with use_bpe=True and use_lm=False ---\n",
      "CTC Text Encoder:\n",
      "pretrained_tokenizer: facebook/wav2vec2-base-960h\n",
      "lm_weight: 0.5\n",
      "beam_size: 100\n",
      "binary_path: 4-gram_lc_correct.bin\n",
      "use_lm: False\n",
      "use_bpe: True\n",
      "Loading unigrams from: librispeech-vocab.txt\n",
      "Loaded 200000 unigrams\n",
      "Initializing tokenizer and using BPE for encoding/decoding.\n",
      "Added '[pad]' to the tokenizer's vocabulary.\n",
      "Added '[unk]' to the tokenizer's vocabulary.\n",
      "Vocabulary (first 20 tokens): ['<pad>', '<s>', '</s>', '<unk>', ' ', 'e', 't', 'a', 'o', 'n', 'i', 'h', 's', 'r', 'd', 'l', 'u', 'm', 'w', 'c']\n",
      "\n",
      "Vocabulary Info:\n",
      "Size: 32\n",
      "Full Vocabulary (up to first 50 tokens): ['<pad>', '<s>', '</s>', '<unk>', ' ', 'e', 't', 'a', 'o', 'n', 'i', 'h', 's', 'r', 'd', 'l', 'u', 'm', 'w', 'c', 'f', 'g', 'y', 'p', 'b', 'v', 'k', \"'\", 'x', 'j', 'q', 'z']\n",
      "Blank token: [pad], Blank index: None\n",
      "Sample ind2char mappings: {0: '<pad>', 1: '<s>', 2: '</s>', 3: '<unk>', 4: ' ', 5: 'e', 6: 't', 7: 'a', 8: 'o', 9: 'n'}\n",
      "Sample char2ind mappings: {'<pad>': 0, '<s>': 1, '</s>': 2, '<unk>': 3, ' ': 4, 'e': 5, 't': 6, 'a': 7, 'o': 8, 'n': 9}\n",
      "Language model usage is disabled.\n",
      "Encoded: tensor([[3, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3]])\n",
      "Decoded: <unk><unk><unk><unk><unk> <unk><unk><unk><unk><unk>\n",
      "Decoded without LM: fakzxiuhhts\n",
      "\n",
      "--- Testing with use_bpe=False and use_lm=True ---\n",
      "CTC Text Encoder:\n",
      "pretrained_tokenizer: facebook/wav2vec2-base-960h\n",
      "lm_weight: 0.5\n",
      "beam_size: 100\n",
      "binary_path: 4-gram_lc_correct.bin\n",
      "use_lm: True\n",
      "use_bpe: False\n",
      "Loading unigrams from: librispeech-vocab.txt\n",
      "Loaded 200000 unigrams\n",
      "Initializing character-based vocabulary without using tokenizer.\n",
      "\n",
      "Vocabulary Info:\n",
      "Size: 29\n",
      "Full Vocabulary (up to first 50 tokens): ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', ' ', '[pad]', '[unk]']\n",
      "Blank token: [pad], Blank index: 27\n",
      "Sample ind2char mappings: {0: 'a', 1: 'b', 2: 'c', 3: 'd', 4: 'e', 5: 'f', 6: 'g', 7: 'h', 8: 'i', 9: 'j'}\n",
      "Sample char2ind mappings: {'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': 7, 'i': 8, 'j': 9}\n",
      "model_path:  4-gram_lc_correct.bin\n",
      "Loaded binary language model.\n",
      "\n",
      "--- Unigrams List ---\n",
      "Unigrams list saved to 'unigrams_list.txt'. Total unigrams: 200000\n",
      "----------------------\n",
      "\n",
      "Successfully initialized language model and decoder.\n",
      "Encoded: tensor([[ 7,  4, 11, 11, 14, 26, 22, 14, 17, 11,  3]])\n",
      "Decoded: hello world\n",
      "Decoded with LM: jacob\n",
      "\n",
      "--- Testing with use_bpe=True and use_lm=True ---\n",
      "CTC Text Encoder:\n",
      "pretrained_tokenizer: facebook/wav2vec2-base-960h\n",
      "lm_weight: 0.5\n",
      "beam_size: 100\n",
      "binary_path: 4-gram_lc_correct.bin\n",
      "use_lm: True\n",
      "use_bpe: True\n",
      "Loading unigrams from: librispeech-vocab.txt\n",
      "Loaded 200000 unigrams\n",
      "Initializing tokenizer and using BPE for encoding/decoding.\n",
      "Added '[pad]' to the tokenizer's vocabulary.\n",
      "Added '[unk]' to the tokenizer's vocabulary.\n",
      "Vocabulary (first 20 tokens): ['<pad>', '<s>', '</s>', '<unk>', ' ', 'e', 't', 'a', 'o', 'n', 'i', 'h', 's', 'r', 'd', 'l', 'u', 'm', 'w', 'c']\n",
      "\n",
      "Vocabulary Info:\n",
      "Size: 32\n",
      "Full Vocabulary (up to first 50 tokens): ['<pad>', '<s>', '</s>', '<unk>', ' ', 'e', 't', 'a', 'o', 'n', 'i', 'h', 's', 'r', 'd', 'l', 'u', 'm', 'w', 'c', 'f', 'g', 'y', 'p', 'b', 'v', 'k', \"'\", 'x', 'j', 'q', 'z']\n",
      "Blank token: [pad], Blank index: None\n",
      "Sample ind2char mappings: {0: '<pad>', 1: '<s>', 2: '</s>', 3: '<unk>', 4: ' ', 5: 'e', 6: 't', 7: 'a', 8: 'o', 9: 'n'}\n",
      "Sample char2ind mappings: {'<pad>': 0, '<s>': 1, '</s>': 2, '<unk>': 3, ' ': 4, 'e': 5, 't': 6, 'a': 7, 'o': 8, 'n': 9}\n",
      "model_path:  4-gram_lc_correct.bin\n",
      "Loaded binary language model.\n",
      "\n",
      "--- Unigrams List ---\n",
      "Unigrams list saved to 'unigrams_list.txt'. Total unigrams: 200000\n",
      "----------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found entries of length > 1 in alphabet. This is unusual unless style is BPE, but the alphabet was not recognized as BPE type. Is this correct?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully initialized language model and decoder.\n",
      "Encoded: tensor([[3, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3]])\n",
      "Decoded: <unk><unk><unk><unk><unk> <unk><unk><unk><unk><unk>\n",
      "Decoded with LM: olivo\n"
     ]
    }
   ],
   "source": [
    "def test_encoder(encoder_cls, text=\"hello world\"):\n",
    "    \"\"\"\n",
    "    Test the encoder for various configurations of use_bpe and use_lm.\n",
    "\n",
    "    Parameters:\n",
    "    - encoder_cls: Class of the encoder to test.\n",
    "    - text (str): The input text to encode and decode.\n",
    "    \"\"\"\n",
    "    test_cases = [\n",
    "        {\"use_bpe\": False, \"use_lm\": False},\n",
    "        {\"use_bpe\": True, \"use_lm\": False},\n",
    "        {\"use_bpe\": False, \"use_lm\": True},\n",
    "        {\"use_bpe\": True, \"use_lm\": True},\n",
    "    ]\n",
    "\n",
    "    for case in test_cases:\n",
    "        print(f\"\\n--- Testing with use_bpe={case['use_bpe']} and use_lm={case['use_lm']} ---\")\n",
    "\n",
    "        # Initialize encoder with the test case settings\n",
    "        encoder = encoder_cls(\n",
    "            use_bpe=case[\"use_bpe\"],\n",
    "            use_lm=case[\"use_lm\"],\n",
    "            pretrained_tokenizer=\"facebook/wav2vec2-base-960h\",\n",
    "            binary_path=\"4-gram_lc_correct.bin\",\n",
    "            unigram_path=\"librispeech-vocab.txt\"\n",
    "        )\n",
    "\n",
    "        # Test encoding\n",
    "        encoded = encoder.encode(text)\n",
    "        print(f\"Encoded: {encoded}\")\n",
    "\n",
    "        # Test decoding from encoded indices\n",
    "        decoded = encoder.decode_simple(encoded[0].tolist())\n",
    "        print(f\"Decoded: {decoded}\")\n",
    "\n",
    "        # Test decoding from logits (fake logits for demonstration)\n",
    "        sequence_length = encoded.shape[1]\n",
    "        vocab_size = len(encoder.vocab)\n",
    "        logits = torch.randn(1, sequence_length, vocab_size)  # Random logits for testing\n",
    "\n",
    "        if case[\"use_lm\"]:\n",
    "            decoded_with_lm = encoder.ctc_decode(logits)\n",
    "            print(f\"Decoded with LM: {decoded_with_lm}\")\n",
    "        else:\n",
    "            decoded_without_lm = encoder.decode_logits(logits)\n",
    "            print(f\"Decoded without LM: {decoded_without_lm}\")\n",
    "\n",
    "# Example usage\n",
    "test_encoder(CTCTextEncoder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/sound_asr\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import kenlm\n",
    "from transformers import Wav2Vec2Processor, AutoProcessor, AutoTokenizer\n",
    "from pyctcdecode import build_ctcdecoder\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from typing import List, Tuple, Optional, Union\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "class CTCTextEncoder:\n",
    "    def __init__(\n",
    "        self,\n",
    "        arpa_path: Optional[str] = None,\n",
    "        # binary_path: Optional[str] = None,\n",
    "        # unigram_path: Optional[str] = None,\n",
    "        binary_path: Optional[str] = \"4-gram_lc_correct.bin\",\n",
    "        unigram_path: Optional[str] = \"librispeech-vocab.txt\",\n",
    "        pretrained_tokenizer: str = \"facebook/wav2vec2-base-960h\",\n",
    "        lm_weight: float = 0.5,\n",
    "        beam_size: int = 100,\n",
    "        use_lm: bool = False,     # **Added use_lm parameter**\n",
    "        use_bpe: bool = False,    # **Added use_bpe parameter**\n",
    "        # blank_token: str = \"<pad>\",  # Blank token as <pad> for Wav2Vec2\n",
    "        # unk_token: str = \"<unk>\",     # UNK token\n",
    "        blank_token: str = \"[PAD]\",\n",
    "        unk_token: str = \"[UNK]\",\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize encoder with conditional tokenizer/processor and language model.\n",
    "\n",
    "        Parameters:\n",
    "        - use_lm (bool): Whether to use the Language Model (LM) during decoding.\n",
    "        - use_bpe (bool): Whether to use Byte Pair Encoding (BPE) via tokenizer/processor.\n",
    "                           If False, perform character-based encoding/decoding without tokenizer.\n",
    "        \"\"\"\n",
    "        self.beam_size = beam_size\n",
    "        self.lm_weight = lm_weight\n",
    "        self.arpa_path = arpa_path\n",
    "        self.binary_path = binary_path\n",
    "        self.blank_token = blank_token\n",
    "        self.unk_token = unk_token\n",
    "        self.use_lm = use_lm # False\n",
    "        self.use_bpe = use_bpe # False MANUAL FOR NOW\n",
    "        # self.use_bpe = False # use_bpe # False # use_bpe\n",
    "        self.printed_samples = 0\n",
    "        self.max_printed_samples = 5\n",
    "        print('CTC Text Encoder:')\n",
    "        print('pretrained_tokenizer:', pretrained_tokenizer)\n",
    "        print('lm_weight:', lm_weight)\n",
    "        print('beam_size:', beam_size)\n",
    "        print('binary_path:', binary_path)\n",
    "        print('use_lm:', self.use_lm)\n",
    "        print('use_bpe:', self.use_bpe)\n",
    "        self.pretrained_tokenizer = pretrained_tokenizer\n",
    "\n",
    "        # Load unigrams if provided\n",
    "        self.unigrams = None\n",
    "        if unigram_path and os.path.exists(unigram_path):\n",
    "            print(f\"Loading unigrams from: {unigram_path}\")\n",
    "            with open(unigram_path, 'r', encoding='utf-8') as f:\n",
    "                self.unigrams = [line.strip().lower() for line in f if line.strip()]\n",
    "            print(f\"Loaded {len(self.unigrams)} unigrams\")\n",
    "\n",
    "        # Initialize the tokenizer or set up character-based vocab\n",
    "        self._initialize_vocabulary(pretrained_tokenizer)\n",
    "\n",
    "        # Create index mappings\n",
    "        self.ind2char = dict(enumerate(self.vocab))\n",
    "        self.char2ind = {v: k for k, v in self.ind2char.items()}\n",
    "        self.blank_index = self.char2ind.get(self.blank_token, None)\n",
    "\n",
    "        print(f\"\\nVocabulary Info:\")\n",
    "        print(f\"Size: {len(self.vocab)}\")\n",
    "        print(\"Full Vocabulary (up to first 50 tokens):\", self.vocab[:50])\n",
    "        print(f\"Blank token: {self.blank_token}, Blank index: {self.blank_index}\")\n",
    "\n",
    "        print(\"Sample ind2char mappings:\", {k: self.ind2char[k] for k in list(self.ind2char.keys())[:10]})\n",
    "        print(\"Sample char2ind mappings:\", {k: self.char2ind[k] for k in list(self.char2ind.keys())[:10]})\n",
    "\n",
    "        # **Conditionally initialize language model based on use_lm**\n",
    "        if self.use_lm:\n",
    "            self._initialize_language_model()\n",
    "        else:\n",
    "            print(\"Language model usage is disabled.\")\n",
    "            self.lm = None\n",
    "            self.decoder = None\n",
    "\n",
    "    \n",
    "    # def _initialize_vocabulary(self, pretrained_tokenizer: str):\n",
    "    #     \"\"\"\n",
    "    #     Initialize the vocabulary either using a tokenizer/processor (BPE) or character-based.\n",
    "\n",
    "    #     Parameters:\n",
    "    #     - pretrained_tokenizer (str): Name or path of the pretrained tokenizer.\n",
    "    #     \"\"\"\n",
    "    #     if self.use_bpe:\n",
    "    #         print(\"Initializing tokenizer and using BPE for encoding/decoding.\")\n",
    "    #         if self.pretrained_tokenizer == \"facebook/wav2vec2-base-960h\":\n",
    "    #             self.tokenizer = Wav2Vec2Processor.from_pretrained(pretrained_tokenizer)\n",
    "    #             vocab_dict = self.tokenizer.tokenizer.get_vocab()\n",
    "    #             sorted_vocab_dict = {k.lower(): v for k, v in sorted(vocab_dict.items(), key=lambda item: item[1])}\n",
    "    #             self.labels = list(sorted_vocab_dict.keys())\n",
    "    #             self.vocab = [token.replace('|', ' ') for token in self.labels]\n",
    "\n",
    "    #         else:\n",
    "    #             self.tokenizer = None\n",
    "    #             self.tokenizer = AutoTokenizer.from_pretrained(pretrained_tokenizer)\n",
    "    #             vocab_dict = self.tokenizer.vocab\n",
    "    #             # self.labels = list(vocab_dict.keys())\n",
    "\n",
    "    #             sorted_vocab = sorted(vocab_dict.items(), key=lambda item: item[1])  # Sort by index\n",
    "    #             self.vocab = [token for token, _ in sorted_vocab]  # Preserve order\n",
    "    #             self.labels = self.vocab  # Keep alignment with tokenizer\n",
    "    #             self.vocab = [token.replace('|', ' ') for token in self.labels]\n",
    "\n",
    "                \n",
    "            \n",
    "            \n",
    "\n",
    "    #         if self.blank_token not in self.tokenizer.get_vocab():\n",
    "    #             self.tokenizer.add_tokens([self.blank_token])\n",
    "    #             print(f\"Added '{self.blank_token}' to the tokenizer's vocabulary.\")\n",
    "\n",
    "    #         if self.unk_token not in self.tokenizer.get_vocab():\n",
    "    #             self.tokenizer.add_tokens([self.unk_token])\n",
    "    #             print(f\"Added '{self.unk_token}' to the tokenizer's vocabulary.\")\n",
    "\n",
    "    #         self.ind2char = dict(enumerate(self.vocab))\n",
    "    #         self.char2ind = {v: k for k, v in self.ind2char.items()}\n",
    "    #         self.blank_index = self.char2ind.get(self.blank_token, None)\n",
    "    #     else:\n",
    "    #         print(\"Initializing character-based vocabulary without using tokenizer.\")\n",
    "    #         self.vocab = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',\n",
    "    #                     'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x',\n",
    "    #                     'y', 'z', ' ']\n",
    "    #         # self.vocab += [self.blank_token, self.unk_token]\n",
    "    #         self.vocab = [self.blank_token] + self.vocab\n",
    "    #         self.ind2char = dict(enumerate(self.vocab))\n",
    "    #         self.char2ind = {v: k for k, v in self.ind2char.items()}\n",
    "    #         self.blank_index = self.char2ind.get(self.blank_token, None)\n",
    "    #         self.tokenizer = None\n",
    "\n",
    "\n",
    "\n",
    "    def _initialize_vocabulary(self, pretrained_tokenizer: str):\n",
    "        \"\"\"\n",
    "        Initialize the vocabulary using BPE or character-based approach.\n",
    "        \"\"\"\n",
    "        if self.use_bpe:\n",
    "            print(\"Initializing tokenizer and using BPE for encoding/decoding.\")\n",
    "            \n",
    "            # Load tokenizer (AutoTokenizer supports most HF models)\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(pretrained_tokenizer)\n",
    "            \n",
    "            # Get the tokenizer's vocabulary\n",
    "            vocab_dict = self.tokenizer.get_vocab()\n",
    "            sorted_vocab = sorted(vocab_dict.items(), key=lambda item: item[1])  # Sort by token ID\n",
    "            \n",
    "            # Ensure true BPE subword tokens are used\n",
    "            self.vocab = [token for token, _ in sorted_vocab]\n",
    "            print(f\"Loaded BPE vocabulary with {len(self.vocab)} tokens.\")\n",
    "            \n",
    "            # Add blank and unknown tokens if not already present\n",
    "            if self.blank_token not in self.vocab:\n",
    "                self.tokenizer.add_tokens([self.blank_token])\n",
    "                self.vocab.append(self.blank_token)\n",
    "                print(f\"Added '{self.blank_token}' to the vocabulary.\")\n",
    "\n",
    "            if self.unk_token not in self.vocab:\n",
    "                self.tokenizer.add_tokens([self.unk_token])\n",
    "                self.vocab.append(self.unk_token)\n",
    "                print(f\"Added '{self.unk_token}' to the vocabulary.\")\n",
    "\n",
    "            # Create index mappings\n",
    "            self.ind2char = dict(enumerate(self.vocab))\n",
    "            self.char2ind = {v: k for k, v in self.ind2char.items()}\n",
    "            self.blank_index = self.char2ind.get(self.blank_token, None)\n",
    "            \n",
    "            print(f\"Sample vocabulary: {self.vocab[:10]}\")\n",
    "\n",
    "            self.labels = self.vocab\n",
    "\n",
    "        else:\n",
    "            print(\"Initializing character-based vocabulary without using tokenizer.\")\n",
    "            self.vocab = [' '] + list(\"abcdefghijklmnopqrstuvwxyzäåéôöü\")\n",
    "            self.vocab += [self.unk_token, self.blank_token]\n",
    "            self.ind2char = dict(enumerate(self.vocab))\n",
    "            self.char2ind = {v: k for k, v in self.ind2char.items()}\n",
    "            self.blank_index = self.char2ind.get(self.blank_token, None)\n",
    "            self.tokenizer = None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # def encode(self, text: str) -> torch.Tensor:\n",
    "    #     \"\"\"\n",
    "    #     Encode text either using tokenizer/processor (BPE) or character-based encoding.\n",
    "\n",
    "    #     Parameters:\n",
    "    #     - text (str): The input text to encode.\n",
    "\n",
    "    #     Returns:\n",
    "    #     - torch.Tensor: Tensor of token indices.\n",
    "    #     \"\"\"\n",
    "    #     if self.use_bpe:\n",
    "    #         if self.pretrained_tokenizer == \"facebook/wav2vec2-base-960h\":\n",
    "    #             text = text.upper() # convert to upper for Wav2Vec2 vocab\n",
    "    #         encoded = self.tokenizer(text, return_tensors=\"pt\", padding=False, truncation=False)\n",
    "    #         token_indices = encoded.input_ids[0].tolist()\n",
    "    #         return torch.tensor(token_indices).unsqueeze(0)\n",
    "    #     else:\n",
    "    #         normalized_text = self.normalize_text(text)\n",
    "    #         token_indices = [self.char2ind.get(char, self.char2ind.get(self.unk_token)) for char in normalized_text]\n",
    "    #         return torch.tensor(token_indices).unsqueeze(0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def encode(self, text: str) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Encode text either using tokenizer/processor (BPE) or character-based encoding.\n",
    "\n",
    "        Parameters:\n",
    "        - text (str): The input text to encode.\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: Tensor of token indices.\n",
    "        \"\"\"\n",
    "        if self.use_bpe:\n",
    "            encoded = self.tokenizer(text, return_tensors=\"pt\", padding=False, truncation=False, add_special_tokens=False)\n",
    "            token_indices = encoded[\"input_ids\"][0].tolist()\n",
    "            print(f\"[DEBUG] BPE Encoding: {text} -> {token_indices}\")\n",
    "            return torch.tensor(token_indices).unsqueeze(0)\n",
    "        else:\n",
    "            normalized_text = self.normalize_text(text)\n",
    "            token_indices = [self.char2ind.get(char, self.char2ind.get(self.unk_token)) for char in normalized_text]\n",
    "            print(f\"[DEBUG] Character Encoding: {text} -> {token_indices}\")\n",
    "            return torch.tensor(token_indices).unsqueeze(0)\n",
    "\n",
    "\n",
    "\n",
    "    def decode_simple(self, indices: List[int]) -> str:\n",
    "        \"\"\"\n",
    "        Simple CTC decoding without language model.\n",
    "        Collapses consecutive duplicate tokens and removes blanks.\n",
    "\n",
    "        Parameters:\n",
    "        - indices (List[int]): List of token indices.\n",
    "\n",
    "        Returns:\n",
    "        - str: Decoded text.\n",
    "        \"\"\"\n",
    "        decoded_chars = []\n",
    "        previous_idx = None\n",
    "\n",
    "        for idx in indices:\n",
    "            if idx == self.blank_index:\n",
    "                previous_idx = idx\n",
    "                continue  # Skip blank tokens\n",
    "            if idx == previous_idx:                   ##### TO TEST DELETION!!! #####\n",
    "                continue  # Skip duplicate tokens     ##### TO TEST DELETION!!! #####\n",
    "            if 0 <= idx < len(self.ind2char):\n",
    "                char = self.ind2char[idx]\n",
    "                decoded_chars.append(char)\n",
    "            previous_idx = idx\n",
    "\n",
    "        # Join characters without spaces and convert to lowercase\n",
    "        text = \"\".join(decoded_chars).strip().lower()\n",
    "\n",
    "        if self.use_bpe and self.tokenizer:\n",
    "            return self.tokenizer.clean_up_tokenization(text)\n",
    "        else:\n",
    "            return text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _initialize_language_model(self):\n",
    "        \"\"\"Initialize language model with explicit blank token handling.\"\"\"\n",
    "        self.lm = None\n",
    "        self.decoder = None\n",
    "\n",
    "        model_path = self.binary_path if self.binary_path else self.arpa_path\n",
    "        print('model_path: ', model_path)\n",
    "        if not model_path or not os.path.exists(model_path):\n",
    "            print(\"No language model path provided or file does not exist.\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            self.lm = kenlm.Model(model_path)\n",
    "            print(f\"Loaded {'binary' if self.binary_path else 'ARPA'} language model.\")\n",
    "\n",
    "            decoder_config = {\n",
    "                \"labels\": self.labels if self.use_bpe else self.vocab,\n",
    "                \"kenlm_model_path\": model_path,\n",
    "                \"alpha\": self.lm_weight,\n",
    "                \"beta\": 0.1,\n",
    "                \"unk_score_offset\": -10.0,\n",
    "                \n",
    "            }\n",
    "\n",
    "            if self.unigrams:\n",
    "                print(\"\\n--- Unigrams List ---\")\n",
    "                # Save the unigrams to a file\n",
    "                with open(\"unigrams_list.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "                    for unigram in self.unigrams:\n",
    "                        f.write(f\"{unigram}\\n\")\n",
    "                print(f\"Unigrams list saved to 'unigrams_list.txt'. Total unigrams: {len(self.unigrams)}\")\n",
    "                print(\"----------------------\\n\")\n",
    "                decoder_config[\"unigrams\"] = self.unigrams\n",
    "\n",
    "            self.decoder = build_ctcdecoder(**decoder_config)\n",
    "            print(\"Successfully initialized language model and decoder.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Failed to initialize decoder: {str(e)}\")\n",
    "            self.decoder = None\n",
    "\n",
    "    \n",
    "    def decode(self, indices: List[int]) -> str:\n",
    "        \"\"\"\n",
    "        Decode indices to text using beam search decoder if available.\n",
    "\n",
    "        Parameters:\n",
    "        - indices (List[int]): List of token indices.\n",
    "\n",
    "        Returns:\n",
    "        - str: Decoded text.\n",
    "        \"\"\"\n",
    "        if self.decoder:\n",
    "            decoded_text = self.decoder.decode(indices)\n",
    "            # Convert to lower case\n",
    "            decoded_text = decoded_text.lower()\n",
    "            return decoded_text\n",
    "        else:\n",
    "            decoded_text = self.decode_simple(indices)\n",
    "            # Convert to lower case\n",
    "            decoded_text = decoded_text.lower()\n",
    "            return decoded_text  # Ensure the decoded text is returned\n",
    "\n",
    "    \n",
    "    def decode_logits(self, logits: Union[torch.Tensor, List[List[float]], np.ndarray]) -> str:\n",
    "        \"\"\"\n",
    "        Decode logits using the decoder if available, otherwise use greedy decoding.\n",
    "\n",
    "        Parameters:\n",
    "        - logits (Union[torch.Tensor, List[List[float]], np.ndarray]): Logits output from the model.\n",
    "\n",
    "        Returns:\n",
    "        - str: Decoded text.\n",
    "        \"\"\"\n",
    "        if isinstance(logits, torch.Tensor):\n",
    "            logits = logits.cpu().numpy()\n",
    "        elif isinstance(logits, list):\n",
    "            logits = np.array(logits)\n",
    "        elif not isinstance(logits, np.ndarray):\n",
    "            raise TypeError(\"logits must be a torch.Tensor, list of lists, or numpy.ndarray\")\n",
    "\n",
    "        if logits.ndim == 3:\n",
    "            logits = logits[0]\n",
    "\n",
    "        if logits.ndim != 2:\n",
    "            raise ValueError(f\"Logits should be 2D (time_steps, vocab_size), got {logits.ndim}D\")\n",
    "\n",
    "        if self.decoder:\n",
    "            decoded_text = self.decoder.decode(logits)\n",
    "            return decoded_text\n",
    "        else:\n",
    "            predicted_indices = np.argmax(logits, axis=-1).tolist()\n",
    "            return self.decode_simple(predicted_indices)\n",
    "\n",
    "    def decode_indices(self, indices: Union[torch.Tensor, List[int], np.ndarray]) -> str:\n",
    "        \"\"\"\n",
    "        Decode token indices to text using simple decoding (no LM).\n",
    "\n",
    "        Parameters:\n",
    "        - indices (Union[torch.Tensor, List[int], np.ndarray]): Token indices.\n",
    "\n",
    "        Returns:\n",
    "        - str: Decoded text.\n",
    "        \"\"\"\n",
    "        if isinstance(indices, torch.Tensor):\n",
    "            indices = indices.squeeze().tolist()\n",
    "        elif isinstance(indices, np.ndarray):\n",
    "            indices = indices.tolist()\n",
    "        elif not isinstance(indices, list):\n",
    "            raise TypeError(\"decode_indices expects a list, torch.Tensor, or numpy.ndarray.\")\n",
    "\n",
    "        return self.decode_simple(indices)\n",
    "\n",
    "    def ctc_decode(self, logits: Union[torch.Tensor, List[int], np.ndarray]) -> str:\n",
    "        \"\"\"\n",
    "        Perform CTC decoding on logits.\n",
    "\n",
    "        Parameters:\n",
    "        - logits (Union[torch.Tensor, List[int], np.ndarray]): Logits or token indices.\n",
    "\n",
    "        Returns:\n",
    "        - str: Decoded text.\n",
    "        \"\"\"\n",
    "        if isinstance(logits, np.ndarray):\n",
    "            logits = torch.from_numpy(logits)\n",
    "        elif isinstance(logits, list):\n",
    "            logits = torch.tensor(logits)\n",
    "\n",
    "        if logits.dim() == 3:\n",
    "            logits = logits[0]  # Reduce to 2D (sequence length, vocab size)\n",
    "\n",
    "        if self.use_bpe:\n",
    "            if self.use_lm and self.decoder:\n",
    "                # Use LM if available\n",
    "                return self.decoder.decode(logits)\n",
    "            else:\n",
    "                # Use tokenizer-based decoding\n",
    "                predicted_indices = torch.argmax(logits, axis=-1).tolist()\n",
    "                return self.decode(predicted_indices)\n",
    "        else:\n",
    "            # Use character-based decoding\n",
    "            predicted_indices = torch.argmax(logits, axis=-1).tolist()\n",
    "            return self.decode_simple(predicted_indices)\n",
    "\n",
    "    \n",
    "\n",
    "    def ctc_beam_search(self, probs, beam_size, use_lm = False, debug: bool = False) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Beam search with optional Language Model support.\n",
    "\n",
    "        Parameters:\n",
    "        - probs: Probability distributions over tokens.\n",
    "        - debug (bool): Whether to print debug information.\n",
    "\n",
    "        Returns:\n",
    "        - List[Tuple[str, float]]: List of decoded text with scores.\n",
    "        \"\"\"\n",
    "        beam_size = self.beam_size\n",
    "        debug = False\n",
    "\n",
    "        if self.use_lm and self.decoder is not None:\n",
    "            try:\n",
    "                if isinstance(probs, torch.Tensor):\n",
    "                    probs = probs.cpu().numpy()\n",
    "                elif isinstance(probs, list):\n",
    "                    probs = np.array(probs)\n",
    "                elif isinstance(probs, np.ndarray):\n",
    "                    pass\n",
    "                else:\n",
    "                    raise TypeError(\"probs must be a torch.Tensor, list, or numpy.ndarray\")\n",
    "\n",
    "                beams = self.decoder.decode_beams(\n",
    "                    probs,\n",
    "                    beam_prune_logp=-10.0,\n",
    "                    token_min_logp=-5.0,\n",
    "                    hotwords=[],\n",
    "                    hotword_weight=10.0,\n",
    "                )\n",
    "\n",
    "                formatted_beams = []\n",
    "                for beam in beams[:beam_size]:\n",
    "                    text = beam[0]\n",
    "                    acoustic_score = beam[3]\n",
    "                    lm_score = beam[4]\n",
    "\n",
    "                    if self.use_bpe and self.tokenizer:\n",
    "                        text = self.tokenizer.clean_up_tokenization(text)\n",
    "                    text = text.lower().strip()\n",
    "\n",
    "                    combined_score = (1 - self.lm_weight) * acoustic_score + self.lm_weight * lm_score\n",
    "                    text_len = max(1, len(text.split())) if self.use_bpe else max(1, len(text))\n",
    "                    normalized_score = combined_score / text_len\n",
    "\n",
    "                    formatted_beams.append((text, normalized_score))\n",
    "\n",
    "                if debug:\n",
    "                    print(\"\\nFormatted beam results with Language Model:\")\n",
    "                    for text, score in formatted_beams[:3]:\n",
    "                        print(f\"Text: '{text}', Score: {score:.4f}\")\n",
    "\n",
    "                if formatted_beams:\n",
    "                    return sorted(formatted_beams, key=lambda x: -x[1])\n",
    "                else:\n",
    "                    print(\"No valid beams found, falling back to standard beam search\")\n",
    "                    return self._standard_beam_search(probs, debug)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Beam search with LM failed: {str(e)}, falling back to standard beam search\")\n",
    "                return self._standard_beam_search(probs, debug)\n",
    "        else:\n",
    "            return self._standard_beam_search(probs, debug)\n",
    "\n",
    "    def _standard_beam_search(self, probs, debug: bool = False) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Original beam search implementation without Language Model.\n",
    "\n",
    "        Parameters:\n",
    "        - probs: Probability distributions over tokens.\n",
    "        - debug (bool): Whether to print debug information.\n",
    "\n",
    "        Returns:\n",
    "        - List[Tuple[str, float]]: List of decoded text with scores.\n",
    "        \"\"\"\n",
    "        beam_size = self.beam_size\n",
    "\n",
    "        if isinstance(probs, np.ndarray):\n",
    "            probs = torch.from_numpy(probs)\n",
    "\n",
    "        if probs.device != torch.device('cpu'):\n",
    "            probs = probs.cpu()\n",
    "\n",
    "        dp = {(\"\", self.blank_token): 0.0}\n",
    "        log_probs = torch.log(probs + 1e-8)\n",
    "\n",
    "        if debug:\n",
    "            print(\"\\nStarting beam search with beam size:\", beam_size)\n",
    "\n",
    "        for t, prob in enumerate(log_probs):\n",
    "            new_dp = defaultdict(lambda: float('-inf'))\n",
    "            top_k = torch.topk(prob, k=min(beam_size, len(prob)))\n",
    "\n",
    "            if debug and t < self.max_printed_samples:\n",
    "                print(f\"\\nTimestep {t}:\")\n",
    "                print(\"Top tokens:\", [(self.ind2char[idx.item()], val.item()) \n",
    "                                    for val, idx in zip(top_k.values, top_k.indices)])\n",
    "\n",
    "            for val, ind in zip(top_k.values, top_k.indices):\n",
    "                curr_char = self.ind2char[ind.item()]\n",
    "                next_token_log_prob = val.item()\n",
    "\n",
    "                for (prefix, last_char), log_prob in dp.items():\n",
    "                    if last_char == curr_char and curr_char != \" \":\n",
    "                        new_prefix = prefix\n",
    "                    else:\n",
    "                        if curr_char != self.blank_token:\n",
    "                            if curr_char == \" \" and prefix.endswith(\" \"):\n",
    "                                continue\n",
    "                            new_prefix = prefix + curr_char\n",
    "                        else:\n",
    "                            new_prefix = prefix\n",
    "\n",
    "                    new_log_prob = log_prob + next_token_log_prob\n",
    "                    key = (new_prefix, curr_char)\n",
    "                    new_dp[key] = max(new_dp[key], new_log_prob)\n",
    "\n",
    "            if len(new_dp) > 0:\n",
    "                max_score = max(score for _, score in new_dp.items())\n",
    "                new_dp = {key: score - max_score for key, score in new_dp.items()}\n",
    "\n",
    "            dp = dict(sorted(new_dp.items(), key=lambda x: -x[1])[:beam_size])\n",
    "\n",
    "            if debug and t < 2:\n",
    "                print(\"\\nCurrent beam:\")\n",
    "                for (text, last_char), score in list(dp.items())[:3]:\n",
    "                    print(f\"Text: '{text}', Last: '{last_char}', Score: {score:.4f}\")\n",
    "\n",
    "        final_beams = []\n",
    "        for (text, _), score in dp.items():\n",
    "            if self.use_bpe and self.tokenizer:\n",
    "                text = self.tokenizer.clean_up_tokenization(text)\n",
    "            text = text.lower().strip()\n",
    "            text_len = max(1, len(text.split())) if self.use_bpe else max(1, len(text))\n",
    "            normalized_score = score / text_len\n",
    "            final_beams.append((text, normalized_score))\n",
    "\n",
    "        final_beams.sort(key=lambda x: -x[1])\n",
    "        if not final_beams:\n",
    "            final_beams = [(\"\", float('-inf'))]\n",
    "\n",
    "        return final_beams[:beam_size]\n",
    "\n",
    "    def test_language_model(self):\n",
    "        \"\"\"Debug function to verify LM functionality\"\"\"\n",
    "        print(\"\\nTesting Language Model...\")\n",
    "\n",
    "        if self.lm is None:\n",
    "            print(\"Error: Language model is not loaded!\")\n",
    "            return\n",
    "\n",
    "        test_sentences = [\n",
    "            \"this is a good sentence\",\n",
    "            \"this is also a good sentence\",\n",
    "            \"thiss iss nott aa goodd sentencee\",\n",
    "            \"random word salad box cat\",\n",
    "            \"the cat sat on the mat\",\n",
    "            \"\",\n",
    "            \"a\",\n",
    "        ]\n",
    "\n",
    "        print(\"\\nTesting individual sentences:\")\n",
    "        for sentence in test_sentences:\n",
    "            score = self.score_with_lm(sentence)\n",
    "            print(f\"\\nText: '{sentence}'\")\n",
    "            print(f\"LM Score: {score:.4f}\")\n",
    "\n",
    "        test_prefixes = [\n",
    "            \"the quick brown\",\n",
    "            \"how are\",\n",
    "            \"thank\",\n",
    "            \"nice to\",\n",
    "        ]\n",
    "\n",
    "        print(\"\\nTesting word completions:\")\n",
    "        for prefix in test_prefixes:\n",
    "            print(f\"\\nPrefix: '{prefix}'\")\n",
    "            completions = [\n",
    "                prefix + \" \" + word for word in [\"you\", \"fox\", \"cat\", \"xyz\", \"meet\"]\n",
    "            ]\n",
    "            scores = [(completion, self.score_with_lm(completion)) \n",
    "                    for completion in completions]\n",
    "            scores.sort(key=lambda x: x[1], reverse=True)\n",
    "            print(\"Top completions by score:\")\n",
    "            for completion, score in scores[:3]:\n",
    "                print(f\"  '{completion}': {score:.4f}\")\n",
    "\n",
    "    def score_with_lm(self, text: str) -> float:\n",
    "        \"\"\"\n",
    "        Score text using language model, handling edge cases.\n",
    "\n",
    "        Parameters:\n",
    "        - text (str): The input text to score.\n",
    "\n",
    "        Returns:\n",
    "        - float: LM score.\n",
    "        \"\"\"\n",
    "        if self.lm is None:\n",
    "            return 0.0\n",
    "\n",
    "        if not text or len(text.strip()) == 0:\n",
    "            return float('-inf')\n",
    "\n",
    "        text = text.lower().strip()\n",
    "        return self.lm.score(text, bos=True, eos=True)\n",
    "\n",
    "    def _basic_ctc_decode(self, logits: np.ndarray, sequence_length: int) -> List[str]:\n",
    "        \"\"\"\n",
    "        Basic CTC decoding without LM.\n",
    "\n",
    "        Parameters:\n",
    "        - logits (np.ndarray): Logits from the model.\n",
    "        - sequence_length (int): Length of the sequence to decode.\n",
    "\n",
    "        Returns:\n",
    "        - List[str]: Decoded text.\n",
    "        \"\"\"\n",
    "        argmax_indices = np.argmax(logits, axis=-1)\n",
    "\n",
    "        if len(argmax_indices.shape) == 0:\n",
    "            argmax_indices = np.array([argmax_indices])\n",
    "\n",
    "        if len(argmax_indices.shape) == 1:\n",
    "            argmax_indices = np.expand_dims(argmax_indices, axis=0)\n",
    "\n",
    "        predictions = []\n",
    "        for sequence in argmax_indices:\n",
    "            decoded = []\n",
    "            last_idx = None\n",
    "\n",
    "            for idx in sequence[:sequence_length]:\n",
    "                if idx != self.blank_index and idx != last_idx:\n",
    "                    decoded.append(self.ind2char[idx])\n",
    "                last_idx = idx\n",
    "\n",
    "            text = \"\".join(decoded)\n",
    "            if self.use_bpe and self.tokenizer:\n",
    "                text = self.tokenizer.clean_up_tokenization(text)\n",
    "            predictions.append(text)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_text(text: str) -> str:\n",
    "        \"\"\"Normalize input text.\"\"\"\n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"[^a-z ]\", \"\", text)\n",
    "        return text\n",
    "\n",
    "    def test_decoder(self, sample_text: str = \"test decoder functionality\"):\n",
    "        \"\"\"Test the decoder setup.\"\"\"\n",
    "        print(\"\\nTesting decoder configuration...\")\n",
    "\n",
    "        encoded = self.encode(sample_text)\n",
    "        decoded = self.decode(encoded[0].tolist())\n",
    "        print(f\"Original text: {sample_text}\")\n",
    "        print(f\"Basic decode: {decoded}\")\n",
    "\n",
    "        sequence_length = 50\n",
    "        vocab_size = len(self)\n",
    "        fake_logits = torch.randn(1, sequence_length, vocab_size)\n",
    "        fake_length = torch.tensor([sequence_length])\n",
    "\n",
    "        if self.decoder is not None:\n",
    "            print(\"\\nTesting pyctcdecode integration...\")\n",
    "            decoded_with_lm = self.ctc_decode(fake_logits)\n",
    "            print(f\"Decoded with LM: {decoded_with_lm}\")\n",
    "\n",
    "            print(f\"\\nBeam width: {self.beam_size}\")\n",
    "            print(f\"LM weight: {self.lm_weight}\")\n",
    "        else:\n",
    "            print(\"\\nNo language model loaded - using basic CTC decoding\")\n",
    "            basic_decoded = self._basic_ctc_decode(fake_logits.numpy(), sequence_length)\n",
    "            print(f\"Basic CTC decoded: {basic_decoded[0]}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the size of the vocabulary.\"\"\"\n",
    "        return len(self.vocab)\n",
    "\n",
    "    def ctc_decode(self, logits: Union[torch.Tensor, List[int], np.ndarray]) -> str:\n",
    "        \"\"\"\n",
    "        Perform CTC decoding on logits.\n",
    "\n",
    "        Parameters:\n",
    "        - logits (Union[torch.Tensor, List[int], np.ndarray]): Logits or token indices.\n",
    "\n",
    "        Returns:\n",
    "        - str: Decoded text.\n",
    "        \"\"\"\n",
    "        if isinstance(logits, np.ndarray):\n",
    "            logits = torch.from_numpy(logits)\n",
    "        elif isinstance(logits, list):\n",
    "            logits = torch.tensor(logits)\n",
    "\n",
    "        if logits.dim() == 3:\n",
    "            decoded_text = self.decode_logits(logits)\n",
    "            return decoded_text\n",
    "        elif logits.dim() == 2:\n",
    "            decoded_text = self.decode_logits(logits)\n",
    "            return decoded_text\n",
    "        elif logits.dim() == 1:\n",
    "            decoded_text = self.decode_indices(logits)\n",
    "            return decoded_text\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported logits shape: {logits.shape}. Expected 1D, 2D, or 3D.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updated WV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import kenlm\n",
    "from pyctcdecode import build_ctcdecoder\n",
    "import numpy as np\n",
    "import os\n",
    "import sentencepiece as spm\n",
    "\n",
    "from typing import List, Tuple, Optional, Union\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "class CTCTextEncoder:\n",
    "    def __init__(\n",
    "        self,\n",
    "        arpa_path: Optional[str] = None,\n",
    "        binary_path: Optional[str] = \"4-gram_lc_correct.bin\",\n",
    "        unigram_path: Optional[str] = \"librispeech-vocab.txt\",\n",
    "        pretrained_tokenizer: str = \"facebook/wav2vec2-base-960h\",\n",
    "        lm_weight: float = 0.5,\n",
    "        beam_size: int = 100,\n",
    "        use_lm: bool = False,     \n",
    "        use_bpe: bool = False,    \n",
    "        blank_token: str = \"[PAD]\",\n",
    "        unk_token: str = \"[UNK]\",\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        CTCTextEncoder can do:\n",
    "          - Character-based encoding if use_bpe=False\n",
    "          - SentencePiece subword encoding if use_bpe=True\n",
    "        \"\"\"\n",
    "        self.beam_size = beam_size\n",
    "        self.lm_weight = lm_weight\n",
    "        self.arpa_path = arpa_path\n",
    "        self.binary_path = binary_path\n",
    "        self.blank_token = blank_token\n",
    "        self.unk_token = unk_token\n",
    "        self.use_lm = use_lm\n",
    "        self.use_bpe = use_bpe\n",
    "        self.printed_samples = 0\n",
    "        self.max_printed_samples = 5\n",
    "\n",
    "        print(\"CTC Text Encoder:\")\n",
    "        print(\"pretrained_tokenizer:\", pretrained_tokenizer)\n",
    "        print(\"lm_weight:\", lm_weight)\n",
    "        print(\"beam_size:\", beam_size)\n",
    "        print(\"binary_path:\", binary_path)\n",
    "        print(\"use_lm:\", self.use_lm)\n",
    "        print(\"use_bpe:\", self.use_bpe)\n",
    "        self.pretrained_tokenizer = pretrained_tokenizer\n",
    "\n",
    "        # Load unigrams if provided\n",
    "        self.unigrams = None\n",
    "        if unigram_path and os.path.exists(unigram_path):\n",
    "            print(f\"Loading unigrams from: {unigram_path}\")\n",
    "            with open(unigram_path, 'r', encoding='utf-8') as f:\n",
    "                self.unigrams = [line.strip().lower() for line in f if line.strip()]\n",
    "            print(f\"Loaded {len(self.unigrams)} unigrams\")\n",
    "\n",
    "        # Initialize the tokenizer or set up character-based vocab\n",
    "        self._initialize_vocabulary(pretrained_tokenizer)\n",
    "\n",
    "        # Create index mappings\n",
    "        self.ind2char = dict(enumerate(self.vocab))\n",
    "        self.char2ind = {v: k for k, v in self.ind2char.items()}\n",
    "        self.blank_index = self.char2ind.get(self.blank_token, None)\n",
    "\n",
    "        print(f\"\\nVocabulary Info:\")\n",
    "        print(f\"Size: {len(self.vocab)}\")\n",
    "        print(\"Full Vocabulary (up to first 50 tokens):\", self.vocab[:50])\n",
    "        print(f\"Blank token: {self.blank_token}, Blank index: {self.blank_index}\")\n",
    "\n",
    "        sample_inds = list(self.ind2char.keys())[:10]\n",
    "        print(\"Sample ind2char mappings:\", {k: self.ind2char[k] for k in sample_inds})\n",
    "        sample_chars = list(self.char2ind.keys())[:10]\n",
    "        print(\"Sample char2ind mappings:\", {k: self.char2ind[k] for k in sample_chars})\n",
    "\n",
    "        # Conditionnally init LM\n",
    "        if self.use_lm:\n",
    "            self._initialize_language_model()\n",
    "        else:\n",
    "            print(\"Language model usage is disabled.\")\n",
    "            self.lm = None\n",
    "            self.decoder = None\n",
    "\n",
    "    def _initialize_vocabulary(self, pretrained_tokenizer: str):\n",
    "        \"\"\"\n",
    "        If use_bpe=True => load a SentencePiece model from `pretrained_tokenizer`.\n",
    "        Otherwise => a standard char-based vocab.\n",
    "        \"\"\"\n",
    "        if self.use_bpe:\n",
    "            print(\"Initializing subword tokenizer (SentencePiece) for encoding/decoding.\")\n",
    "            if not os.path.exists(pretrained_tokenizer):\n",
    "                raise FileNotFoundError(\n",
    "                    f\"[ERROR] SentencePiece model file not found: {pretrained_tokenizer}\"\n",
    "                )\n",
    "            self.sp = spm.SentencePieceProcessor()\n",
    "            self.sp.load(pretrained_tokenizer)\n",
    "\n",
    "            vocab_size = self.sp.get_piece_size()\n",
    "            print(f\"Loaded SentencePiece model with vocab size={vocab_size}\")\n",
    "\n",
    "            # For LM usage, store the subwords in self.labels\n",
    "            self.labels = [self.sp.id_to_piece(i) for i in range(vocab_size)]\n",
    "            self.vocab = self.labels  # Keep indexing consistent\n",
    "        else:\n",
    "            print(\"Initializing character-based vocabulary without using tokenizer.\")\n",
    "            # Keep your old char-based logic\n",
    "            self.vocab = [\n",
    "                'a','b','c','d','e','f','g','h','i','j','k','l',\n",
    "                'm','n','o','p','q','r','s','t','u','v','w','x','y','z',' '\n",
    "            ]\n",
    "            # put blank at front\n",
    "            self.vocab = [self.blank_token] + self.vocab\n",
    "            self.labels = self.vocab\n",
    "            self.sp = None\n",
    "\n",
    "    def encode(self, text: str) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Encode text => token IDs. \n",
    "        If use_bpe => subword via SentencePiece; else => char-based.\n",
    "        \"\"\"\n",
    "        if self.use_bpe and self.sp is not None:\n",
    "            # OPTIONAL: If your SentencePiece model was trained on uppercase data, do text = text.upper().\n",
    "            # Otherwise, keep as is or do text.lower() based on how your model was trained.\n",
    "            token_ids = self.sp.encode(text, out_type=int)\n",
    "            return torch.tensor([token_ids], dtype=torch.long)\n",
    "        else:\n",
    "            # Char-based\n",
    "            normalized_text = self.normalize_text(text)\n",
    "            token_indices = [self.char2ind.get(ch, self.char2ind.get(self.unk_token))\n",
    "                             for ch in normalized_text]\n",
    "            return torch.tensor(token_indices).unsqueeze(0)\n",
    "\n",
    "    def decode_simple(self, indices: List[int]) -> str:\n",
    "        \"\"\"\n",
    "        Simple CTC decoding: collapse repeated tokens, remove blank.\n",
    "        Then if we're in BPE mode, we can do a direct sp.decode fallback \n",
    "        for a better subword reconstruction (rather than naive char-join).\n",
    "        \"\"\"\n",
    "        # If using BPE, let's do a direct SentencePiece decode \n",
    "        # for a more correct subword -> text mapping \n",
    "        # (especially for unknown words, spaces, etc.).\n",
    "        if self.use_bpe and self.sp is not None:\n",
    "            # But we must handle repeated tokens for CTC:\n",
    "            collapsed = []\n",
    "            prev_idx = None\n",
    "            for idx in indices:\n",
    "                if idx == self.blank_index:\n",
    "                    prev_idx = idx\n",
    "                    continue\n",
    "                if idx != prev_idx:\n",
    "                    collapsed.append(idx)\n",
    "                prev_idx = idx\n",
    "            # Now decode with sp\n",
    "            text = self.sp.decode(collapsed)\n",
    "            return text\n",
    "        else:\n",
    "            # Character-based or fallback approach\n",
    "            decoded_chars = []\n",
    "            previous_idx = None\n",
    "            for idx in indices:\n",
    "                if idx == self.blank_index:\n",
    "                    previous_idx = idx\n",
    "                    continue\n",
    "                if idx == previous_idx:\n",
    "                    continue\n",
    "                if 0 <= idx < len(self.ind2char):\n",
    "                    decoded_chars.append(self.ind2char[idx])\n",
    "                previous_idx = idx\n",
    "            text = \"\".join(decoded_chars).strip().lower()\n",
    "            return text\n",
    "\n",
    "    def decode(self, indices: List[int]) -> str:\n",
    "        \"\"\"\n",
    "        If an LM is available, do LM decode. Otherwise, fallback to decode_simple.\n",
    "        \"\"\"\n",
    "        if self.decoder:\n",
    "            decoded_text = self.decoder.decode(indices)\n",
    "            return decoded_text.lower().strip()\n",
    "        else:\n",
    "            return self.decode_simple(indices).lower().strip()\n",
    "\n",
    "    def decode_logits(self, logits: Union[torch.Tensor, List[List[float]], np.ndarray]) -> str:\n",
    "        \"\"\"\n",
    "        If LM is present, decode with beam search. Otherwise greedy decode.\n",
    "        \"\"\"\n",
    "        if isinstance(logits, torch.Tensor):\n",
    "            logits = logits.cpu().numpy()\n",
    "        elif isinstance(logits, list):\n",
    "            logits = np.array(logits)\n",
    "        elif not isinstance(logits, np.ndarray):\n",
    "            raise TypeError(\"logits must be a torch.Tensor, list of lists, or np.ndarray.\")\n",
    "\n",
    "        if logits.ndim == 3:\n",
    "            logits = logits[0]\n",
    "        if logits.ndim != 2:\n",
    "            raise ValueError(f\"Logits should be 2D, got shape={logits.shape}\")\n",
    "\n",
    "        if self.decoder:\n",
    "            return self.decoder.decode(logits)\n",
    "        else:\n",
    "            indices = np.argmax(logits, axis=-1).tolist()\n",
    "            return self.decode_simple(indices)\n",
    "\n",
    "    def decode_indices(self, indices: Union[torch.Tensor, List[int], np.ndarray]) -> str:\n",
    "        \"\"\"\n",
    "        Naive decode from token indices (no LM).\n",
    "        \"\"\"\n",
    "        if isinstance(indices, torch.Tensor):\n",
    "            indices = indices.squeeze().tolist()\n",
    "        elif isinstance(indices, np.ndarray):\n",
    "            indices = indices.tolist()\n",
    "        return self.decode_simple(indices)\n",
    "\n",
    "    def ctc_decode(self, logits: Union[torch.Tensor, List[int], np.ndarray]) -> str:\n",
    "        \"\"\"\n",
    "        CTC decode from logits or direct token IDs.\n",
    "        \"\"\"\n",
    "        if isinstance(logits, np.ndarray):\n",
    "            logits = torch.from_numpy(logits)\n",
    "        elif isinstance(logits, list):\n",
    "            logits = torch.tensor(logits)\n",
    "\n",
    "        if logits.dim() == 3:\n",
    "            logits = logits[0]\n",
    "        if self.use_bpe:\n",
    "            if self.use_lm and self.decoder:\n",
    "                return self.decoder.decode(logits)\n",
    "            else:\n",
    "                predicted_indices = torch.argmax(logits, axis=-1).tolist()\n",
    "                return self.decode(predicted_indices)\n",
    "        else:\n",
    "            predicted_indices = torch.argmax(logits, axis=-1).tolist()\n",
    "            return self.decode_simple(predicted_indices)\n",
    "\n",
    "    def _initialize_language_model(self):\n",
    "        \"\"\"Initialize KenLM + pyctcdecode for an external LM if needed.\"\"\"\n",
    "        self.lm = None\n",
    "        self.decoder = None\n",
    "\n",
    "        model_path = self.binary_path if self.binary_path else self.arpa_path\n",
    "        print(\"model_path:\", model_path)\n",
    "        if not model_path or not os.path.exists(model_path):\n",
    "            print(\"No language model path or file does not exist.\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            self.lm = kenlm.Model(model_path)\n",
    "            print(\"Loaded KenLM language model.\")\n",
    "\n",
    "            decoder_config = {\n",
    "                \"labels\": self.labels if self.use_bpe else self.vocab,\n",
    "                \"kenlm_model_path\": model_path,\n",
    "                \"alpha\": self.lm_weight,\n",
    "                \"beta\": 0.1,\n",
    "                \"unk_score_offset\": -10.0,\n",
    "            }\n",
    "\n",
    "            if self.unigrams:\n",
    "                print(\"\\n--- Unigrams List ---\")\n",
    "                with open(\"unigrams_list.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "                    for unigram in self.unigrams:\n",
    "                        f.write(f\"{unigram}\\n\")\n",
    "                print(f\"Unigrams list saved to 'unigrams_list.txt'. Count={len(self.unigrams)}\")\n",
    "                decoder_config[\"unigrams\"] = self.unigrams\n",
    "\n",
    "            self.decoder = build_ctcdecoder(**decoder_config)\n",
    "            print(\"Successfully initialized pyctcdecode with LM.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not init LM: {str(e)}\")\n",
    "            self.decoder = None\n",
    "\n",
    "    def score_with_lm(self, text: str) -> float:\n",
    "        if self.lm is None:\n",
    "            return 0.0\n",
    "        if not text.strip():\n",
    "            return float(\"-inf\")\n",
    "        return self.lm.score(text.lower().strip(), bos=True, eos=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_text(text: str) -> str:\n",
    "        \"\"\"Used only in char-based mode.\"\"\"\n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"[^a-z ]\", \"\", text)\n",
    "        return text\n",
    "\n",
    "    def test_decoder(self, sample_text: str = \"test decoder functionality\"):\n",
    "        print(\"\\nTesting decoder configuration...\")\n",
    "\n",
    "        encoded = self.encode(sample_text)\n",
    "        print(f\"Encoded => {encoded}\")\n",
    "        decoded = self.decode_simple(encoded[0].tolist())\n",
    "        print(f\"Naive decode => {decoded}\")\n",
    "\n",
    "        # Test random logits\n",
    "        seq_len = 10\n",
    "        vocab_size = len(self)\n",
    "        fake_logits = torch.randn(1, seq_len, vocab_size)\n",
    "\n",
    "        if self.decoder:\n",
    "            print(\"\\nTesting pyctcdecode with random logits...\")\n",
    "            text_lm = self.ctc_decode(fake_logits)\n",
    "            print(f\"Decoded with LM => {text_lm}\")\n",
    "        else:\n",
    "            print(\"\\nNo LM loaded => using basic greedy decode.\")\n",
    "            pred = self.decode_logits(fake_logits)\n",
    "            print(f\"Greedy decoded => {pred}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import kenlm\n",
    "from transformers import Wav2Vec2Processor, AutoProcessor, AutoTokenizer\n",
    "from pyctcdecode import build_ctcdecoder\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from typing import List, Tuple, Optional, Union\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "class CTCTextEncoder:\n",
    "    def __init__(\n",
    "        self,\n",
    "        arpa_path: Optional[str] = None,\n",
    "        binary_path: Optional[str] = \"4-gram_lc_correct.bin\",\n",
    "        unigram_path: Optional[str] = \"librispeech-vocab.txt\",\n",
    "        pretrained_tokenizer: str = \"facebook/wav2vec2-base-960h\",\n",
    "        lm_weight: float = 0.5,\n",
    "        beam_size: int = 100,\n",
    "        use_lm: bool = False,\n",
    "        use_bpe: bool = False,\n",
    "        blank_token: str = \"[PAD]\",\n",
    "        unk_token: str = \"[UNK]\",\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize encoder with conditional subword (SentencePiece) or char-based vocab.\n",
    "        \"\"\"\n",
    "        self.beam_size = beam_size\n",
    "        self.lm_weight = lm_weight\n",
    "        self.arpa_path = arpa_path\n",
    "        self.binary_path = binary_path\n",
    "        self.blank_token = blank_token\n",
    "        self.unk_token = unk_token\n",
    "        self.use_lm = use_lm\n",
    "        self.use_bpe = use_bpe\n",
    "        self.printed_samples = 0\n",
    "        self.max_printed_samples = 5\n",
    "\n",
    "        print(\"CTC Text Encoder:\")\n",
    "        print(\"pretrained_tokenizer:\", pretrained_tokenizer)\n",
    "        print(\"lm_weight:\", lm_weight)\n",
    "        print(\"beam_size:\", beam_size)\n",
    "        print(\"binary_path:\", binary_path)\n",
    "        print(\"use_lm:\", self.use_lm)\n",
    "        print(\"use_bpe:\", self.use_bpe)\n",
    "\n",
    "        self.pretrained_tokenizer = pretrained_tokenizer\n",
    "\n",
    "        # Load unigrams if provided\n",
    "        self.unigrams = None\n",
    "        if unigram_path and os.path.exists(unigram_path):\n",
    "            print(f\"Loading unigrams from: {unigram_path}\")\n",
    "            with open(unigram_path, 'r', encoding='utf-8') as f:\n",
    "                self.unigrams = [line.strip().lower() for line in f if line.strip()]\n",
    "            print(f\"Loaded {len(self.unigrams)} unigrams\")\n",
    "\n",
    "        # Initialize vocabulary/tokenizer\n",
    "        self._initialize_vocabulary(pretrained_tokenizer)\n",
    "\n",
    "        # Create index mappings\n",
    "        self.ind2char = dict(enumerate(self.vocab))\n",
    "        self.char2ind = {v: k for k, v in self.ind2char.items()}\n",
    "        self.blank_index = self.char2ind.get(self.blank_token, None)\n",
    "\n",
    "        print(\"\\nVocabulary Info:\")\n",
    "        print(f\"Size: {len(self.vocab)}\")\n",
    "        print(\"Full Vocabulary (up to first 50 tokens):\", self.vocab[:50])\n",
    "        print(f\"Blank token: {self.blank_token}, Blank index: {self.blank_index}\")\n",
    "\n",
    "        sample_ind_keys = list(self.ind2char.keys())[:10]\n",
    "        print(\"Sample ind2char mappings:\", {k: self.ind2char[k] for k in sample_ind_keys})\n",
    "\n",
    "        sample_char_keys = list(self.char2ind.keys())[:10]\n",
    "        print(\"Sample char2ind mappings:\", {k: self.char2ind[k] for k in sample_char_keys})\n",
    "\n",
    "        # Conditionally load LM\n",
    "        if self.use_lm:\n",
    "            self._initialize_language_model()\n",
    "        else:\n",
    "            print(\"Language model usage is disabled.\")\n",
    "            self.lm = None\n",
    "            self.decoder = None\n",
    "\n",
    "    def _initialize_vocabulary(self, pretrained_tokenizer: str):\n",
    "        \"\"\"\n",
    "        If use_bpe=True, load a SentencePiece model from `pretrained_tokenizer` path.\n",
    "        Otherwise, use a basic character-based vocab.\n",
    "        \"\"\"\n",
    "        if self.use_bpe:\n",
    "            print(\"Initializing subword tokenizer using SentencePiece.\")\n",
    "            import sentencepiece as spm\n",
    "\n",
    "            if not os.path.exists(pretrained_tokenizer):\n",
    "                raise FileNotFoundError(\n",
    "                    f\"SentencePiece model not found at: {pretrained_tokenizer}\"\n",
    "                )\n",
    "\n",
    "            # Load the SentencePiece model\n",
    "            self.sp = spm.SentencePieceProcessor()\n",
    "            self.sp.load(pretrained_tokenizer)\n",
    "\n",
    "            # Create a dummy \"vocab\" list to keep indexing logic consistent\n",
    "            # We'll store each piece as a string, e.g. \"sp_0\", \"sp_1\", ... or piece text\n",
    "            vocab_size = self.sp.get_piece_size()\n",
    "            print(f\"Loaded SentencePiece model with vocab size={vocab_size}\")\n",
    "\n",
    "            # We'll store the actual piece text in self.labels for LM usage if needed\n",
    "            self.labels = [self.sp.id_to_piece(i) for i in range(vocab_size)]\n",
    "            self.vocab = self.labels  # So len(self.vocab) = sp.get_piece_size()\n",
    "\n",
    "            # We do not forcibly insert blank_token or unk_token here\n",
    "            # SentencePiece might have <unk> at id=0, etc.\n",
    "\n",
    "        else:\n",
    "            print(\"Initializing character-based vocabulary without tokenizer.\")\n",
    "            self.vocab = [\n",
    "                \"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\",\n",
    "                \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\",\n",
    "                \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\",\n",
    "                \"y\", \"z\", \" \"\n",
    "            ]\n",
    "            # Put blank at the front\n",
    "            self.vocab = [self.blank_token] + self.vocab\n",
    "            self.labels = self.vocab\n",
    "            self.sp = None\n",
    "\n",
    "    def encode(self, text: str) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Encode text: if use_bpe => SentencePiece; else => char-based.\n",
    "        Returns a 2D tensor [1, seq_len].\n",
    "        \"\"\"\n",
    "        if self.use_bpe:\n",
    "            # SentencePiece encode\n",
    "            token_ids = self.sp.encode(text, out_type=int)\n",
    "            # Wrap in batch dimension\n",
    "            return torch.tensor([token_ids], dtype=torch.long)\n",
    "        else:\n",
    "            # Character-based\n",
    "            normalized_text = self.normalize_text(text)\n",
    "            token_indices = [\n",
    "                self.char2ind.get(char, self.char2ind.get(self.unk_token))\n",
    "                for char in normalized_text\n",
    "            ]\n",
    "            return torch.tensor(token_indices).unsqueeze(0)\n",
    "\n",
    "    def decode_simple(self, indices: List[int]) -> str:\n",
    "        \"\"\"\n",
    "        Simple CTC-like decoding: collapses repeats & removes blank. Then lowercases.\n",
    "        For subword, this is naive but consistent with original code logic.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.use_bpe and self.sp is not None:\n",
    "        # Handle repeated tokens due to CTC decoding\n",
    "            collapsed = []\n",
    "            prev_idx = None\n",
    "            for idx in indices:\n",
    "                if idx == self.blank_index:\n",
    "                    prev_idx = idx\n",
    "                    continue\n",
    "                if idx != prev_idx:\n",
    "                    collapsed.append(idx)\n",
    "                prev_idx = idx\n",
    "\n",
    "            # Decode subword tokens using SentencePiece\n",
    "            text = self.sp.decode(collapsed)\n",
    "            # Remove SentencePiece's `▁` symbol (denotes spaces)\n",
    "            text = text.replace(\"▁\", \" \").strip()\n",
    "            return text\n",
    "        else:\n",
    "\n",
    "            decoded_chars = []\n",
    "            previous_idx = None\n",
    "\n",
    "            for idx in indices:\n",
    "                if idx == self.blank_index:\n",
    "                    previous_idx = idx\n",
    "                    continue\n",
    "                if idx == previous_idx:\n",
    "                    continue\n",
    "                if 0 <= idx < len(self.ind2char):\n",
    "                    decoded_chars.append(self.ind2char[idx])\n",
    "                previous_idx = idx\n",
    "\n",
    "            text = \"\".join(decoded_chars).strip().lower()\n",
    "\n",
    "            # If using subword, the above is naive (we're literally concatenating piece strings)\n",
    "            # A better approach is to call self.sp.decode(...) but we keep this for consistency.\n",
    "        return text\n",
    "\n",
    "    def decode(self, indices: List[int]) -> str:\n",
    "        \"\"\"\n",
    "        If we have a pyctcdecode LM, use that. Else fall back to `decode_simple`.\n",
    "        \"\"\"\n",
    "        if self.decoder:\n",
    "            decoded_text = self.decoder.decode(indices)\n",
    "            decoded_text = decoded_text.lower()\n",
    "            decoded_text = decoded_text.replace(\"▁\", \" \")\n",
    "            return decoded_text\n",
    "        else:\n",
    "            # Fallback: naive method\n",
    "            decoded_text = self.decode_simple(indices)\n",
    "            return decoded_text.lower()\n",
    "\n",
    "    def decode_logits(self, logits: Union[torch.Tensor, List[List[float]], np.ndarray]) -> str:\n",
    "        \"\"\"\n",
    "        Decode from raw logits. If LM is present => use it; else greedy.\n",
    "        \"\"\"\n",
    "        if isinstance(logits, torch.Tensor):\n",
    "            logits = logits.cpu().numpy()\n",
    "        elif isinstance(logits, list):\n",
    "            logits = np.array(logits)\n",
    "        elif not isinstance(logits, np.ndarray):\n",
    "            raise TypeError(\"logits must be torch.Tensor, list of lists, or np.ndarray\")\n",
    "\n",
    "        if logits.ndim == 3:\n",
    "            logits = logits[0]\n",
    "\n",
    "        if logits.ndim != 2:\n",
    "            raise ValueError(f\"Logits should be 2D, got shape {logits.shape}\")\n",
    "\n",
    "        if self.decoder:\n",
    "            decoded_text = self.decoder.decode(logits)\n",
    "            return decoded_text\n",
    "        else:\n",
    "            # Greedy decode\n",
    "            predicted_indices = np.argmax(logits, axis=-1).tolist()\n",
    "            return self.decode_simple(predicted_indices)\n",
    "\n",
    "    def decode_indices(self, indices: Union[torch.Tensor, List[int], np.ndarray]) -> str:\n",
    "        \"\"\"\n",
    "        Decode token indices to text with naive approach (no LM).\n",
    "        \"\"\"\n",
    "        if isinstance(indices, torch.Tensor):\n",
    "            indices = indices.squeeze().tolist()\n",
    "        elif isinstance(indices, np.ndarray):\n",
    "            indices = indices.tolist()\n",
    "        elif not isinstance(indices, list):\n",
    "            raise TypeError(\"decode_indices expects list/torch.Tensor/np.ndarray.\")\n",
    "\n",
    "        return self.decode_simple(indices)\n",
    "\n",
    "    def ctc_decode(self, logits: Union[torch.Tensor, List[int], np.ndarray]) -> str:\n",
    "        \"\"\"\n",
    "        CTC decode from logits or indices. \n",
    "        For subword, this is naive unless you do beam search or LM.\n",
    "        \"\"\"\n",
    "        if isinstance(logits, np.ndarray):\n",
    "            logits = torch.from_numpy(logits)\n",
    "        elif isinstance(logits, list):\n",
    "            logits = torch.tensor(logits)\n",
    "\n",
    "        if logits.dim() == 3:\n",
    "            logits = logits[0]  # [1, T, V] -> [T, V]\n",
    "\n",
    "        if self.use_bpe:\n",
    "            if self.use_lm and self.decoder:\n",
    "                # LM-based decode\n",
    "                return self.decoder.decode(logits)\n",
    "            else:\n",
    "                # Greedy subword decode\n",
    "                predicted_indices = torch.argmax(logits, axis=-1).tolist()\n",
    "                return self.decode(predicted_indices)\n",
    "        else:\n",
    "            # Character-based\n",
    "            predicted_indices = torch.argmax(logits, axis=-1).tolist()\n",
    "            return self.decode_simple(predicted_indices)\n",
    "\n",
    "    def _initialize_language_model(self):\n",
    "        \"\"\"Initialize KenLM + pyctcdecode if LM is requested.\"\"\"\n",
    "        self.lm = None\n",
    "        self.decoder = None\n",
    "\n",
    "        model_path = self.binary_path if self.binary_path else self.arpa_path\n",
    "        print(\"model_path: \", model_path)\n",
    "        if not model_path or not os.path.exists(model_path):\n",
    "            print(\"No language model path provided or file does not exist.\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            self.lm = kenlm.Model(model_path)\n",
    "            print(f\"Loaded {'binary' if self.binary_path else 'ARPA'} language model.\")\n",
    "\n",
    "            decoder_config = {\n",
    "                \"labels\": self.labels if self.use_bpe else self.vocab,\n",
    "                \"kenlm_model_path\": model_path,\n",
    "                \"alpha\": self.lm_weight,\n",
    "                \"beta\": 0.1,\n",
    "                \"unk_score_offset\": -10.0,\n",
    "            }\n",
    "\n",
    "            if self.unigrams:\n",
    "                print(\"\\n--- Unigrams List ---\")\n",
    "                with open(\"unigrams_list.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "                    for unigram in self.unigrams:\n",
    "                        f.write(f\"{unigram}\\n\")\n",
    "                print(f\"Unigrams list saved to 'unigrams_list.txt'. Total unigrams: {len(self.unigrams)}\")\n",
    "                print(\"----------------------\\n\")\n",
    "                decoder_config[\"unigrams\"] = self.unigrams\n",
    "\n",
    "            self.decoder = build_ctcdecoder(**decoder_config)\n",
    "            print(\"Successfully initialized language model and decoder.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Failed to initialize decoder: {str(e)}\")\n",
    "            self.decoder = None\n",
    "\n",
    "    def test_language_model(self):\n",
    "        \"\"\"Debug: test LM if loaded.\"\"\"\n",
    "        print(\"\\nTesting Language Model...\")\n",
    "\n",
    "        if self.lm is None:\n",
    "            print(\"Error: Language model is not loaded!\")\n",
    "            return\n",
    "\n",
    "        test_sentences = [\n",
    "            \"this is a good sentence\",\n",
    "            \"this is also a good sentence\",\n",
    "            \"thiss iss nott aa goodd sentencee\",\n",
    "        ]\n",
    "\n",
    "        for s in test_sentences:\n",
    "            score = self.score_with_lm(s)\n",
    "            print(f\"LM Score for '{s}': {score:.4f}\")\n",
    "\n",
    "    def score_with_lm(self, text: str) -> float:\n",
    "        \"\"\"Score text with LM if present.\"\"\"\n",
    "        if self.lm is None:\n",
    "            return 0.0\n",
    "        if not text or not text.strip():\n",
    "            return float(\"-inf\")\n",
    "        text = text.lower().strip()\n",
    "        return self.lm.score(text, bos=True, eos=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_text(text: str) -> str:\n",
    "        \"\"\"Normalize for character-based approach.\"\"\"\n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"[^a-z ]\", \"\", text)\n",
    "        return text\n",
    "\n",
    "    def _basic_ctc_decode(self, logits: np.ndarray, sequence_length: int) -> List[str]:\n",
    "        \"\"\"\n",
    "        Basic CTC greedy decode without LM. \n",
    "        Typically for use_bpe=False path or fallback testing.\n",
    "        \"\"\"\n",
    "        argmax_indices = np.argmax(logits, axis=-1)\n",
    "\n",
    "        if len(argmax_indices.shape) == 0:\n",
    "            argmax_indices = np.array([argmax_indices])\n",
    "\n",
    "        if len(argmax_indices.shape) == 1:\n",
    "            argmax_indices = np.expand_dims(argmax_indices, axis=0)\n",
    "\n",
    "        predictions = []\n",
    "        for sequence in argmax_indices:\n",
    "            decoded = []\n",
    "            last_idx = None\n",
    "            for idx in sequence[:sequence_length]:\n",
    "                if idx != self.blank_index and idx != last_idx:\n",
    "                    decoded.append(self.ind2char[idx])\n",
    "                last_idx = idx\n",
    "\n",
    "            text = \"\".join(decoded)\n",
    "            if self.use_bpe and self.sp:\n",
    "                # We could do self.sp.decode(...) but to keep it consistent, just do naive cleanup\n",
    "                pass\n",
    "            predictions.append(text)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def test_decoder(self, sample_text: str = \"test decoder functionality\"):\n",
    "        \"\"\"Test the encode/decode pipeline.\"\"\"\n",
    "        print(\"\\nTesting decoder configuration...\")\n",
    "\n",
    "        encoded = self.encode(sample_text)\n",
    "        decoded = self.decode(encoded[0].tolist())\n",
    "        print(f\"Original text: {sample_text}\")\n",
    "        print(f\"Basic decode: {decoded}\")\n",
    "\n",
    "        seq_length = 50\n",
    "        vocab_size = len(self)\n",
    "        fake_logits = torch.randn(1, seq_length, vocab_size)\n",
    "\n",
    "        if self.decoder is not None:\n",
    "            print(\"\\nTesting pyctcdecode integration with fake logits...\")\n",
    "            decoded_with_lm = self.ctc_decode(fake_logits)\n",
    "            print(f\"Decoded with LM: {decoded_with_lm}\")\n",
    "        else:\n",
    "            print(\"\\nNo language model loaded - using basic CTC decoding\")\n",
    "            basic_decoded = self._basic_ctc_decode(fake_logits.numpy(), seq_length)\n",
    "            print(f\"Basic CTC decoded: {basic_decoded[0]}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of tokens in the vocab.\"\"\"\n",
    "        return len(self.vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hasbooted  onstart.sh\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/sound_asr\n"
     ]
    }
   ],
   "source": [
    "%cd /workspace/sound_asr/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing with use_bpe=False and use_lm=False ---\n",
      "CTC Text Encoder:\n",
      "pretrained_tokenizer: sentencepiece_model/librispeech_unigram_model.model\n",
      "lm_weight: 0.5\n",
      "beam_size: 100\n",
      "binary_path: 4-gram_lc_correct.bin\n",
      "use_lm: False\n",
      "use_bpe: False\n",
      "Loading unigrams from: librispeech-vocab.txt\n",
      "Loaded 200000 unigrams\n",
      "Initializing character-based vocabulary without tokenizer.\n",
      "\n",
      "Vocabulary Info:\n",
      "Size: 28\n",
      "Full Vocabulary (up to first 50 tokens): ['[PAD]', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', ' ']\n",
      "Blank token: [PAD], Blank index: 0\n",
      "Sample ind2char mappings: {0: '[PAD]', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i'}\n",
      "Sample char2ind mappings: {'[PAD]': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9}\n",
      "Language model usage is disabled.\n",
      "Encoded: tensor([[ 8,  5, 12, 12, 15, 27, 23, 15, 18, 12,  4]])\n",
      "Decoded: helo world\n",
      "\n",
      "--- Testing with use_bpe=True and use_lm=False ---\n",
      "CTC Text Encoder:\n",
      "pretrained_tokenizer: sentencepiece_model/librispeech_unigram_model.model\n",
      "lm_weight: 0.5\n",
      "beam_size: 100\n",
      "binary_path: 4-gram_lc_correct.bin\n",
      "use_lm: False\n",
      "use_bpe: True\n",
      "Loading unigrams from: librispeech-vocab.txt\n",
      "Loaded 200000 unigrams\n",
      "Initializing subword tokenizer using SentencePiece.\n",
      "Loaded SentencePiece model with vocab size=10000\n",
      "\n",
      "Vocabulary Info:\n",
      "Size: 10000\n",
      "Full Vocabulary (up to first 50 tokens): ['<unk>', '<pad>', '.', '!', '?', ',', \"'\", 's', '▁', 'e', 'a', 'er', 'd', 'ing', 't', 'y', 'ed', 'o', 'in', 'i', 'n', 'ly', 'es', 'en', 'an', 'on', 'al', 'le', 'k', 'r', 'g', 'l', 'or', 'el', 'us', '▁un', 'p', 'u', 'h', 'm', 'man', 'ion', 'c', 'ness', 'ton', 'is', 'ie', 'z', 'ar', 'et']\n",
      "Blank token: [PAD], Blank index: None\n",
      "Sample ind2char mappings: {0: '<unk>', 1: '<pad>', 2: '.', 3: '!', 4: '?', 5: ',', 6: \"'\", 7: 's', 8: '▁', 9: 'e'}\n",
      "Sample char2ind mappings: {'<unk>': 0, '<pad>': 1, '.': 2, '!': 3, '?': 4, ',': 5, \"'\": 6, 's': 7, '▁': 8, 'e': 9}\n",
      "Language model usage is disabled.\n",
      "Encoded: tensor([[1707,   17,    8, 2425]])\n",
      "Decoded: hello world\n",
      "\n",
      "--- Testing with use_bpe=False and use_lm=True ---\n",
      "CTC Text Encoder:\n",
      "pretrained_tokenizer: sentencepiece_model/librispeech_unigram_model.model\n",
      "lm_weight: 0.5\n",
      "beam_size: 100\n",
      "binary_path: 4-gram_lc_correct.bin\n",
      "use_lm: True\n",
      "use_bpe: False\n",
      "Loading unigrams from: librispeech-vocab.txt\n",
      "Loaded 200000 unigrams\n",
      "Initializing character-based vocabulary without tokenizer.\n",
      "\n",
      "Vocabulary Info:\n",
      "Size: 28\n",
      "Full Vocabulary (up to first 50 tokens): ['[PAD]', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', ' ']\n",
      "Blank token: [PAD], Blank index: 0\n",
      "Sample ind2char mappings: {0: '[PAD]', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i'}\n",
      "Sample char2ind mappings: {'[PAD]': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9}\n",
      "model_path:  4-gram_lc_correct.bin\n",
      "Loaded binary language model.\n",
      "\n",
      "--- Unigrams List ---\n",
      "Unigrams list saved to 'unigrams_list.txt'. Total unigrams: 200000\n",
      "----------------------\n",
      "\n",
      "Successfully initialized language model and decoder.\n",
      "Encoded: tensor([[ 8,  5, 12, 12, 15, 27, 23, 15, 18, 12,  4]])\n",
      "Decoded: helo world\n",
      "\n",
      "--- Testing with use_bpe=True and use_lm=True ---\n",
      "CTC Text Encoder:\n",
      "pretrained_tokenizer: sentencepiece_model/librispeech_unigram_model.model\n",
      "lm_weight: 0.5\n",
      "beam_size: 100\n",
      "binary_path: 4-gram_lc_correct.bin\n",
      "use_lm: True\n",
      "use_bpe: True\n",
      "Loading unigrams from: librispeech-vocab.txt\n",
      "Loaded 200000 unigrams\n",
      "Initializing subword tokenizer using SentencePiece.\n",
      "Loaded SentencePiece model with vocab size=10000\n",
      "\n",
      "Vocabulary Info:\n",
      "Size: 10000\n",
      "Full Vocabulary (up to first 50 tokens): ['<unk>', '<pad>', '.', '!', '?', ',', \"'\", 's', '▁', 'e', 'a', 'er', 'd', 'ing', 't', 'y', 'ed', 'o', 'in', 'i', 'n', 'ly', 'es', 'en', 'an', 'on', 'al', 'le', 'k', 'r', 'g', 'l', 'or', 'el', 'us', '▁un', 'p', 'u', 'h', 'm', 'man', 'ion', 'c', 'ness', 'ton', 'is', 'ie', 'z', 'ar', 'et']\n",
      "Blank token: [PAD], Blank index: None\n",
      "Sample ind2char mappings: {0: '<unk>', 1: '<pad>', 2: '.', 3: '!', 4: '?', 5: ',', 6: \"'\", 7: 's', 8: '▁', 9: 'e'}\n",
      "Sample char2ind mappings: {'<unk>': 0, '<pad>': 1, '.': 2, '!': 3, '?': 4, ',': 5, \"'\": 6, 's': 7, '▁': 8, 'e': 9}\n",
      "model_path:  4-gram_lc_correct.bin\n",
      "Loaded binary language model.\n",
      "\n",
      "--- Unigrams List ---\n",
      "Unigrams list saved to 'unigrams_list.txt'. Total unigrams: 200000\n",
      "----------------------\n",
      "\n",
      "Successfully initialized language model and decoder.\n",
      "Encoded: tensor([[1707,   17,    8, 2425]])\n",
      "Decoded: hello world\n"
     ]
    }
   ],
   "source": [
    "def test_encoder(encoder_cls, text=\"hello world\"):\n",
    "    \"\"\"\n",
    "    Test the encoder for various configurations of use_bpe and use_lm.\n",
    "\n",
    "    Parameters:\n",
    "    - encoder_cls: Class of the encoder to test.\n",
    "    - text (str): The input text to encode and decode.\n",
    "    \"\"\"\n",
    "    test_cases = [\n",
    "        {\"use_bpe\": False, \"use_lm\": False},\n",
    "        {\"use_bpe\": True, \"use_lm\": False},\n",
    "        {\"use_bpe\": False, \"use_lm\": True},\n",
    "        {\"use_bpe\": True, \"use_lm\": True},\n",
    "    ]\n",
    "\n",
    "    for case in test_cases:\n",
    "        print(f\"\\n--- Testing with use_bpe={case['use_bpe']} and use_lm={case['use_lm']} ---\")\n",
    "\n",
    "        # Initialize encoder with the test case settings\n",
    "        encoder = encoder_cls(\n",
    "            use_bpe=case[\"use_bpe\"],\n",
    "            use_lm=case[\"use_lm\"],\n",
    "            # pretrained_tokenizer=\"facebook/wav2vec2-base-960h\",\n",
    "            pretrained_tokenizer = \"sentencepiece_model/librispeech_unigram_model.model\",\n",
    "            # sp_model_path = \"sentencepiece_model/librispeech_unigram_model.model\",\n",
    "            # pretrained_tokenizer=\"hf-test/xls-r-300m-sv\",\n",
    "            # pretrained_tokenizer = \"bert-base-uncased\",\n",
    "            binary_path=\"4-gram_lc_correct.bin\",\n",
    "            unigram_path=\"librispeech-vocab.txt\"\n",
    "        )\n",
    "\n",
    "        # Test encoding\n",
    "        encoded = encoder.encode(text)\n",
    "        print(f\"Encoded: {encoded}\")\n",
    "\n",
    "        # Test decoding from encoded indices\n",
    "        decoded = encoder.decode_simple(encoded[0].tolist())\n",
    "        print(f\"Decoded: {decoded}\")\n",
    "\n",
    "test_encoder(CTCTextEncoder)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
